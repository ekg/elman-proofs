/-
Copyright (c) 2024 Elman Ablation Ladder Project. All rights reserved.
Released under Apache 2.0 license as described in the file LICENSE.
Authors: Elman Ablation Ladder Team
-/

import Mathlib.Analysis.Calculus.Gradient.Basic
import Mathlib.Analysis.Convex.Basic
import Mathlib.Analysis.Calculus.MeanValue
import Mathlib.Analysis.Calculus.Deriv.Basic
import Mathlib.Analysis.Calculus.Deriv.MeanValue
import Mathlib.Analysis.InnerProductSpace.Calculus

/-!
# Gradient Flow and Learning Dynamics

This file formalizes gradient descent as a dynamical system and proves
convergence results relevant to neural network training.

## Main Definitions

* `GradientDescentStep`: One step of gradient descent
* `IsLSmooth`: Function with L-Lipschitz gradient
* `IsStronglyConvex`: Œº-strongly convex function

## Main Theorems

* `gradient_descent_convex`: O(1/k) convergence for convex functions
* `gradient_descent_strongly_convex`: O(c^k) convergence for strongly convex

## Application to RNN Training

For RNN training with loss L(Œ∏):
- If L is L-smooth and Œº-strongly convex, gradient descent converges linearly
- The condition number Œ∫ = L/Œº determines convergence rate

-/

namespace Gradient

variable {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ‚Ñù E] [CompleteSpace E]

/-- A function is L-smooth if its gradient is L-Lipschitz. -/
def IsLSmooth (f : E ‚Üí ‚Ñù) (L : ‚Ñù) : Prop :=
  Differentiable ‚Ñù f ‚àß ‚àÄ x y, ‚Äñgradient f x - gradient f y‚Äñ ‚â§ L * ‚Äñx - y‚Äñ

/-- A function is Œº-strongly convex. -/
def IsStronglyConvex (f : E ‚Üí ‚Ñù) (Œº : ‚Ñù) : Prop :=
  ‚àÄ x y : E, ‚àÄ t : ‚Ñù, 0 ‚â§ t ‚Üí t ‚â§ 1 ‚Üí
    f (t ‚Ä¢ x + (1 - t) ‚Ä¢ y) ‚â§ t * f x + (1 - t) * f y - (Œº / 2) * t * (1 - t) * ‚Äñx - y‚Äñ^2

/-- Strong convexity implies ordinary convexity.

    If f is Œº-strongly convex with Œº ‚â• 0, then f is convex on the whole space.
-/
theorem stronglyConvex_implies_convexOn (f : E ‚Üí ‚Ñù) (Œº : ‚Ñù) (hŒº : 0 ‚â§ Œº)
    (hStrong : IsStronglyConvex f Œº) : ConvexOn ‚Ñù Set.univ f := by
  constructor
  ¬∑ exact convex_univ
  ¬∑ intro x _ y _ a b ha hb hab
    -- ConvexOn uses weights a, b with a + b = 1
    -- IsStronglyConvex uses t with (1-t)
    -- We have: a ‚Ä¢ x + b ‚Ä¢ y where a + b = 1
    -- We want to show: f (a ‚Ä¢ x + b ‚Ä¢ y) ‚â§ a * f x + b * f y
    -- Using IsStronglyConvex with t = a:
    -- f (a ‚Ä¢ x + (1 - a) ‚Ä¢ y) ‚â§ a * f x + (1 - a) * f y - (Œº/2) * a * (1 - a) * ‚Äñx - y‚Äñ¬≤
    have hb_eq : b = 1 - a := by linarith
    rw [hb_eq]
    have ha_le_1 : a ‚â§ 1 := by linarith
    have h1_minus_a_nonneg : 0 ‚â§ 1 - a := by linarith
    have hStrong' := hStrong x y a ha ha_le_1
    have h_nonneg : 0 ‚â§ (Œº / 2) * a * (1 - a) * ‚Äñx - y‚Äñ^2 := by
      have h1 : 0 ‚â§ Œº / 2 := by linarith
      have h2 : 0 ‚â§ a * (1 - a) := mul_nonneg ha h1_minus_a_nonneg
      have h3 : 0 ‚â§ (Œº / 2) * (a * (1 - a)) := mul_nonneg h1 h2
      have h4 : 0 ‚â§ ‚Äñx - y‚Äñ ^ 2 := sq_nonneg _
      calc (Œº / 2) * a * (1 - a) * ‚Äñx - y‚Äñ^2
          = (Œº / 2) * (a * (1 - a)) * ‚Äñx - y‚Äñ^2 := by ring
        _ ‚â• 0 := mul_nonneg h3 h4
    -- Convert smul to mul for reals: a ‚Ä¢ r = a * r
    simp only [smul_eq_mul] at *
    linarith

/-- Strong convexity implies a lower bound on the gradient inner product.

    For Œº-strongly convex f with ‚àáf(x*) = 0:
    ‚ü®‚àáf(x), x - x*‚ü© ‚â• (Œº/2)‚Äñx - x*‚Äñ¬≤

    This follows from the first-order characterization of strong convexity.
-/
theorem strong_convex_gradient_lower_bound (f : E ‚Üí ‚Ñù) (Œº : ‚Ñù) (hŒº : 0 < Œº)
    (hStrong : IsStronglyConvex f Œº) (hDiff : Differentiable ‚Ñù f)
    (x x_star : E) (hMin : gradient f x_star = 0) :
    @inner ‚Ñù E _ (gradient f x) (x - x_star) ‚â• (Œº / 2) * ‚Äñx - x_star‚Äñ^2 := by
  /- The proof uses the first-order characterization of strong convexity.

     For Œº-strongly convex f, the definition gives:
     f(t¬∑a + (1-t)¬∑b) ‚â§ t¬∑f(a) + (1-t)¬∑f(b) - (Œº/2)¬∑t¬∑(1-t)¬∑‚Äña - b‚Äñ¬≤

     The first-order characterization (for differentiable f) is:
     f(y) ‚â• f(x) + ‚ü®‚àáf(x), y - x‚ü© + (Œº/2)¬∑‚Äñy - x‚Äñ¬≤

     Setting y = x* (where ‚àáf(x*) = 0):
     f(x*) ‚â• f(x) + ‚ü®‚àáf(x), x* - x‚ü© + (Œº/2)¬∑‚Äñx* - x‚Äñ¬≤

     Rearranging:
     ‚ü®‚àáf(x), x - x*‚ü© = -‚ü®‚àáf(x), x* - x‚ü© ‚â• f(x) - f(x*) + (Œº/2)¬∑‚Äñx - x*‚Äñ¬≤

     Since x* is a critical point (‚àáf(x*) = 0) for a strongly convex function,
     it's the unique global minimum, so f(x) - f(x*) ‚â• 0.

     Therefore: ‚ü®‚àáf(x), x - x*‚ü© ‚â• (Œº/2)¬∑‚Äñx - x*‚Äñ¬≤

     The key step requiring formalization is deriving the first-order
     characterization from the definition of IsStronglyConvex.
     This typically requires taking limits as t ‚Üí 0 in the definition.
  -/

  -- The formal proof requires:
  -- 1. Deriving first-order characterization from IsStronglyConvex
  -- 2. Using that ‚àáf(x*) = 0 implies x* is global minimum for strongly convex f
  -- 3. Combining the bounds

  -- Key derivation: From the strong convexity definition with a = x, b = x*, t ‚àà (0,1]:
  -- f(t‚Ä¢x + (1-t)‚Ä¢x*) ‚â§ t‚Ä¢f(x) + (1-t)‚Ä¢f(x*) - (Œº/2)‚Ä¢t‚Ä¢(1-t)‚Ä¢‚Äñx - x*‚Äñ¬≤
  --
  -- Rearranging: f(x* + t(x - x*)) ‚â§ f(x*) + t(f(x) - f(x*)) - (Œº/2)t(1-t)‚Äñx - x*‚Äñ¬≤
  --
  -- Taking the derivative w.r.t. t at t = 0 (using differentiability):
  -- LHS derivative: ‚ü®‚àáf(x*), x - x*‚ü© = 0 (since ‚àáf(x*) = 0)
  -- RHS derivative: f(x) - f(x*) - (Œº/2)(1)‚Äñx - x*‚Äñ¬≤ = f(x) - f(x*) - (Œº/2)‚Äñx - x*‚Äñ¬≤
  --
  -- Wait, this gives information at x*, not at x. Let me use a = x*, b = x instead:
  -- f(t‚Ä¢x* + (1-t)‚Ä¢x) ‚â§ t‚Ä¢f(x*) + (1-t)‚Ä¢f(x) - (Œº/2)‚Ä¢t‚Ä¢(1-t)‚Ä¢‚Äñx* - x‚Äñ¬≤
  --
  -- Rewrite LHS: f(x + t(x* - x)) = f(x - t(x - x*))
  --
  -- Taking derivative w.r.t. t at t = 0:
  -- LHS: ‚ü®‚àáf(x), x* - x‚ü© = -‚ü®‚àáf(x), x - x*‚ü©
  -- RHS: (d/dt)[t‚Ä¢f(x*) + (1-t)‚Ä¢f(x) - (Œº/2)‚Ä¢t‚Ä¢(1-t)‚Ä¢‚Äñx - x*‚Äñ¬≤] at t=0
  --     = f(x*) - f(x) - (Œº/2)‚Ä¢(1-2t)‚Ä¢‚Äñx - x*‚Äñ¬≤ at t=0
  --     = f(x*) - f(x) - (Œº/2)‚Äñx - x*‚Äñ¬≤
  --
  -- The strong convexity inequality gives: LHS ‚â§ RHS (as t ‚Üí 0‚Å∫)
  -- -‚ü®‚àáf(x), x - x*‚ü© ‚â§ f(x*) - f(x) - (Œº/2)‚Äñx - x*‚Äñ¬≤
  -- ‚ü®‚àáf(x), x - x*‚ü© ‚â• f(x) - f(x*) + (Œº/2)‚Äñx - x*‚Äñ¬≤
  --
  -- Since x* is a critical point of strongly convex f, it's the global minimum:
  -- f(x) - f(x*) ‚â• 0
  --
  -- Therefore: ‚ü®‚àáf(x), x - x*‚ü© ‚â• (Œº/2)‚Äñx - x*‚Äñ¬≤

  -- The formal proof requires taking limits as t ‚Üí 0 in the strong convexity
  -- definition and using differentiability. This involves:
  -- 1. Showing the function g(t) = f(x + t(x* - x)) is differentiable at t = 0
  -- 2. Computing g'(0) = ‚ü®‚àáf(x), x* - x‚ü©
  -- 3. Bounding g(t) using strong convexity
  -- 4. Taking the limit to get the first-order condition

  -- Define the direction and path
  let d := x_star - x
  let g := fun t : ‚Ñù => f (x + t ‚Ä¢ d)
  -- The upper bound from strong convexity: h(t) = (1-t)f(x) + tf(x*) - (Œº/2)t(1-t)‚Äñd‚Äñ¬≤
  let h := fun t : ‚Ñù => (1 - t) * f x + t * f x_star - (Œº / 2) * t * (1 - t) * ‚Äñd‚Äñ^2
  -- Strong convexity gives g(t) ‚â§ h(t) for t ‚àà [0, 1]
  have h_ineq : ‚àÄ t, 0 ‚â§ t ‚Üí t ‚â§ 1 ‚Üí g t ‚â§ h t := by
    intro t ht0 ht1
    have hconv := hStrong x_star x t ht0 ht1
    -- t‚Ä¢x* + (1-t)‚Ä¢x = x + t‚Ä¢(x* - x) = x + t‚Ä¢d
    have heq : t ‚Ä¢ x_star + (1 - t) ‚Ä¢ x = x + t ‚Ä¢ d := by
      simp only [d]; rw [smul_sub]; ring_nf; module
    simp only [g, h, heq] at hconv ‚ä¢
    have hnorm : ‚Äñx_star - x‚Äñ = ‚Äñd‚Äñ := by simp only [d]
    rw [hnorm] at hconv
    linarith
  -- At t = 0: g(0) = h(0) = f(x)
  have hg0 : g 0 = f x := by simp only [g, zero_smul, add_zero]
  have hh0 : h 0 = f x := by simp only [h]; ring
  -- Compute h'(0) = f(x*) - f(x) - (Œº/2)‚Äñd‚Äñ¬≤
  have h_deriv : HasDerivAt h (f x_star - f x - (Œº / 2) * ‚Äñd‚Äñ^2) 0 := by
    -- h(t) = (1-t)f(x) + tf(x*) - (Œº/2)t(1-t)‚Äñd‚Äñ¬≤
    -- Rewrite as: h(t) = f(x) + t*(f(x*) - f(x)) - (Œº/2)*‚Äñd‚Äñ¬≤*(t - t¬≤)
    -- h'(t) = f(x*) - f(x) - (Œº/2)*‚Äñd‚Äñ¬≤*(1 - 2t)
    -- h'(0) = f(x*) - f(x) - (Œº/2)*‚Äñd‚Äñ¬≤
    have h1 : HasDerivAt (fun t : ‚Ñù => (1 - t) * f x) (-f x) 0 := by
      have hid : HasDerivAt (fun t : ‚Ñù => 1 - t) (-1) 0 :=
        (hasDerivAt_const (0 : ‚Ñù) (1 : ‚Ñù)).sub (hasDerivAt_id (0 : ‚Ñù)) |>.congr_deriv (by ring)
      convert hid.mul_const (f x) using 1; ring
    have h2 : HasDerivAt (fun t : ‚Ñù => t * f x_star) (f x_star) 0 := by
      convert (hasDerivAt_id (0 : ‚Ñù)).mul_const (f x_star) using 1; ring
    have h3 : HasDerivAt (fun t : ‚Ñù => (Œº / 2) * t * (1 - t) * ‚Äñd‚Äñ^2) ((Œº / 2) * ‚Äñd‚Äñ^2) 0 := by
      -- (Œº/2)*t*(1-t)*‚Äñd‚Äñ¬≤ has derivative (Œº/2)*‚Äñd‚Äñ¬≤*(1 - 2t) at t
      -- At t = 0: (Œº/2)*‚Äñd‚Äñ¬≤
      have hpoly : HasDerivAt (fun t : ‚Ñù => t * (1 - t)) 1 0 := by
        have h1' := hasDerivAt_id (0 : ‚Ñù)
        have h2' : HasDerivAt (fun t : ‚Ñù => 1 - t) (-1) 0 :=
          (hasDerivAt_const (0 : ‚Ñù) (1 : ‚Ñù)).sub (hasDerivAt_id (0 : ‚Ñù)) |>.congr_deriv (by ring)
        have hprod := h1'.mul h2'
        convert hprod using 2 <;> simp
      convert hpoly.const_mul ((Œº / 2) * ‚Äñd‚Äñ^2) using 1
      ¬∑ ext t; ring
      ¬∑ ring
    convert (h1.add h2).sub h3 using 1; ring
  -- Compute g'(0) = ‚ü®‚àáf(x), d‚ü©
  have g_deriv : HasDerivAt g (@inner ‚Ñù E _ (gradient f x) d) 0 := by
    have hŒ≥ : HasDerivAt (fun t : ‚Ñù => x + t ‚Ä¢ d) d 0 := by
      have h1 : HasDerivAt (fun _ : ‚Ñù => x) 0 0 := hasDerivAt_const 0 x
      have h2 : HasDerivAt (fun t : ‚Ñù => t ‚Ä¢ d) ((1 : ‚Ñù) ‚Ä¢ d) 0 :=
        (hasDerivAt_id 0).smul_const d
      have hsum := h1.add h2
      simp only [zero_add, one_smul] at hsum
      exact hsum
    have hf_grad : HasGradientAt f (gradient f x) x := (hDiff x).hasGradientAt
    have hf_fderiv : HasFDerivAt f (innerSL (ùïú := ‚Ñù) (gradient f x)) x := hf_grad.hasFDerivAt
    have hf_fderiv' : HasFDerivAt f (innerSL (ùïú := ‚Ñù) (gradient f x)) (x + (0 : ‚Ñù) ‚Ä¢ d) := by
      simp only [zero_smul, add_zero]; exact hf_fderiv
    have hcomp := hf_fderiv'.comp_hasDerivAt (0 : ‚Ñù) hŒ≥
    simp only [Function.comp_apply, innerSL_apply_apply, zero_smul, add_zero] at hcomp
    exact hcomp
  -- Key lemma: if g(0) = h(0) and g(t) ‚â§ h(t) for t ‚àà (0, 1], then g'(0) ‚â§ h'(0)
  -- This follows from: (g(t) - g(0))/t ‚â§ (h(t) - h(0))/t for t > 0
  -- Taking limit as t ‚Üí 0‚Å∫ gives g'(0) ‚â§ h'(0)
  have h_deriv_ineq : @inner ‚Ñù E _ (gradient f x) d ‚â§ f x_star - f x - (Œº / 2) * ‚Äñd‚Äñ^2 := by
    by_contra hcontra
    push_neg at hcontra
    -- Let Œ¥ = g'(0) - h'(0) > 0
    let Œ¥ := @inner ‚Ñù E _ (gradient f x) d - (f x_star - f x - (Œº / 2) * ‚Äñd‚Äñ^2)
    have hŒ¥_pos : Œ¥ > 0 := by simp only [Œ¥]; linarith
    -- From HasDerivAt, the difference quotient converges to the derivative
    -- For g: (g(t) - g(0))/t ‚Üí g'(0) as t ‚Üí 0
    -- For h: (h(t) - h(0))/t ‚Üí h'(0) as t ‚Üí 0
    -- So (g(t) - h(t))/t ‚Üí g'(0) - h'(0) = Œ¥ > 0
    have h_gh_deriv : HasDerivAt (fun t => g t - h t) Œ¥ 0 := HasDerivAt.sub g_deriv h_deriv
    have h_gh_0 : (fun t => g t - h t) 0 = 0 := by simp only [hg0, hh0, sub_self]
    -- HasDerivAt gives: (g-h)(t) = (g-h)(0) + Œ¥*t + o(t) = Œ¥*t + o(t)
    -- For small t > 0: (g-h)(t) ‚âà Œ¥*t > 0 since Œ¥ > 0
    rw [hasDerivAt_iff_isLittleO] at h_gh_deriv
    -- h_gh_deriv : (fun t => (g-h)(0+t) - (g-h)(0) - (t-0)‚Ä¢Œ¥) =o[ùìù 0] (fun t => t-0)
    -- Use IsLittleO.def to get: for c = Œ¥/2 > 0, eventually ‚Äñ...‚Äñ ‚â§ c * ‚Äñt - 0‚Äñ
    have hŒµ_half : 0 < Œ¥ / 2 := by linarith
    have h_bound_evt := h_gh_deriv.def hŒµ_half
    -- h_bound_evt : ‚àÄ·∂† t in ùìù 0, ‚Äñ(g t - h t) - (g 0 - h 0) - (t - 0) ‚Ä¢ Œ¥‚Äñ ‚â§ (Œ¥/2) * ‚Äñt - 0‚Äñ
    simp only [h_gh_0, sub_zero, smul_eq_mul] at h_bound_evt
    -- h_bound_evt : ‚àÄ·∂† t in ùìù 0, ‚Äñg t - h t - t * Œ¥‚Äñ ‚â§ (Œ¥/2) * ‚Äñt‚Äñ
    rw [Filter.eventually_iff_exists_mem] at h_bound_evt
    obtain ‚ü®s, hs_mem, hs_bound‚ü© := h_bound_evt
    rw [Metric.mem_nhds_iff] at hs_mem
    obtain ‚ü®Œµ, hŒµ_pos, hŒµ_sub‚ü© := hs_mem
    -- Pick t = min(Œµ/2, 1/2) > 0
    let t := min (Œµ / 2) (1 / 2)
    have ht_pos : 0 < t := by positivity
    have ht_lt_Œµ : t < Œµ := by simp only [t]; linarith [min_le_left (Œµ / 2) (1 / 2)]
    have ht_le_1 : t ‚â§ 1 := by simp only [t]; linarith [min_le_right (Œµ / 2) (1 / 2)]
    have ht_in_ball : t ‚àà Metric.ball 0 Œµ := by
      simp only [Metric.mem_ball, dist_zero_right, Real.norm_eq_abs, abs_of_pos ht_pos]
      exact ht_lt_Œµ
    have ht_in_s : t ‚àà s := hŒµ_sub ht_in_ball
    -- hs_bound says: ‚Äñ(g-h)(t) - t*Œ¥‚Äñ ‚â§ (Œ¥/2) * ‚Äñt‚Äñ
    have h_bound := hs_bound t ht_in_s
    simp only [Real.norm_eq_abs, abs_of_pos ht_pos] at h_bound
    -- h_bound : |g t - h t - t * Œ¥| ‚â§ (Œ¥ / 2) * t
    -- |f(t) - t*Œ¥| ‚â§ (Œ¥/2)*t means f(t) ‚â• t*Œ¥ - (Œ¥/2)*t = (Œ¥/2)*t > 0
    have h_lower : g t - h t ‚â• t * Œ¥ - (Œ¥ / 2) * t := by
      have h1 : -((Œ¥ / 2) * t) ‚â§ (g t - h t) - t * Œ¥ := by
        have := neg_abs_le (g t - h t - t * Œ¥)
        linarith
      linarith
    have h_diff_pos : g t - h t > 0 := by
      have : t * Œ¥ - (Œ¥ / 2) * t = (Œ¥ / 2) * t := by ring
      rw [this] at h_lower
      have : (Œ¥ / 2) * t > 0 := mul_pos (by linarith) ht_pos
      linarith
    -- But h_ineq says g(t) ‚â§ h(t), contradiction
    have h_le := h_ineq t (le_of_lt ht_pos) ht_le_1
    linarith
  -- Now: ‚ü®‚àáf(x), d‚ü© ‚â§ f(x*) - f(x) - (Œº/2)‚Äñd‚Äñ¬≤
  -- Since d = x* - x, we have ‚ü®‚àáf(x), x - x*‚ü© = -‚ü®‚àáf(x), d‚ü©
  have h_inner_neg : @inner ‚Ñù E _ (gradient f x) (x - x_star) =
      -@inner ‚Ñù E _ (gradient f x) d := by
    simp only [d, ‚Üê inner_neg_right, neg_sub]
  rw [h_inner_neg]
  -- Need: -‚ü®‚àáf(x), d‚ü© ‚â• (Œº/2)‚Äñx - x*‚Äñ¬≤
  -- From h_deriv_ineq: ‚ü®‚àáf(x), d‚ü© ‚â§ f(x*) - f(x) - (Œº/2)‚Äñd‚Äñ¬≤
  -- So: -‚ü®‚àáf(x), d‚ü© ‚â• f(x) - f(x*) + (Œº/2)‚Äñd‚Äñ¬≤
  -- Need to show f(x) - f(x*) ‚â• 0, i.e., x* is global minimum
  have h_min : f x_star ‚â§ f x := by
    -- Use derivative limit argument at x* with ‚àáf(x*) = 0
    -- Define path from x* to x: p(t) = f(x* + t(x - x*))
    -- Strong convexity gives p(t) ‚â§ RHS, and taking derivative limit at t = 0
    -- with p'(0) = ‚ü®‚àáf(x*), x - x*‚ü© = 0 gives the desired inequality.
    let e := x - x_star
    let p := fun t : ‚Ñù => f (x_star + t ‚Ä¢ e)
    let q := fun t : ‚Ñù => t * f x + (1 - t) * f x_star - (Œº / 2) * t * (1 - t) * ‚Äñe‚Äñ^2
    -- Strong convexity gives p(t) ‚â§ q(t) for t ‚àà [0, 1]
    have hpq_ineq : ‚àÄ t, 0 ‚â§ t ‚Üí t ‚â§ 1 ‚Üí p t ‚â§ q t := by
      intro t ht0 ht1
      have hconv := hStrong x x_star t ht0 ht1
      have heq : t ‚Ä¢ x + (1 - t) ‚Ä¢ x_star = x_star + t ‚Ä¢ e := by
        simp only [e]; rw [smul_sub]; ring_nf; module
      simp only [p, q, heq] at hconv ‚ä¢
      have hnorm : ‚Äñx - x_star‚Äñ = ‚Äñe‚Äñ := by simp only [e]
      rw [hnorm] at hconv
      linarith
    -- At t = 0: p(0) = q(0) = f(x*)
    have hp0 : p 0 = f x_star := by simp only [p, zero_smul, add_zero]
    have hq0 : q 0 = f x_star := by simp only [q]; ring
    -- Compute q'(0) = f(x) - f(x*) - (Œº/2)‚Äñe‚Äñ¬≤
    have q_deriv : HasDerivAt q (f x - f x_star - (Œº / 2) * ‚Äñe‚Äñ^2) 0 := by
      have h1 : HasDerivAt (fun t : ‚Ñù => t * f x) (f x) 0 := by
        convert (hasDerivAt_id (0 : ‚Ñù)).mul_const (f x) using 1; ring
      have h2 : HasDerivAt (fun t : ‚Ñù => (1 - t) * f x_star) (-f x_star) 0 := by
        have hid : HasDerivAt (fun t : ‚Ñù => 1 - t) (-1) 0 :=
          (hasDerivAt_const (0 : ‚Ñù) (1 : ‚Ñù)).sub (hasDerivAt_id (0 : ‚Ñù)) |>.congr_deriv (by ring)
        convert hid.mul_const (f x_star) using 1; ring
      have h3 : HasDerivAt (fun t : ‚Ñù => (Œº / 2) * t * (1 - t) * ‚Äñe‚Äñ^2) ((Œº / 2) * ‚Äñe‚Äñ^2) 0 := by
        have hpoly : HasDerivAt (fun t : ‚Ñù => t * (1 - t)) 1 0 := by
          have ha : HasDerivAt (fun t : ‚Ñù => t) 1 0 := hasDerivAt_id (0 : ‚Ñù)
          have hb : HasDerivAt (fun t : ‚Ñù => 1 - t) (-1) 0 :=
            (hasDerivAt_const (0 : ‚Ñù) (1 : ‚Ñù)).sub (hasDerivAt_id (0 : ‚Ñù)) |>.congr_deriv (by ring)
          exact (ha.mul hb).congr_deriv (by simp [id])
        convert hpoly.const_mul ((Œº / 2) * ‚Äñe‚Äñ^2) using 1
        ¬∑ ext t; ring
        ¬∑ ring
      convert (h1.add h2).sub h3 using 1 <;> ring
    -- Compute p'(0) = ‚ü®‚àáf(x*), e‚ü© = 0 (since ‚àáf(x*) = 0)
    have p_deriv : HasDerivAt p 0 0 := by
      have hŒ≥ : HasDerivAt (fun t : ‚Ñù => x_star + t ‚Ä¢ e) e 0 := by
        have h1 : HasDerivAt (fun _ : ‚Ñù => x_star) 0 0 := hasDerivAt_const 0 x_star
        have h2 : HasDerivAt (fun t : ‚Ñù => t ‚Ä¢ e) ((1 : ‚Ñù) ‚Ä¢ e) 0 :=
          (hasDerivAt_id 0).smul_const e
        have hsum := h1.add h2
        simp only [zero_add, one_smul] at hsum
        exact hsum
      have hf_grad : HasGradientAt f (gradient f x_star) x_star := (hDiff x_star).hasGradientAt
      have hf_fderiv : HasFDerivAt f (innerSL (ùïú := ‚Ñù) (gradient f x_star)) x_star :=
        hf_grad.hasFDerivAt
      have hf_fderiv' : HasFDerivAt f (innerSL (ùïú := ‚Ñù) (gradient f x_star)) (x_star + (0 : ‚Ñù) ‚Ä¢ e) := by
        simp only [zero_smul, add_zero]; exact hf_fderiv
      have hcomp := hf_fderiv'.comp_hasDerivAt (0 : ‚Ñù) hŒ≥
      simp only [Function.comp_apply, innerSL_apply_apply, zero_smul, add_zero, hMin, inner_zero_left] at hcomp
      exact hcomp
    -- Key: if p(0) = q(0), p(t) ‚â§ q(t) for t > 0, and both differentiable at 0, then p'(0) ‚â§ q'(0)
    have hderiv_ineq : 0 ‚â§ f x - f x_star - (Œº / 2) * ‚Äñe‚Äñ^2 := by
      by_contra hcontra
      push_neg at hcontra
      -- Let Œ¥ = p'(0) - q'(0) = 0 - q'(0) = -(f x - f x_star - (Œº/2)‚Äñe‚Äñ¬≤) > 0
      let Œ¥ := -(f x - f x_star - (Œº / 2) * ‚Äñe‚Äñ^2)
      have hŒ¥_pos : Œ¥ > 0 := by simp only [Œ¥]; linarith
      have h_pq_deriv : HasDerivAt (fun t => p t - q t) Œ¥ 0 := by
        have := HasDerivAt.sub p_deriv q_deriv
        convert this using 2
        simp only [Œ¥]; ring
      have h_pq_0 : (fun t => p t - q t) 0 = 0 := by simp only [hp0, hq0, sub_self]
      -- Use isLittleO characterization instead of tendsto_slope (which gives nhdsWithin)
      rw [hasDerivAt_iff_isLittleO] at h_pq_deriv
      have hŒµ_half : 0 < Œ¥ / 2 := by linarith
      have h_bound_evt := h_pq_deriv.def hŒµ_half
      simp only [h_pq_0, sub_zero, smul_eq_mul] at h_bound_evt
      rw [Filter.eventually_iff_exists_mem] at h_bound_evt
      obtain ‚ü®s, hs_mem, hs_bound‚ü© := h_bound_evt
      rw [Metric.mem_nhds_iff] at hs_mem
      obtain ‚ü®Œµ, hŒµ_pos, hŒµ_sub‚ü© := hs_mem
      let t := min (Œµ / 2) (1 / 2)
      have ht_pos : 0 < t := by positivity
      have ht_lt_Œµ : t < Œµ := by simp only [t]; linarith [min_le_left (Œµ / 2) (1 / 2)]
      have ht_le_1 : t ‚â§ 1 := by simp only [t]; linarith [min_le_right (Œµ / 2) (1 / 2)]
      have ht_in_ball : t ‚àà Metric.ball 0 Œµ := by
        simp only [Metric.mem_ball, dist_zero_right, Real.norm_eq_abs, abs_of_pos ht_pos]
        exact ht_lt_Œµ
      have ht_in_s : t ‚àà s := hŒµ_sub ht_in_ball
      have h_bound := hs_bound t ht_in_s
      simp only [Real.norm_eq_abs, abs_of_pos ht_pos] at h_bound
      -- h_bound : ‚Äñp t - q t - t * Œ¥‚Äñ ‚â§ (Œ¥/2) * t
      -- This means: -(Œ¥/2)*t ‚â§ (p t - q t) - t*Œ¥ ‚â§ (Œ¥/2)*t
      -- So: t*Œ¥ - (Œ¥/2)*t ‚â§ p t - q t, i.e., (Œ¥/2)*t ‚â§ p t - q t
      have h_lower : p t - q t ‚â• t * Œ¥ - (Œ¥ / 2) * t := by
        have h1 : -((Œ¥ / 2) * t) ‚â§ (p t - q t) - t * Œ¥ := by
          have := neg_abs_le (p t - q t - t * Œ¥)
          linarith
        linarith
      have h_diff_pos : p t - q t > 0 := by
        have : t * Œ¥ - (Œ¥ / 2) * t = (Œ¥ / 2) * t := by ring
        rw [this] at h_lower
        have : (Œ¥ / 2) * t > 0 := mul_pos (by linarith) ht_pos
        linarith
      have h_le := hpq_ineq t (le_of_lt ht_pos) ht_le_1
      linarith
    -- From 0 ‚â§ f(x) - f(x*) - (Œº/2)‚Äñe‚Äñ¬≤, we get f(x*) ‚â§ f(x) - (Œº/2)‚Äñe‚Äñ¬≤ ‚â§ f(x)
    have h_e_sq_nonneg : 0 ‚â§ (Œº / 2) * ‚Äñe‚Äñ^2 := by positivity
    linarith
  have h_d_norm : ‚Äñd‚Äñ = ‚Äñx - x_star‚Äñ := by simp only [d, norm_sub_rev]
  rw [h_d_norm] at h_deriv_ineq
  linarith

/-- Gradient monotonicity for strongly convex functions (full Œº, not Œº/2).

    For Œº-strongly convex f with ‚àáf(x*) = 0:
    ‚ü®‚àáf(x), x - x*‚ü© ‚â• Œº‚Äñx - x*‚Äñ¬≤

    This is twice as strong as `strong_convex_gradient_lower_bound` and comes from
    adding the first-order conditions at both x and x*.

    Proof:
    1. First-order at x: ‚ü®‚àáf(x), x* - x‚ü© ‚â§ f(x*) - f(x) - (Œº/2)‚Äñx - x*‚Äñ¬≤
       ‚Üí ‚ü®‚àáf(x), x - x*‚ü© ‚â• f(x) - f(x*) + (Œº/2)‚Äñx - x*‚Äñ¬≤
    2. First-order at x* with ‚àáf(x*) = 0: 0 ‚â§ f(x) - f(x*) - (Œº/2)‚Äñx - x*‚Äñ¬≤
       ‚Üí f(x) - f(x*) ‚â• (Œº/2)‚Äñx - x*‚Äñ¬≤
    3. Combining: ‚ü®‚àáf(x), x - x*‚ü© ‚â• (Œº/2)‚Äñx - x*‚Äñ¬≤ + (Œº/2)‚Äñx - x*‚Äñ¬≤ = Œº‚Äñx - x*‚Äñ¬≤
-/
theorem strong_convex_gradient_monotonicity (f : E ‚Üí ‚Ñù) (Œº : ‚Ñù) (hŒº : 0 < Œº)
    (hStrong : IsStronglyConvex f Œº) (hDiff : Differentiable ‚Ñù f)
    (x x_star : E) (hMin : gradient f x_star = 0) :
    @inner ‚Ñù E _ (gradient f x) (x - x_star) ‚â• Œº * ‚Äñx - x_star‚Äñ^2 := by
  -- The proof combines two first-order conditions from strong convexity.
  -- Define d = x* - x and e = x - x*
  let d := x_star - x
  let e := x - x_star

  -- Part 1: First-order condition at x gives:
  -- ‚ü®‚àáf(x), x - x*‚ü© ‚â• f(x) - f(x*) + (Œº/2)‚Äñx - x*‚Äñ¬≤
  -- (This is derived in strong_convex_gradient_lower_bound as h_deriv_ineq)

  -- We'll derive both bounds together using the same technique.

  -- Step A: Derive ‚ü®‚àáf(x), d‚ü© ‚â§ f(x*) - f(x) - (Œº/2)‚Äñd‚Äñ¬≤ via derivative limit
  let g := fun t : ‚Ñù => f (x + t ‚Ä¢ d)
  let h := fun t : ‚Ñù => (1 - t) * f x + t * f x_star - (Œº / 2) * t * (1 - t) * ‚Äñd‚Äñ^2
  have h_ineq : ‚àÄ t, 0 ‚â§ t ‚Üí t ‚â§ 1 ‚Üí g t ‚â§ h t := by
    intro t ht0 ht1
    have hconv := hStrong x_star x t ht0 ht1
    have heq : t ‚Ä¢ x_star + (1 - t) ‚Ä¢ x = x + t ‚Ä¢ d := by
      simp only [d]; rw [smul_sub]; ring_nf; module
    simp only [g, h, heq] at hconv ‚ä¢
    have hnorm : ‚Äñx_star - x‚Äñ = ‚Äñd‚Äñ := by simp only [d]
    rw [hnorm] at hconv
    linarith
  have hg0 : g 0 = f x := by simp only [g, zero_smul, add_zero]
  have hh0 : h 0 = f x := by simp only [h]; ring
  have h_deriv : HasDerivAt h (f x_star - f x - (Œº / 2) * ‚Äñd‚Äñ^2) 0 := by
    have h1 : HasDerivAt (fun t : ‚Ñù => (1 - t) * f x) (-f x) 0 := by
      have hid : HasDerivAt (fun t : ‚Ñù => 1 - t) (-1) 0 :=
        (hasDerivAt_const (0 : ‚Ñù) (1 : ‚Ñù)).sub (hasDerivAt_id (0 : ‚Ñù)) |>.congr_deriv (by ring)
      convert hid.mul_const (f x) using 1; ring
    have h2 : HasDerivAt (fun t : ‚Ñù => t * f x_star) (f x_star) 0 := by
      convert (hasDerivAt_id (0 : ‚Ñù)).mul_const (f x_star) using 1; ring
    have h3 : HasDerivAt (fun t : ‚Ñù => (Œº / 2) * t * (1 - t) * ‚Äñd‚Äñ^2) ((Œº / 2) * ‚Äñd‚Äñ^2) 0 := by
      have hpoly : HasDerivAt (fun t : ‚Ñù => t * (1 - t)) 1 0 := by
        have h1' := hasDerivAt_id (0 : ‚Ñù)
        have h2' : HasDerivAt (fun t : ‚Ñù => 1 - t) (-1) 0 :=
          (hasDerivAt_const (0 : ‚Ñù) (1 : ‚Ñù)).sub (hasDerivAt_id (0 : ‚Ñù)) |>.congr_deriv (by ring)
        have hprod := h1'.mul h2'
        convert hprod using 2 <;> simp
      convert hpoly.const_mul ((Œº / 2) * ‚Äñd‚Äñ^2) using 1
      ¬∑ ext t; ring
      ¬∑ ring
    convert (h1.add h2).sub h3 using 1; ring
  have g_deriv : HasDerivAt g (@inner ‚Ñù E _ (gradient f x) d) 0 := by
    have hŒ≥ : HasDerivAt (fun t : ‚Ñù => x + t ‚Ä¢ d) d 0 := by
      have h1 : HasDerivAt (fun _ : ‚Ñù => x) 0 0 := hasDerivAt_const 0 x
      have h2 : HasDerivAt (fun t : ‚Ñù => t ‚Ä¢ d) ((1 : ‚Ñù) ‚Ä¢ d) 0 :=
        (hasDerivAt_id 0).smul_const d
      have hsum := h1.add h2
      simp only [zero_add, one_smul] at hsum
      exact hsum
    have hf_grad : HasGradientAt f (gradient f x) x := (hDiff x).hasGradientAt
    have hf_fderiv : HasFDerivAt f (innerSL (ùïú := ‚Ñù) (gradient f x)) x := hf_grad.hasFDerivAt
    have hf_fderiv' : HasFDerivAt f (innerSL (ùïú := ‚Ñù) (gradient f x)) (x + (0 : ‚Ñù) ‚Ä¢ d) := by
      simp only [zero_smul, add_zero]; exact hf_fderiv
    have hcomp := hf_fderiv'.comp_hasDerivAt (0 : ‚Ñù) hŒ≥
    simp only [Function.comp_apply, innerSL_apply_apply, zero_smul, add_zero] at hcomp
    exact hcomp
  -- Derivative limit argument: g(0) = h(0), g ‚â§ h on (0,1], so g'(0) ‚â§ h'(0)
  have h_deriv_ineq : @inner ‚Ñù E _ (gradient f x) d ‚â§ f x_star - f x - (Œº / 2) * ‚Äñd‚Äñ^2 := by
    by_contra hcontra
    push_neg at hcontra
    let Œ¥ := @inner ‚Ñù E _ (gradient f x) d - (f x_star - f x - (Œº / 2) * ‚Äñd‚Äñ^2)
    have hŒ¥_pos : Œ¥ > 0 := by simp only [Œ¥]; linarith
    have h_gh_deriv : HasDerivAt (fun t => g t - h t) Œ¥ 0 := HasDerivAt.sub g_deriv h_deriv
    have h_gh_0 : (fun t => g t - h t) 0 = 0 := by simp only [hg0, hh0, sub_self]
    rw [hasDerivAt_iff_isLittleO] at h_gh_deriv
    have hŒµ_half : 0 < Œ¥ / 2 := by linarith
    have h_bound_evt := h_gh_deriv.def hŒµ_half
    simp only [h_gh_0, sub_zero, smul_eq_mul] at h_bound_evt
    rw [Filter.eventually_iff_exists_mem] at h_bound_evt
    obtain ‚ü®s, hs_mem, hs_bound‚ü© := h_bound_evt
    rw [Metric.mem_nhds_iff] at hs_mem
    obtain ‚ü®Œµ, hŒµ_pos, hŒµ_sub‚ü© := hs_mem
    let t := min (Œµ / 2) (1 / 2)
    have ht_pos : 0 < t := by positivity
    have ht_lt_Œµ : t < Œµ := by simp only [t]; linarith [min_le_left (Œµ / 2) (1 / 2)]
    have ht_le_1 : t ‚â§ 1 := by simp only [t]; linarith [min_le_right (Œµ / 2) (1 / 2)]
    have ht_in_ball : t ‚àà Metric.ball 0 Œµ := by
      simp only [Metric.mem_ball, dist_zero_right, Real.norm_eq_abs, abs_of_pos ht_pos]
      exact ht_lt_Œµ
    have ht_in_s : t ‚àà s := hŒµ_sub ht_in_ball
    have h_bound := hs_bound t ht_in_s
    simp only [Real.norm_eq_abs, abs_of_pos ht_pos] at h_bound
    have h_lower : g t - h t ‚â• t * Œ¥ - (Œ¥ / 2) * t := by
      have h1 : -((Œ¥ / 2) * t) ‚â§ (g t - h t) - t * Œ¥ := by
        have := neg_abs_le (g t - h t - t * Œ¥)
        linarith
      linarith
    have h_diff_pos : g t - h t > 0 := by
      have : t * Œ¥ - (Œ¥ / 2) * t = (Œ¥ / 2) * t := by ring
      rw [this] at h_lower
      have : (Œ¥ / 2) * t > 0 := mul_pos (by linarith) ht_pos
      linarith
    have h_le := h_ineq t (le_of_lt ht_pos) ht_le_1
    linarith

  -- Step B: Derive f(x) - f(x*) ‚â• (Œº/2)‚Äñe‚Äñ¬≤ via derivative limit at x*
  let p := fun t : ‚Ñù => f (x_star + t ‚Ä¢ e)
  let q := fun t : ‚Ñù => t * f x + (1 - t) * f x_star - (Œº / 2) * t * (1 - t) * ‚Äñe‚Äñ^2
  have hpq_ineq : ‚àÄ t, 0 ‚â§ t ‚Üí t ‚â§ 1 ‚Üí p t ‚â§ q t := by
    intro t ht0 ht1
    have hconv := hStrong x x_star t ht0 ht1
    have heq : t ‚Ä¢ x + (1 - t) ‚Ä¢ x_star = x_star + t ‚Ä¢ e := by
      simp only [e]; rw [smul_sub]; ring_nf; module
    simp only [p, q, heq] at hconv ‚ä¢
    have hnorm : ‚Äñx - x_star‚Äñ = ‚Äñe‚Äñ := by simp only [e]
    rw [hnorm] at hconv
    linarith
  have hp0 : p 0 = f x_star := by simp only [p, zero_smul, add_zero]
  have hq0 : q 0 = f x_star := by simp only [q]; ring
  have q_deriv : HasDerivAt q (f x - f x_star - (Œº / 2) * ‚Äñe‚Äñ^2) 0 := by
    have h1 : HasDerivAt (fun t : ‚Ñù => t * f x) (f x) 0 := by
      convert (hasDerivAt_id (0 : ‚Ñù)).mul_const (f x) using 1; ring
    have h2 : HasDerivAt (fun t : ‚Ñù => (1 - t) * f x_star) (-f x_star) 0 := by
      have hid : HasDerivAt (fun t : ‚Ñù => 1 - t) (-1) 0 :=
        (hasDerivAt_const (0 : ‚Ñù) (1 : ‚Ñù)).sub (hasDerivAt_id (0 : ‚Ñù)) |>.congr_deriv (by ring)
      convert hid.mul_const (f x_star) using 1; ring
    have h3 : HasDerivAt (fun t : ‚Ñù => (Œº / 2) * t * (1 - t) * ‚Äñe‚Äñ^2) ((Œº / 2) * ‚Äñe‚Äñ^2) 0 := by
      have hpoly : HasDerivAt (fun t : ‚Ñù => t * (1 - t)) 1 0 := by
        have ha : HasDerivAt (fun t : ‚Ñù => t) 1 0 := hasDerivAt_id (0 : ‚Ñù)
        have hb : HasDerivAt (fun t : ‚Ñù => 1 - t) (-1) 0 :=
          (hasDerivAt_const (0 : ‚Ñù) (1 : ‚Ñù)).sub (hasDerivAt_id (0 : ‚Ñù)) |>.congr_deriv (by ring)
        exact (ha.mul hb).congr_deriv (by simp [id])
      convert hpoly.const_mul ((Œº / 2) * ‚Äñe‚Äñ^2) using 1
      ¬∑ ext t; ring
      ¬∑ ring
    convert (h1.add h2).sub h3 using 1 <;> ring
  have p_deriv : HasDerivAt p 0 0 := by
    have hŒ≥ : HasDerivAt (fun t : ‚Ñù => x_star + t ‚Ä¢ e) e 0 := by
      have h1 : HasDerivAt (fun _ : ‚Ñù => x_star) 0 0 := hasDerivAt_const 0 x_star
      have h2 : HasDerivAt (fun t : ‚Ñù => t ‚Ä¢ e) ((1 : ‚Ñù) ‚Ä¢ e) 0 :=
        (hasDerivAt_id 0).smul_const e
      have hsum := h1.add h2
      simp only [zero_add, one_smul] at hsum
      exact hsum
    have hf_grad : HasGradientAt f (gradient f x_star) x_star := (hDiff x_star).hasGradientAt
    have hf_fderiv : HasFDerivAt f (innerSL (ùïú := ‚Ñù) (gradient f x_star)) x_star :=
      hf_grad.hasFDerivAt
    have hf_fderiv' : HasFDerivAt f (innerSL (ùïú := ‚Ñù) (gradient f x_star)) (x_star + (0 : ‚Ñù) ‚Ä¢ e) := by
      simp only [zero_smul, add_zero]; exact hf_fderiv
    have hcomp := hf_fderiv'.comp_hasDerivAt (0 : ‚Ñù) hŒ≥
    simp only [Function.comp_apply, innerSL_apply_apply, zero_smul, add_zero, hMin, inner_zero_left] at hcomp
    exact hcomp
  -- p'(0) = 0 ‚â§ q'(0) = f(x) - f(x*) - (Œº/2)‚Äñe‚Äñ¬≤ gives f(x) - f(x*) ‚â• (Œº/2)‚Äñe‚Äñ¬≤
  have h_func_bound : 0 ‚â§ f x - f x_star - (Œº / 2) * ‚Äñe‚Äñ^2 := by
    by_contra hcontra
    push_neg at hcontra
    let Œ¥ := -(f x - f x_star - (Œº / 2) * ‚Äñe‚Äñ^2)
    have hŒ¥_pos : Œ¥ > 0 := by simp only [Œ¥]; linarith
    have h_pq_deriv : HasDerivAt (fun t => p t - q t) Œ¥ 0 := by
      have := HasDerivAt.sub p_deriv q_deriv
      convert this using 2
      simp only [Œ¥]; ring
    have h_pq_0 : (fun t => p t - q t) 0 = 0 := by simp only [hp0, hq0, sub_self]
    rw [hasDerivAt_iff_isLittleO] at h_pq_deriv
    have hŒµ_half : 0 < Œ¥ / 2 := by linarith
    have h_bound_evt := h_pq_deriv.def hŒµ_half
    simp only [h_pq_0, sub_zero, smul_eq_mul] at h_bound_evt
    rw [Filter.eventually_iff_exists_mem] at h_bound_evt
    obtain ‚ü®s, hs_mem, hs_bound‚ü© := h_bound_evt
    rw [Metric.mem_nhds_iff] at hs_mem
    obtain ‚ü®Œµ, hŒµ_pos, hŒµ_sub‚ü© := hs_mem
    let t := min (Œµ / 2) (1 / 2)
    have ht_pos : 0 < t := by positivity
    have ht_lt_Œµ : t < Œµ := by simp only [t]; linarith [min_le_left (Œµ / 2) (1 / 2)]
    have ht_le_1 : t ‚â§ 1 := by simp only [t]; linarith [min_le_right (Œµ / 2) (1 / 2)]
    have ht_in_ball : t ‚àà Metric.ball 0 Œµ := by
      simp only [Metric.mem_ball, dist_zero_right, Real.norm_eq_abs, abs_of_pos ht_pos]
      exact ht_lt_Œµ
    have ht_in_s : t ‚àà s := hŒµ_sub ht_in_ball
    have h_bound := hs_bound t ht_in_s
    simp only [Real.norm_eq_abs, abs_of_pos ht_pos] at h_bound
    have h_lower : p t - q t ‚â• t * Œ¥ - (Œ¥ / 2) * t := by
      have h1 : -((Œ¥ / 2) * t) ‚â§ (p t - q t) - t * Œ¥ := by
        have := neg_abs_le (p t - q t - t * Œ¥)
        linarith
      linarith
    have h_diff_pos : p t - q t > 0 := by
      have : t * Œ¥ - (Œ¥ / 2) * t = (Œ¥ / 2) * t := by ring
      rw [this] at h_lower
      have : (Œ¥ / 2) * t > 0 := mul_pos (by linarith) ht_pos
      linarith
    have h_le := hpq_ineq t (le_of_lt ht_pos) ht_le_1
    linarith

  -- Step C: Combine the two bounds
  -- From h_deriv_ineq: ‚ü®‚àáf(x), d‚ü© ‚â§ f(x*) - f(x) - (Œº/2)‚Äñd‚Äñ¬≤
  -- So: ‚ü®‚àáf(x), x - x*‚ü© = -‚ü®‚àáf(x), d‚ü© ‚â• f(x) - f(x*) + (Œº/2)‚Äñd‚Äñ¬≤
  have h_inner_neg : @inner ‚Ñù E _ (gradient f x) (x - x_star) =
      -@inner ‚Ñù E _ (gradient f x) d := by
    simp only [d, ‚Üê inner_neg_right, neg_sub]
  have h_d_norm : ‚Äñd‚Äñ = ‚Äñx - x_star‚Äñ := by simp only [d, norm_sub_rev]
  have h_e_norm : ‚Äñe‚Äñ = ‚Äñx - x_star‚Äñ := by simp only [e]

  -- From h_deriv_ineq: -‚ü®‚àáf(x), d‚ü© ‚â• f(x) - f(x*) + (Œº/2)‚Äñd‚Äñ¬≤
  have h_inner_lb : @inner ‚Ñù E _ (gradient f x) (x - x_star) ‚â•
      f x - f x_star + (Œº / 2) * ‚Äñx - x_star‚Äñ^2 := by
    rw [h_inner_neg]
    simp only [h_d_norm] at h_deriv_ineq
    linarith

  -- From h_func_bound: f(x) - f(x*) ‚â• (Œº/2)‚Äñe‚Äñ¬≤ = (Œº/2)‚Äñx - x*‚Äñ¬≤
  have h_func_lb : f x - f x_star ‚â• (Œº / 2) * ‚Äñx - x_star‚Äñ^2 := by
    rw [h_e_norm] at h_func_bound
    linarith

  -- Combine: ‚ü®‚àáf(x), x - x*‚ü© ‚â• (Œº/2)‚Äñx - x*‚Äñ¬≤ + (Œº/2)‚Äñx - x*‚Äñ¬≤ = Œº‚Äñx - x*‚Äñ¬≤
  calc @inner ‚Ñù E _ (gradient f x) (x - x_star)
      ‚â• f x - f x_star + (Œº / 2) * ‚Äñx - x_star‚Äñ^2 := h_inner_lb
    _ ‚â• (Œº / 2) * ‚Äñx - x_star‚Äñ^2 + (Œº / 2) * ‚Äñx - x_star‚Äñ^2 := by linarith [h_func_lb]
    _ = Œº * ‚Äñx - x_star‚Äñ^2 := by ring

/-- Interpolation condition for strongly convex AND smooth functions.

    For Œº-strongly convex and L-smooth f with ‚àáf(x*) = 0:
    ‚ü®‚àáf(x), x - x*‚ü© ‚â• (ŒºL)/(Œº+L) ‚Äñx - x*‚Äñ¬≤ + 1/(Œº+L) ‚Äñ‚àáf(x)‚Äñ¬≤

    This is stronger than using strong convexity or smoothness alone.
    It's the key to achieving the optimal (1 - Œº/L) contraction rate.
-/
theorem strong_smooth_interpolation (f : E ‚Üí ‚Ñù) (L Œº : ‚Ñù) (hL : 0 < L) (hŒº : 0 < Œº)
    (hSmooth : IsLSmooth f L) (hStrong : IsStronglyConvex f Œº)
    (x x_star : E) (hMin : gradient f x_star = 0) :
    @inner ‚Ñù E _ (gradient f x) (x - x_star) ‚â•
      (Œº * L) / (Œº + L) * ‚Äñx - x_star‚Äñ^2 + 1 / (Œº + L) * ‚Äñgradient f x‚Äñ^2 := by
  -- This is the interpolation condition for functions that are BOTH strongly convex
  -- AND smooth. It provides a tighter bound than either alone.
  --
  -- **Available ingredients**:
  -- 1. Strong convexity (gradient monotonicity): ‚ü®‚àáf(x) - ‚àáf(y), x - y‚ü© ‚â• Œº‚Äñx - y‚Äñ¬≤
  -- 2. Co-coercivity (from L-smoothness): ‚ü®‚àáf(x) - ‚àáf(y), x - y‚ü© ‚â• (1/L)‚Äñ‚àáf(x) - ‚àáf(y)‚Äñ¬≤
  --
  -- **The interpolation condition**:
  -- ‚ü®‚àáf(x) - ‚àáf(y), x - y‚ü© ‚â• (ŒºL)/(Œº+L)‚Äñx - y‚Äñ¬≤ + 1/(Œº+L)‚Äñ‚àáf(x) - ‚àáf(y)‚Äñ¬≤
  --
  -- **Proof strategy**:
  -- The key is to use BOTH conditions simultaneously in an optimal way.
  --
  -- Consider the auxiliary function: h(x) = f(x) - (Œº/2)‚Äñx‚Äñ¬≤
  -- Since f is Œº-strongly convex, h is convex.
  -- Since f is L-smooth, h is (L-Œº)-smooth.
  -- Apply co-coercivity to h at the optimum.
  --
  -- Alternatively, use the proximal operator characterization:
  -- For the proximal of f at x with parameter 1/L:
  -- prox_{f/L}(x) = argmin_z [f(z) + (L/2)‚Äñz - x‚Äñ¬≤]
  --
  -- **Simplified proof when y = x* (‚àáf(x*) = 0)**:
  -- Let g = ‚àáf(x). We need:
  -- ‚ü®g, x - x*‚ü© ‚â• (ŒºL)/(Œº+L)‚Äñx - x*‚Äñ¬≤ + 1/(Œº+L)‚Äñg‚Äñ¬≤
  --
  -- From strong convexity at x*: ‚ü®g, x - x*‚ü© ‚â• Œº‚Äñx - x*‚Äñ¬≤ (using ‚àáf(x*) = 0)
  -- From co-coercivity: ‚ü®g, x - x*‚ü© ‚â• (1/L)‚Äñg‚Äñ¬≤ (using ‚àáf(x*) = 0)
  --
  -- The weighted combination uses both:
  -- (Œº+L)‚ü®g, x - x*‚ü© = L‚ü®g, x - x*‚ü© + Œº‚ü®g, x - x*‚ü©
  --                   ‚â• L¬∑Œº‚Äñx - x*‚Äñ¬≤ + Œº¬∑(1/L)‚Äñg‚Äñ¬≤
  --                   = ŒºL‚Äñx - x*‚Äñ¬≤ + (Œº/L)‚Äñg‚Äñ¬≤
  --
  -- This gives: ‚ü®g, x - x*‚ü© ‚â• (ŒºL)/(Œº+L)‚Äñx - x*‚Äñ¬≤ + Œº/(L(Œº+L))‚Äñg‚Äñ¬≤
  --
  -- The coefficient Œº/(L(Œº+L)) is weaker than 1/(Œº+L) when Œº < L (typical case).
  -- The sharper bound requires the full interpolation argument using:
  -- - The Fenchel conjugate f* which is (1/Œº)-smooth and (1/L)-strongly convex
  -- - Or the "operator splitting" viewpoint
  --
  -- For our purposes in the convergence theorem, the weaker bound suffices
  -- since we only need ‚ü®g, x - x*‚ü© ‚â• c‚ÇÅ‚Äñx - x*‚Äñ¬≤ + c‚ÇÇ‚Äñg‚Äñ¬≤ for some c‚ÇÅ, c‚ÇÇ > 0.

  sorry

/-- Fundamental inequality for L-smooth functions:
    f(y) ‚â§ f(x) + ‚ü®‚àáf(x), y - x‚ü© + (L/2)‚Äñy - x‚Äñ¬≤

    ## Mathematical Proof

    This follows from integrating the gradient along the line from x to y
    and using the Lipschitz condition on the gradient.

    **Step 1: Define the path**

    Let Œ≥(t) = x + t(y - x) for t ‚àà [0, 1].
    Then Œ≥(0) = x and Œ≥(1) = y.

    **Step 2: Apply the Fundamental Theorem of Calculus**

    Define g(t) = f(Œ≥(t)). By chain rule: g'(t) = ‚ü®‚àáf(Œ≥(t)), y - x‚ü©.

    Therefore: f(y) - f(x) = g(1) - g(0) = ‚à´‚ÇÄ¬π g'(t) dt = ‚à´‚ÇÄ¬π ‚ü®‚àáf(Œ≥(t)), y - x‚ü© dt.

    **Step 3: Decompose and bound**

    f(y) - f(x) - ‚ü®‚àáf(x), y - x‚ü©
      = ‚à´‚ÇÄ¬π ‚ü®‚àáf(Œ≥(t)), y - x‚ü© dt - ‚ü®‚àáf(x), y - x‚ü©
      = ‚à´‚ÇÄ¬π ‚ü®‚àáf(Œ≥(t)) - ‚àáf(x), y - x‚ü© dt

    By Cauchy-Schwarz:
      |‚ü®‚àáf(Œ≥(t)) - ‚àáf(x), y - x‚ü©| ‚â§ ‚Äñ‚àáf(Œ≥(t)) - ‚àáf(x)‚Äñ ¬∑ ‚Äñy - x‚Äñ

    By L-smoothness (gradient is L-Lipschitz):
      ‚Äñ‚àáf(Œ≥(t)) - ‚àáf(x)‚Äñ ‚â§ L ¬∑ ‚ÄñŒ≥(t) - x‚Äñ = L ¬∑ t ¬∑ ‚Äñy - x‚Äñ

    Therefore:
      ‚ü®‚àáf(Œ≥(t)) - ‚àáf(x), y - x‚ü© ‚â§ L ¬∑ t ¬∑ ‚Äñy - x‚Äñ¬≤

    **Step 4: Integrate**

    f(y) - f(x) - ‚ü®‚àáf(x), y - x‚ü© ‚â§ ‚à´‚ÇÄ¬π L ¬∑ t ¬∑ ‚Äñy - x‚Äñ¬≤ dt
                                   = L ¬∑ ‚Äñy - x‚Äñ¬≤ ¬∑ ‚à´‚ÇÄ¬π t dt
                                   = L ¬∑ ‚Äñy - x‚Äñ¬≤ ¬∑ (1/2)
                                   = (L/2) ¬∑ ‚Äñy - x‚Äñ¬≤

    **Lean Formalization Requirements**

    1. `MeasureTheory.integral_Icc_eq_integral_Ioc` - integration on [0,1]
    2. `HasDerivAt.integral_eq_sub` - FTC for path integrals
    3. `MeasureTheory.integral_mono` - for bounding integrals
    4. `integral_id` or similar for ‚à´‚ÇÄ¬π t dt = 1/2
-/
theorem lsmooth_fundamental_ineq (f : E ‚Üí ‚Ñù) (L : ‚Ñù) (hL : 0 ‚â§ L)
    (hSmooth : IsLSmooth f L) (x y : E) :
    f y ‚â§ f x + @inner ‚Ñù E _ (gradient f x) (y - x) + (L / 2) * ‚Äñy - x‚Äñ^2 := by
  obtain ‚ü®hDiff, hLip‚ü© := hSmooth
  -- Special case: if x = y, the inequality is trivially true
  by_cases hxy : x = y
  ¬∑ simp only [hxy, sub_self, inner_zero_right, norm_zero, sq, mul_zero, add_zero, le_refl]
  -- Special case: if L = 0, gradient is constant, so f is affine
  by_cases hL0 : L = 0
  ¬∑ -- When L = 0, ‚àáf is constant (0-Lipschitz means constant)
    -- So f(y) = f(x) + ‚ü®‚àáf(x), y - x‚ü© for all x, y
    simp only [hL0, zero_div, zero_mul, add_zero]
    -- For constant gradient, f is affine: f(y) - f(x) = ‚ü®‚àáf(x), y - x‚ü©
    -- From 0-Lipschitz: ‚Äñ‚àáf(x) - ‚àáf(y)‚Äñ ‚â§ 0 * ‚Äñx - y‚Äñ = 0. So ‚àáf(x) = ‚àáf(y) for all x, y.
    -- When gradient is constant, by the MVT: f(y) - f(x) = ‚ü®‚àáf(Œæ), y - x‚ü© for some Œæ.
    -- Since ‚àáf is constant, ‚àáf(Œæ) = ‚àáf(x), so f(y) - f(x) = ‚ü®‚àáf(x), y - x‚ü©
    have h_grad_const : ‚àÄ z, gradient f z = gradient f x := by
      intro z
      have h0 : ‚Äñgradient f z - gradient f x‚Äñ ‚â§ 0 * ‚Äñz - x‚Äñ := by
        rw [‚Üê hL0]
        exact hLip z x
      simp only [zero_mul, norm_le_zero_iff] at h0
      exact sub_eq_zero.mp h0
    -- For the formal proof, we use that zero Frechet derivative implies constant.
    -- Define h(z) = f(z) - ‚ü®‚àáf(x), z‚ü©. Then fderiv h z = 0 (gradient is constant).
    -- Zero fderiv on convex set implies h is constant, so h(y) = h(x).
    let g := gradient f x
    let h := fun z => f z - @inner ‚Ñù E _ g z
    have hh_diff : Differentiable ‚Ñù h := by
      intro z
      apply DifferentiableAt.sub (hDiff z)
      exact (innerSL (ùïú := ‚Ñù) g).differentiableAt
    -- h has zero Frechet derivative everywhere
    have h_fderiv_zero : ‚àÄ z, fderiv ‚Ñù h z = 0 := by
      intro z
      have hf_diff : DifferentiableAt ‚Ñù f z := hDiff z
      have hg_diff : DifferentiableAt ‚Ñù (fun w => @inner ‚Ñù E _ g w) z :=
        (innerSL (ùïú := ‚Ñù) g).differentiableAt
      -- fderiv of f z = innerSL (gradient f z)
      have h_fderiv_f : fderiv ‚Ñù f z = innerSL (ùïú := ‚Ñù) (gradient f z) := by
        have hgrad := hf_diff.hasGradientAt
        exact hgrad.hasFDerivAt.fderiv
      -- fderiv of (inner g ¬∑) = innerSL g
      have h_fderiv_inner : fderiv ‚Ñù (fun w => @inner ‚Ñù E _ g w) z = innerSL (ùïú := ‚Ñù) g :=
        (innerSL (ùïú := ‚Ñù) g).fderiv
      -- fderiv of h = fderiv f - fderiv inner
      have h1 : fderiv ‚Ñù h z = fderiv ‚Ñù f z - fderiv ‚Ñù (fun w => @inner ‚Ñù E _ g w) z := by
        exact fderiv_sub hf_diff hg_diff
      rw [h1, h_fderiv_f, h_fderiv_inner, h_grad_const z]
      exact sub_self _
    -- h is constant: use that zero derivative on convex set implies constant
    have h_const : h y = h x := by
      have hconvex : Convex ‚Ñù (Set.univ : Set E) := convex_univ
      have hdiff_on : DifferentiableOn ‚Ñù h Set.univ := hh_diff.differentiableOn
      have hfderiv_on : ‚àÄ z ‚àà Set.univ, fderivWithin ‚Ñù h Set.univ z = 0 := by
        intro z _
        rw [fderivWithin_univ]
        exact h_fderiv_zero z
      exact Convex.is_const_of_fderivWithin_eq_zero hconvex hdiff_on hfderiv_on
        (Set.mem_univ x) (Set.mem_univ y)
    -- Expand h(y) = h(x): f(y) - ‚ü®g, y‚ü© = f(x) - ‚ü®g, x‚ü©, so f(y) = f(x) + ‚ü®g, y - x‚ü©
    simp only [h] at h_const
    have h_inner_sub : @inner ‚Ñù E _ g y - @inner ‚Ñù E _ g x = @inner ‚Ñù E _ g (y - x) := by
      rw [inner_sub_right]
    linarith [h_const, h_inner_sub]
  -- Main case: L > 0
  have hL_pos : 0 < L := lt_of_le_of_ne hL (Ne.symm hL0)
  /- The proof uses integration along the line segment from x to y.

     Define Œ≥(t) = x + t(y - x) for t ‚àà [0, 1].
     Define g(t) = f(Œ≥(t)).

     Then g'(t) = ‚ü®‚àáf(Œ≥(t)), y - x‚ü©.

     By the fundamental theorem of calculus:
     f(y) - f(x) = g(1) - g(0) = ‚à´‚ÇÄ¬π g'(t) dt = ‚à´‚ÇÄ¬π ‚ü®‚àáf(Œ≥(t)), y - x‚ü© dt

     Therefore:
     f(y) - f(x) - ‚ü®‚àáf(x), y - x‚ü© = ‚à´‚ÇÄ¬π ‚ü®‚àáf(Œ≥(t)) - ‚àáf(x), y - x‚ü© dt

     By Cauchy-Schwarz and L-Lipschitz gradient:
     ‚ü®‚àáf(Œ≥(t)) - ‚àáf(x), y - x‚ü© ‚â§ ‚Äñ‚àáf(Œ≥(t)) - ‚àáf(x)‚Äñ ¬∑ ‚Äñy - x‚Äñ
                                 ‚â§ L ¬∑ ‚ÄñŒ≥(t) - x‚Äñ ¬∑ ‚Äñy - x‚Äñ
                                 = L ¬∑ t ¬∑ ‚Äñy - x‚Äñ¬≤

     Integrating:
     f(y) - f(x) - ‚ü®‚àáf(x), y - x‚ü© ‚â§ ‚à´‚ÇÄ¬π L ¬∑ t ¬∑ ‚Äñy - x‚Äñ¬≤ dt
                                   = L ¬∑ ‚Äñy - x‚Äñ¬≤ ¬∑ [t¬≤/2]‚ÇÄ¬π
                                   = (L/2) ¬∑ ‚Äñy - x‚Äñ¬≤

     This requires Mathlib's MeasureTheory.integral machinery and
     careful handling of the FTC for paths in Hilbert spaces.

     **Mathlib theorems needed**:
     - `MeasureTheory.integral_Icc` for ‚à´‚ÇÄ¬π ... dt
     - `HasDerivAt.integral_eq_sub` for FTC
     - `real_inner_le_norm` for Cauchy-Schwarz
     - `intervalIntegral.integral_mono` for bounding integrals

     **Alternative approach via second derivative**:
     Define g(t) = f(x + t(y-x)). Then:
     - g'(t) = ‚ü®‚àáf(x + t(y-x)), y - x‚ü©
     - g''(t) = ‚ü®Hf(x + t(y-x))(y-x), y - x‚ü© where Hf is the Hessian
     - For L-smooth f, the Hessian satisfies ‚ÄñHf‚Äñ ‚â§ L, so g''(t) ‚â§ L‚Äñy-x‚Äñ¬≤

     Integrating g''(t) twice:
     - g'(t) ‚â§ g'(0) + L¬∑t¬∑‚Äñy-x‚Äñ¬≤
     - g(t) ‚â§ g(0) + g'(0)¬∑t + (L/2)¬∑t¬≤¬∑‚Äñy-x‚Äñ¬≤

     At t = 1:
     - f(y) ‚â§ f(x) + ‚ü®‚àáf(x), y-x‚ü© + (L/2)‚Äñy-x‚Äñ¬≤
  -/

  /- **Proof Strategy using Monotonicity (avoids MeasureTheory integration)**
     Define:
     - Œ≥(t) = x + t ‚Ä¢ (y - x) for t ‚àà [0, 1]
     - g(t) = f(Œ≥(t)) - t * ‚ü®‚àáf(x), y - x‚ü©
     - K = L * ‚Äñy - x‚Äñ¬≤
     - h(t) = g(t) - (K/2) * t¬≤
     Then:
     - g'(t) = ‚ü®‚àáf(Œ≥(t)) - ‚àáf(x), y - x‚ü© (after simplification)
     - g'(t) ‚â§ L * t * ‚Äñy - x‚Äñ¬≤ = K * t (by Lipschitz + Cauchy-Schwarz)
     - h'(t) = g'(t) - K * t ‚â§ 0
     - By antitoneOn_of_deriv_nonpos: h(1) ‚â§ h(0)
     - Expanding: g(1) - K/2 ‚â§ g(0)
     - So: f(y) - ‚ü®‚àáf(x), y-x‚ü© - (L/2)‚Äñy-x‚Äñ¬≤ ‚â§ f(x)
     - Rearranging: f(y) ‚â§ f(x) + ‚ü®‚àáf(x), y-x‚ü© + (L/2)‚Äñy-x‚Äñ¬≤
  -/
  -- Define the path Œ≥(t) = x + t ‚Ä¢ (y - x)
  let Œ≥ := fun t : ‚Ñù => x + t ‚Ä¢ (y - x)
  -- Define K = L * ‚Äñy - x‚Äñ¬≤
  let K := L * ‚Äñy - x‚Äñ^2
  -- Define inner_val = ‚ü®‚àáf(x), y - x‚ü©
  let inner_val := @inner ‚Ñù E _ (gradient f x) (y - x)
  -- Define g(t) = f(Œ≥(t)) - t * inner_val : measures deviation from linear model
  let g := fun t : ‚Ñù => f (Œ≥ t) - t * inner_val
  -- Define h(t) = g(t) - (K/2) * t¬≤ : we'll show h is antitone
  let h := fun t : ‚Ñù => g t - (K / 2) * t^2
  -- Key boundary values
  have hŒ≥0 : Œ≥ 0 = x := by simp only [Œ≥, zero_smul, add_zero]
  have hŒ≥1 : Œ≥ 1 = y := by simp only [Œ≥, one_smul, add_sub_cancel]
  have hg0 : g 0 = f x := by simp only [g, hŒ≥0, zero_mul, sub_zero]
  have hg1 : g 1 = f y - inner_val := by simp only [g, hŒ≥1, one_mul]
  have hh0 : h 0 = f x := by simp only [h, hg0, sq, mul_zero, sub_zero]
  have hh1 : h 1 = f y - inner_val - K / 2 := by
    simp only [h, hg1, one_pow, mul_one]
  -- Œ≥(t) - x = t ‚Ä¢ (y - x) for the Lipschitz bound
  have hŒ≥_diff : ‚àÄ t, Œ≥ t - x = t ‚Ä¢ (y - x) := by
    intro t; simp only [Œ≥, add_sub_cancel_left]
  -- ‚ÄñŒ≥(t) - x‚Äñ = |t| * ‚Äñy - x‚Äñ
  have hŒ≥_norm : ‚àÄ t, ‚ÄñŒ≥ t - x‚Äñ = |t| * ‚Äñy - x‚Äñ := by
    intro t; rw [hŒ≥_diff, norm_smul, Real.norm_eq_abs]
  -- For t ‚àà [0, 1], |t| = t
  have h_abs_t : ‚àÄ t : ‚Ñù, 0 ‚â§ t ‚Üí t ‚â§ 1 ‚Üí |t| = t := fun t ht _ => abs_of_nonneg ht
  -- The key bound: ‚ü®‚àáf(Œ≥(t)) - ‚àáf(x), y - x‚ü© ‚â§ L * t * ‚Äñy - x‚Äñ¬≤ for t ‚àà [0, 1]
  -- This uses: Cauchy-Schwarz, then L-Lipschitz of gradient, then ‚ÄñŒ≥(t) - x‚Äñ = t * ‚Äñy - x‚Äñ
  have h_grad_bound : ‚àÄ t, 0 ‚â§ t ‚Üí t ‚â§ 1 ‚Üí
      @inner ‚Ñù E _ (gradient f (Œ≥ t) - gradient f x) (y - x) ‚â§ L * t * ‚Äñy - x‚Äñ^2 := by
    intro t ht0 ht1
    have hCS : @inner ‚Ñù E _ (gradient f (Œ≥ t) - gradient f x) (y - x) ‚â§
        ‚Äñgradient f (Œ≥ t) - gradient f x‚Äñ * ‚Äñy - x‚Äñ := real_inner_le_norm _ _
    have hLip : ‚Äñgradient f (Œ≥ t) - gradient f x‚Äñ ‚â§ L * ‚ÄñŒ≥ t - x‚Äñ := hLip (Œ≥ t) x
    have hNorm : ‚ÄñŒ≥ t - x‚Äñ = t * ‚Äñy - x‚Äñ := by rw [hŒ≥_norm, h_abs_t t ht0 ht1]
    calc @inner ‚Ñù E _ (gradient f (Œ≥ t) - gradient f x) (y - x)
        ‚â§ ‚Äñgradient f (Œ≥ t) - gradient f x‚Äñ * ‚Äñy - x‚Äñ := hCS
      _ ‚â§ (L * ‚ÄñŒ≥ t - x‚Äñ) * ‚Äñy - x‚Äñ := by nlinarith [norm_nonneg (y - x)]
      _ = L * (t * ‚Äñy - x‚Äñ) * ‚Äñy - x‚Äñ := by rw [hNorm]
      _ = L * t * ‚Äñy - x‚Äñ^2 := by ring
  -- Step 1: h is continuous on [0, 1]
  -- Œ≥ is continuous
  have hŒ≥_cont : Continuous Œ≥ := by
    simp only [Œ≥]
    exact continuous_const.add (continuous_id.smul continuous_const)
  -- f ‚àò Œ≥ is continuous
  have hfŒ≥_cont : Continuous (f ‚àò Œ≥) := hDiff.continuous.comp hŒ≥_cont
  -- g is continuous
  have hg_cont : Continuous g := by
    simp only [g]
    exact hfŒ≥_cont.sub (continuous_id.mul continuous_const)
  -- h is continuous
  have hh_cont : Continuous h := by
    simp only [h]
    exact hg_cont.sub (continuous_const.mul (continuous_pow 2))
  have h_cont : ContinuousOn h (Set.Icc 0 1) := hh_cont.continuousOn
  -- Step 2: h is differentiable on interior (0, 1)
  -- The derivative of h at t is: ‚ü®‚àáf(Œ≥(t)), y-x‚ü© - inner_val - K*t
  --                            = ‚ü®‚àáf(Œ≥(t)) - ‚àáf(x), y-x‚ü© - K*t
  -- We use the chain rule: deriv (f ‚àò Œ≥) t = fderiv f (Œ≥ t) (deriv Œ≥ t)
  --                                        = ‚ü®‚àáf(Œ≥(t)), y - x‚ü©
  -- Since Œ≥(t) = x + t ‚Ä¢ (y - x), we have deriv Œ≥ t = y - x (constant)
  have h_deriv : ‚àÄ t ‚àà Set.Ioo (0 : ‚Ñù) 1,
      HasDerivAt h (@inner ‚Ñù E _ (gradient f (Œ≥ t) - gradient f x) (y - x) - K * t) t := by
    intro t _ht
    -- Œ≥ has derivative y - x
    have hŒ≥_deriv : HasDerivAt Œ≥ (y - x) t := by
      have h1 : HasDerivAt (fun s : ‚Ñù => x) 0 t := hasDerivAt_const t x
      have h2 : HasDerivAt (fun s : ‚Ñù => s ‚Ä¢ (y - x)) ((1 : ‚Ñù) ‚Ä¢ (y - x)) t := by
        exact (hasDerivAt_id t).smul_const (y - x)
      have h3 := h1.add h2
      simp only [zero_add, one_smul] at h3
      convert h3 using 1
    -- f ‚àò Œ≥ has derivative ‚ü®‚àáf(Œ≥(t)), y - x‚ü©
    have hfŒ≥_deriv : HasDerivAt (f ‚àò Œ≥) (@inner ‚Ñù E _ (gradient f (Œ≥ t)) (y - x)) t := by
      have hf_grad : HasGradientAt f (gradient f (Œ≥ t)) (Œ≥ t) := (hDiff (Œ≥ t)).hasGradientAt
      have hf_fderiv : HasFDerivAt f (innerSL (ùïú := ‚Ñù) (gradient f (Œ≥ t))) (Œ≥ t) :=
        hf_grad.hasFDerivAt
      have := hf_fderiv.comp_hasDerivAt t hŒ≥_deriv
      simp only [innerSL_apply_apply] at this
      exact this
    -- (t ‚Ü¶ t * inner_val) has derivative inner_val
    have h_lin_deriv : HasDerivAt (fun s => s * inner_val) inner_val t := by
      have := (hasDerivAt_id t).mul_const inner_val
      simp only [one_mul] at this
      exact this
    -- g = (f ‚àò Œ≥) - (t ‚Ü¶ t * inner_val) has derivative ‚ü®‚àáf(Œ≥(t)), y-x‚ü© - inner_val
    have hg_deriv : HasDerivAt g (@inner ‚Ñù E _ (gradient f (Œ≥ t)) (y - x) - inner_val) t := by
      exact hfŒ≥_deriv.sub h_lin_deriv
    -- Rewrite using inner_sub_left: ‚ü®a, v‚ü© - ‚ü®b, v‚ü© = ‚ü®a - b, v‚ü©
    have h_inner_eq : @inner ‚Ñù E _ (gradient f (Œ≥ t)) (y - x) - inner_val =
        @inner ‚Ñù E _ (gradient f (Œ≥ t) - gradient f x) (y - x) := by
      simp only [inner_val, inner_sub_left]
    rw [h_inner_eq] at hg_deriv
    -- (t ‚Ü¶ (K/2) * t¬≤) has derivative K * t
    have h_quad_deriv : HasDerivAt (fun s => (K / 2) * s^2) (K * t) t := by
      have h1 := hasDerivAt_pow 2 t
      have h2 := h1.const_mul (K / 2)
      simp only [Nat.cast_ofNat] at h2
      convert h2 using 1
      ring
    -- h = g - (t ‚Ü¶ (K/2) * t¬≤)
    exact hg_deriv.sub h_quad_deriv
  -- Step 3: deriv h t ‚â§ 0 on (0, 1)
  have h_deriv_nonpos : ‚àÄ t ‚àà Set.Ioo (0 : ‚Ñù) 1, deriv h t ‚â§ 0 := by
    intro t ht
    have hd := h_deriv t ht
    rw [hd.deriv]
    have hbound := h_grad_bound t (le_of_lt ht.1) (le_of_lt ht.2)
    linarith
  -- Step 4: Apply antitone result
  -- interior of Icc 0 1 = Ioo 0 1
  have h_interior : interior (Set.Icc (0 : ‚Ñù) 1) = Set.Ioo 0 1 := interior_Icc
  have h_diff_on : DifferentiableOn ‚Ñù h (interior (Set.Icc (0 : ‚Ñù) 1)) := by
    rw [h_interior]
    intro t ht
    exact (h_deriv t ht).differentiableAt.differentiableWithinAt
  have h_deriv_le : ‚àÄ t ‚àà interior (Set.Icc (0 : ‚Ñù) 1), deriv h t ‚â§ 0 := by
    rw [h_interior]
    exact h_deriv_nonpos
  have h_mono := Convex.image_sub_le_mul_sub_of_deriv_le (convex_Icc (0 : ‚Ñù) 1) h_cont h_diff_on
    h_deriv_le 0 (Set.left_mem_Icc.mpr zero_le_one) 1 (Set.right_mem_Icc.mpr zero_le_one)
    zero_le_one
  -- h(1) - h(0) ‚â§ 0 * (1 - 0) = 0
  simp only [zero_mul, sub_zero] at h_mono
  -- h(1) ‚â§ h(0) means f(y) - inner_val - K/2 ‚â§ f(x)
  rw [hh1, hh0] at h_mono
  -- Conclude: f(y) ‚â§ f(x) + inner_val + K/2
  simp only [inner_val, K] at h_mono
  linarith

/-- First-order optimality for convex functions: if ‚àáf(x*) = 0 and f is convex and differentiable,
    then x* is a global minimizer.

    This is proved using a derivative limit argument: define p(t) = f(x* + t(y - x*)).
    Convexity gives p(t) ‚â§ (1-t)p(0) + tp(1). At t = 0, p(0) = p(0), so equality holds.
    p'(0) = ‚ü®‚àáf(x*), y - x*‚ü© = 0. For the convex bound p(t) ‚â§ (1-t)p(0) + tp(1) with
    derivative 0 at t = 0, we must have p(0) ‚â§ p(1), i.e., f(x*) ‚â§ f(y). -/
lemma convex_first_order_optimality (f : E ‚Üí ‚Ñù) (hConvex : ConvexOn ‚Ñù Set.univ f)
    (hDiff : Differentiable ‚Ñù f) (x_star : E) (hMin : gradient f x_star = 0) :
    ‚àÄ y, f x_star ‚â§ f y := by
  -- First-order optimality: for convex differentiable f, ‚àáf(x*) = 0 implies x* is a global minimizer.
  -- The proof uses convexity along paths and derivative comparison at t = 0.
  intro y
  -- Define p(t) = f(x* + t(y - x*)) and q(t) = (1-t)f(x*) + tf(y)
  let e := y - x_star
  let p := fun t : ‚Ñù => f (x_star + t ‚Ä¢ e)
  let q := fun t : ‚Ñù => (1 - t) * f x_star + t * f y
  -- By convexity: p(t) ‚â§ q(t) for t ‚àà [0, 1]
  have hpq_ineq : ‚àÄ t, 0 ‚â§ t ‚Üí t ‚â§ 1 ‚Üí p t ‚â§ q t := by
    intro t ht0 ht1
    have h1mt : 0 ‚â§ 1 - t := by linarith
    have hsum : (1 - t) + t = 1 := by ring
    have hconv := hConvex.2 (Set.mem_univ x_star) (Set.mem_univ y) h1mt ht0 hsum
    have heq : (1 - t) ‚Ä¢ x_star + t ‚Ä¢ y = x_star + t ‚Ä¢ e := by
      simp only [e, smul_sub]
      ring_nf
      module
    simp only [p, q, heq, smul_eq_mul] at hconv ‚ä¢
    linarith
  -- At t = 0: p(0) = q(0) = f(x*)
  have hp0 : p 0 = f x_star := by simp only [p, zero_smul, add_zero]
  have hq0 : q 0 = f x_star := by simp only [q]; ring
  -- p'(0) = ‚ü®‚àáf(x*), e‚ü© = 0
  have p_deriv : HasDerivAt p 0 0 := by
    have hŒ≥ : HasDerivAt (fun t : ‚Ñù => x_star + t ‚Ä¢ e) e 0 := by
      have h1 : HasDerivAt (fun _ : ‚Ñù => x_star) 0 0 := hasDerivAt_const 0 x_star
      have h2 : HasDerivAt (fun t : ‚Ñù => t ‚Ä¢ e) ((1 : ‚Ñù) ‚Ä¢ e) 0 := (hasDerivAt_id 0).smul_const e
      have hsum := h1.add h2
      simp only [zero_add, one_smul] at hsum
      exact hsum
    have hf_grad : HasGradientAt f (gradient f x_star) x_star := (hDiff x_star).hasGradientAt
    have hf_fderiv : HasFDerivAt f (innerSL (ùïú := ‚Ñù) (gradient f x_star)) x_star := hf_grad.hasFDerivAt
    have hf_fderiv' : HasFDerivAt f (innerSL (ùïú := ‚Ñù) (gradient f x_star)) (x_star + (0 : ‚Ñù) ‚Ä¢ e) := by
      simp only [zero_smul, add_zero]; exact hf_fderiv
    have hcomp := hf_fderiv'.comp_hasDerivAt (0 : ‚Ñù) hŒ≥
    simp only [Function.comp_apply, innerSL_apply_apply, zero_smul, add_zero, hMin, inner_zero_left] at hcomp
    exact hcomp
  -- q'(0) = f(y) - f(x*)
  have q_deriv : HasDerivAt q (f y - f x_star) 0 := by
    have h1 : HasDerivAt (fun t : ‚Ñù => (1 - t) * f x_star) (-f x_star) 0 := by
      have hid : HasDerivAt (fun t : ‚Ñù => 1 - t) (-1) 0 := by
        have := (hasDerivAt_const (0 : ‚Ñù) (1 : ‚Ñù)).sub (hasDerivAt_id (0 : ‚Ñù))
        convert this using 1
        ring
      have := hid.mul_const (f x_star)
      convert this using 1
      ring
    have h2 : HasDerivAt (fun t : ‚Ñù => t * f y) (f y) 0 := by
      have := (hasDerivAt_id (0 : ‚Ñù)).mul_const (f y)
      convert this using 1
      ring
    have h3 := h1.add h2
    convert h3 using 1
    ring
  -- Proof by contradiction: assume f(x*) > f(y)
  by_contra hcontra
  push_neg at hcontra
  let Œ¥ := f x_star - f y
  have hŒ¥_pos : Œ¥ > 0 := by simp only [Œ¥]; linarith
  -- (p - q)'(0) = 0 - (f(y) - f(x*)) = Œ¥ > 0
  have h_pq_deriv : HasDerivAt (fun t => p t - q t) Œ¥ 0 := by
    have hsub := HasDerivAt.sub p_deriv q_deriv
    convert hsub using 1
    simp only [Œ¥]
    ring
  have h_pq_0 : (fun t => p t - q t) 0 = 0 := by simp only [hp0, hq0, sub_self]
  -- Use isLittleO characterization of derivative
  rw [hasDerivAt_iff_isLittleO] at h_pq_deriv
  have hŒµ_half : 0 < Œ¥ / 2 := by linarith
  have h_bound_evt := h_pq_deriv.def hŒµ_half
  simp only [h_pq_0, sub_zero, smul_eq_mul] at h_bound_evt
  rw [Filter.eventually_iff_exists_mem] at h_bound_evt
  obtain ‚ü®s, hs_mem, hs_bound‚ü© := h_bound_evt
  rw [Metric.mem_nhds_iff] at hs_mem
  obtain ‚ü®Œµ, hŒµ_pos, hŒµ_sub‚ü© := hs_mem
  -- Choose t in (0, min(Œµ/2, 1/2)]
  let t := min (Œµ / 2) (1 / 2)
  have ht_pos : 0 < t := by positivity
  have ht_lt_Œµ : t < Œµ := by simp only [t]; linarith [min_le_left (Œµ / 2) (1 / 2)]
  have ht_le_1 : t ‚â§ 1 := by simp only [t]; linarith [min_le_right (Œµ / 2) (1 / 2)]
  have ht_in_ball : t ‚àà Metric.ball 0 Œµ := by
    simp only [Metric.mem_ball, dist_zero_right, Real.norm_eq_abs, abs_of_pos ht_pos]
    exact ht_lt_Œµ
  have ht_in_s : t ‚àà s := hŒµ_sub ht_in_ball
  have h_bound := hs_bound t ht_in_s
  simp only [Real.norm_eq_abs, abs_of_pos ht_pos] at h_bound
  -- From derivative approximation: |(p(t) - q(t)) - t*Œ¥| ‚â§ (Œ¥/2)*t
  -- So p(t) - q(t) ‚â• t*Œ¥ - (Œ¥/2)*t = (Œ¥/2)*t > 0
  have h_lower : p t - q t ‚â• t * Œ¥ - (Œ¥ / 2) * t := by
    have h1 : -((Œ¥ / 2) * t) ‚â§ (p t - q t) - t * Œ¥ := by
      have := neg_abs_le (p t - q t - t * Œ¥)
      linarith
    linarith
  have h_diff_pos : p t - q t > 0 := by
    have : t * Œ¥ - (Œ¥ / 2) * t = (Œ¥ / 2) * t := by ring
    rw [this] at h_lower
    have : (Œ¥ / 2) * t > 0 := mul_pos (by linarith) ht_pos
    linarith
  -- But convexity says p(t) ‚â§ q(t), contradiction
  have h_le := hpq_ineq t (le_of_lt ht_pos) ht_le_1
  linarith

/-- Co-coercivity of L-smooth gradients (Baillon-Haddad theorem).

    For L-smooth f: ‚ü®‚àáf(x) - ‚àáf(y), x - y‚ü© ‚â• (1/L)‚Äñ‚àáf(x) - ‚àáf(y)‚Äñ¬≤

    With y = x* where ‚àáf(x*) = 0:
    ‚ü®‚àáf(x), x - x*‚ü© ‚â• (1/L)‚Äñ‚àáf(x)‚Äñ¬≤

    Equivalently: ‚Äñ‚àáf(x)‚Äñ¬≤ ‚â§ L‚ü®‚àáf(x), x - x*‚ü© -/
theorem lsmooth_cocoercivity (f : E ‚Üí ‚Ñù) (L : ‚Ñù) (hL : 0 < L)
    (hSmooth : IsLSmooth f L) (hConvex : ConvexOn ‚Ñù Set.univ f)
    (x x_star : E) (hMin : gradient f x_star = 0) :
    ‚Äñgradient f x‚Äñ^2 ‚â§ L * @inner ‚Ñù E _ (gradient f x) (x - x_star) := by
  -- The proof uses the tilted function technique combining:
  -- 1. L-smooth descent: f(x - (1/L)g) ‚â§ f(x) - (1/2L)‚Äñg‚Äñ¬≤
  -- 2. First-order optimality for the tilted function h(z) = f(z) - ‚ü®g, z‚ü©
  -- Adding the two bounds gives (1/L)‚Äñg‚Äñ¬≤ ‚â§ ‚ü®g, x - x*‚ü©
  have hDiff : Differentiable ‚Ñù f := hSmooth.1
  let g := gradient f x

  -- Step 1: x* minimizes f (since ‚àáf(x*) = 0 and f is convex)
  have h_xstar_min : ‚àÄ y, f x_star ‚â§ f y := convex_first_order_optimality f hConvex hDiff x_star hMin

  -- Step 2: Apply lsmooth_fundamental_ineq to get descent at x
  have h_fund_f := lsmooth_fundamental_ineq f L (le_of_lt hL) hSmooth x (x - (1 / L) ‚Ä¢ g)
  have h_descent_f : f (x - (1 / L) ‚Ä¢ g) ‚â§ f x - (1 / (2 * L)) * ‚Äñg‚Äñ^2 := by
    have h_diff : (x - (1 / L) ‚Ä¢ g) - x = -((1 / L) ‚Ä¢ g) := by simp [sub_eq_add_neg, add_comm]
    have h_inner : @inner ‚Ñù E _ g ((x - (1 / L) ‚Ä¢ g) - x) = -(1 / L) * ‚Äñg‚Äñ^2 := by
      rw [h_diff]
      simp only [inner_neg_right, inner_smul_right, real_inner_self_eq_norm_sq]
      ring
    have h_norm : ‚Äñ(x - (1 / L) ‚Ä¢ g) - x‚Äñ^2 = (1 / L)^2 * ‚Äñg‚Äñ^2 := by
      rw [h_diff, norm_neg, norm_smul, Real.norm_eq_abs, abs_of_pos (by positivity : 1/L > 0)]
      ring
    calc f (x - (1 / L) ‚Ä¢ g) ‚â§ f x + @inner ‚Ñù E _ g ((x - (1 / L) ‚Ä¢ g) - x) +
                                 (L / 2) * ‚Äñ(x - (1 / L) ‚Ä¢ g) - x‚Äñ^2 := h_fund_f
      _ = f x + (-(1 / L) * ‚Äñg‚Äñ^2) + (L / 2) * ((1 / L)^2 * ‚Äñg‚Äñ^2) := by rw [h_inner, h_norm]
      _ = f x - (1 / (2 * L)) * ‚Äñg‚Äñ^2 := by field_simp; ring

  -- Bound A: (1/2L)‚Äñg‚Äñ¬≤ ‚â§ f(x) - f(x*)
  have h_bound_A : (1 / (2 * L)) * ‚Äñg‚Äñ^2 ‚â§ f x - f x_star := by
    have := h_xstar_min (x - (1 / L) ‚Ä¢ g)
    linarith

  -- Step 3: Apply fundamental ineq at x_star
  have h_fund_xstar := lsmooth_fundamental_ineq f L (le_of_lt hL) hSmooth x_star (x_star + (1 / L) ‚Ä¢ g)
  have h_fund_xstar_bound : f (x_star + (1 / L) ‚Ä¢ g) ‚â§ f x_star + (1 / (2 * L)) * ‚Äñg‚Äñ^2 := by
    have h_diff : (x_star + (1 / L) ‚Ä¢ g) - x_star = (1 / L) ‚Ä¢ g := by abel
    have h_inner : @inner ‚Ñù E _ (gradient f x_star) ((x_star + (1 / L) ‚Ä¢ g) - x_star) = 0 := by
      rw [hMin, inner_zero_left]
    have h_norm : ‚Äñ(x_star + (1 / L) ‚Ä¢ g) - x_star‚Äñ^2 = (1 / L)^2 * ‚Äñg‚Äñ^2 := by
      rw [h_diff, norm_smul, Real.norm_eq_abs, abs_of_pos (by positivity : 1/L > 0)]
      ring
    calc f (x_star + (1 / L) ‚Ä¢ g) ‚â§ f x_star + @inner ‚Ñù E _ (gradient f x_star)
          ((x_star + (1 / L) ‚Ä¢ g) - x_star) + (L / 2) * ‚Äñ(x_star + (1 / L) ‚Ä¢ g) - x_star‚Äñ^2 := h_fund_xstar
      _ = f x_star + 0 + (L / 2) * ((1 / L)^2 * ‚Äñg‚Äñ^2) := by rw [h_inner, h_norm]
      _ = f x_star + (1 / (2 * L)) * ‚Äñg‚Äñ^2 := by field_simp; ring

  -- Step 4: Tilted function h(z) = f(z) - ‚ü®g, z‚ü© is convex
  have h_convex : ConvexOn ‚Ñù Set.univ (fun z => f z - @inner ‚Ñù E _ g z) := by
    have h_linear_concave : ConcaveOn ‚Ñù Set.univ (fun z => @inner ‚Ñù E _ g z) := by
      constructor
      ¬∑ exact convex_univ
      ¬∑ intro z _ w _ a b ha hb hab
        simp only [inner_add_right, inner_smul_right, smul_eq_mul]
        linarith
    exact hConvex.sub h_linear_concave

  -- ‚àáh(x) = 0
  have h_grad_h_x : gradient (fun z => f z - @inner ‚Ñù E _ g z) x = 0 := by
    have hf_diff : DifferentiableAt ‚Ñù f x := hDiff x
    have hg_diff : DifferentiableAt ‚Ñù (fun z => @inner ‚Ñù E _ g z) x :=
      (innerSL (ùïú := ‚Ñù) g).differentiableAt
    -- The gradient of z ‚Ü¶ ‚ü®g, z‚ü© is g
    have hg_grad : HasGradientAt (fun z => @inner ‚Ñù E _ g z) g x := by
      rw [hasGradientAt_iff_hasFDerivAt]
      have h1 := (innerSL (ùïú := ‚Ñù) g).hasFDerivAt (x := x)
      simp only [InnerProductSpace.toDual] at h1 ‚ä¢
      convert h1 using 1
    -- HasGradientAt (f - inner g) (g - g) x
    have hf_grad : HasGradientAt f g x := hf_diff.hasGradientAt
    have h_sub : HasGradientAt (fun z => f z - @inner ‚Ñù E _ g z) (g - g) x := by
      have h1 := hasGradientAt_iff_hasFDerivAt.mp hf_grad
      have h2 := hasGradientAt_iff_hasFDerivAt.mp hg_grad
      have h3 := h1.sub h2
      rw [hasGradientAt_iff_hasFDerivAt]
      convert h3 using 1
      simp only [map_sub]
    rw [sub_self] at h_sub
    exact h_sub.gradient

  -- h is differentiable
  have h_diff_h : Differentiable ‚Ñù (fun z => f z - @inner ‚Ñù E _ g z) := by
    intro z
    exact (hDiff z).sub (innerSL (ùïú := ‚Ñù) g).differentiableAt

  -- x minimizes h via first-order optimality
  have h_x_min_h : ‚àÄ y, (f x - @inner ‚Ñù E _ g x) ‚â§ (f y - @inner ‚Ñù E _ g y) :=
    convex_first_order_optimality (fun z => f z - @inner ‚Ñù E _ g z) h_convex h_diff_h x h_grad_h_x

  -- h(x) ‚â§ h(x_star + (1/L)g)
  have h_hx_le := h_x_min_h (x_star + (1 / L) ‚Ä¢ g)

  -- Expand ‚ü®g, x_star + (1/L)g‚ü©
  have h_inner_xstar'_g : @inner ‚Ñù E _ g (x_star + (1 / L) ‚Ä¢ g) =
      @inner ‚Ñù E _ g x_star + (1 / L) * ‚Äñg‚Äñ^2 := by
    simp only [inner_add_right, inner_smul_right, real_inner_self_eq_norm_sq]

  -- Bound B: (1/2L)‚Äñg‚Äñ¬≤ ‚â§ f(x_star) - f(x) + ‚ü®g, x - x_star‚ü©
  have h_bound_B : (1 / (2 * L)) * ‚Äñg‚Äñ^2 ‚â§ f x_star - f x + @inner ‚Ñù E _ g (x - x_star) := by
    -- From h(x) ‚â§ h(x_star + (1/L)g):
    -- f(x) - ‚ü®g, x‚ü© ‚â§ f(x_star + (1/L)g) - ‚ü®g, x_star + (1/L)g‚ü©
    --              ‚â§ [f(x_star) + (1/2L)‚Äñg‚Äñ¬≤] - [‚ü®g, x_star‚ü© + (1/L)‚Äñg‚Äñ¬≤]
    --              = f(x_star) - ‚ü®g, x_star‚ü© - (1/2L)‚Äñg‚Äñ¬≤
    -- Rearranging: (1/2L)‚Äñg‚Äñ¬≤ ‚â§ f(x_star) - f(x) + ‚ü®g, x‚ü© - ‚ü®g, x_star‚ü©
    --                        = f(x_star) - f(x) + ‚ü®g, x - x_star‚ü©
    have h4 : @inner ‚Ñù E _ g (x - x_star) = @inner ‚Ñù E _ g x - @inner ‚Ñù E _ g x_star :=
      inner_sub_right g x x_star
    -- Substitute step3 into step1
    have step1' : f x - @inner ‚Ñù E _ g x ‚â§ f (x_star + (1 / L) ‚Ä¢ g) -
        (@inner ‚Ñù E _ g x_star + (1 / L) * ‚Äñg‚Äñ^2) := by
      rw [‚Üê h_inner_xstar'_g]
      exact h_hx_le
    -- Combine with step2
    have step2' : f (x_star + (1 / L) ‚Ä¢ g) - (@inner ‚Ñù E _ g x_star + (1 / L) * ‚Äñg‚Äñ^2) ‚â§
        f x_star + (1 / (2 * L)) * ‚Äñg‚Äñ^2 - (@inner ‚Ñù E _ g x_star + (1 / L) * ‚Äñg‚Äñ^2) := by
      linarith [h_fund_xstar_bound]
    -- Chain inequalities
    have step3' : f x - @inner ‚Ñù E _ g x ‚â§
        f x_star - @inner ‚Ñù E _ g x_star - (1 / (2 * L)) * ‚Äñg‚Äñ^2 := by
      have := le_trans step1' step2'
      have eq1 : f x_star + (1 / (2 * L)) * ‚Äñg‚Äñ^2 - (@inner ‚Ñù E _ g x_star + (1 / L) * ‚Äñg‚Äñ^2) =
          f x_star - @inner ‚Ñù E _ g x_star - (1 / (2 * L)) * ‚Äñg‚Äñ^2 := by ring
      linarith
    -- Rearrange
    have step4 : (1 / (2 * L)) * ‚Äñg‚Äñ^2 ‚â§ f x_star - f x + @inner ‚Ñù E _ g x - @inner ‚Ñù E _ g x_star := by
      linarith
    linarith

  -- Add bounds A and B: (1/L)‚Äñg‚Äñ¬≤ ‚â§ ‚ü®g, x - x*‚ü©
  have h_combined : (1 / L) * ‚Äñg‚Äñ^2 ‚â§ @inner ‚Ñù E _ g (x - x_star) := by
    have h_add := add_le_add h_bound_A h_bound_B
    -- h_add: (1/(2L))‚Äñg‚Äñ¬≤ + (1/(2L))‚Äñg‚Äñ¬≤ ‚â§ (f x - f x_star) + (f x_star - f x + ‚ü®g, x - x_star‚ü©)
    -- LHS = (1/L)‚Äñg‚Äñ¬≤, RHS = ‚ü®g, x - x_star‚ü©
    have lhs_eq : (1 / (2 * L)) * ‚Äñg‚Äñ^2 + (1 / (2 * L)) * ‚Äñg‚Äñ^2 = (1 / L) * ‚Äñg‚Äñ^2 := by field_simp; ring
    have rhs_eq : (f x - f x_star) + (f x_star - f x + @inner ‚Ñù E _ g (x - x_star)) =
        @inner ‚Ñù E _ g (x - x_star) := by ring
    linarith

  -- Multiply by L
  calc ‚Äñg‚Äñ^2 = L * ((1 / L) * ‚Äñg‚Äñ^2) := by field_simp
    _ ‚â§ L * @inner ‚Ñù E _ g (x - x_star) := by
        apply mul_le_mul_of_nonneg_left h_combined (le_of_lt hL)

/-- One step of gradient descent with learning rate Œ∑. -/
noncomputable def gradientDescentStep (f : E ‚Üí ‚Ñù) (Œ∑ : ‚Ñù) (x : E) : E :=
  x - Œ∑ ‚Ä¢ gradient f x

/-- k steps of gradient descent. -/
noncomputable def gradientDescentIterates (f : E ‚Üí ‚Ñù) (Œ∑ : ‚Ñù) (x‚ÇÄ : E) : ‚Ñï ‚Üí E
  | 0 => x‚ÇÄ
  | n + 1 => gradientDescentStep f Œ∑ (gradientDescentIterates f Œ∑ x‚ÇÄ n)

/-- Convergence rate for smooth convex functions.
    After k iterations: f(x_k) - f(x*) ‚â§ ‚Äñx‚ÇÄ - x*‚Äñ¬≤ / (2Œ∑k) -/
theorem convex_convergence_rate (f : E ‚Üí ‚Ñù) (L : ‚Ñù) (hL : 0 < L)
    (hSmooth : IsLSmooth f L) (hConvex : ConvexOn ‚Ñù Set.univ f)
    (x_star : E) (hMin : ‚àÄ x, f x_star ‚â§ f x)
    (Œ∑ : ‚Ñù) (hŒ∑ : 0 < Œ∑) (hŒ∑L : Œ∑ ‚â§ 1 / L) (x‚ÇÄ : E) :
    ‚àÄ k : ‚Ñï, k > 0 ‚Üí
      f (gradientDescentIterates f Œ∑ x‚ÇÄ k) - f x_star ‚â§ ‚Äñx‚ÇÄ - x_star‚Äñ^2 / (2 * Œ∑ * k) := by
  intro k hk

  /- Convergence Proof via Telescoping Descent Lemma

  For smooth convex functions, we prove O(1/k) convergence by combining:

  1. **Descent Lemma (L-smoothness)**:
     f(x_{i+1}) ‚â§ f(x_i) - (Œ∑/2)‚Äñ‚àáf(x_i)‚Äñ¬≤

  2. **First-Order Convexity**:
     For convex f: f(x) - f(x*) ‚â§ ‚ü®‚àáf(x), x - x*‚ü©

  3. **Telescoping Sum**:
     Sum descent inequalities over i = 0, ..., k-1:
     f(x_k) - f(x_0) ‚â§ -(Œ∑/2) ‚àë·µ¢ ‚Äñ‚àáf(x_i)‚Äñ¬≤

  4. **Cauchy-Schwarz Lower Bound on Gradient Norms**:
     From convexity: ‚Äñ‚àáf(x_i)‚Äñ¬≤ ‚â• 2(f(x_i) - f(x*))¬≤ / ‚Äñx_i - x*‚Äñ¬≤

     However, this requires bounded domain assumptions that conflict with the
     general statement. The standard proof instead uses:

     ‚ü®‚àáf(x), x - x*‚ü© ‚â• f(x) - f(x*) (convexity)

     Which combined with ‚Äñ‚àáf(x)‚Äñ ¬∑ ‚Äñx - x*‚Äñ ‚â• |‚ü®‚àáf(x), x - x*‚ü©| gives:
     ‚Äñ‚àáf(x)‚Äñ ‚â• (f(x) - f(x*)) / ‚Äñx - x*‚Äñ

  5. **Proof Dependencies**:
     - `lsmooth_fundamental_ineq` (COMPLETE)
     - `descent_lemma` (COMPLETE, uses lsmooth_fundamental_ineq)
     - First-order convexity characterization from Mathlib's ConvexOn
     - Telescoping sum machinery
  -/

  -- The proof uses the key identity for distance to optimum:
  -- ‚Äñx_{k+1} - x*‚Äñ¬≤ = ‚Äñx_k - Œ∑‚àáf(x_k) - x*‚Äñ¬≤
  --                 = ‚Äñx_k - x*‚Äñ¬≤ - 2Œ∑‚ü®‚àáf(x_k), x_k - x*‚ü© + Œ∑¬≤‚Äñ‚àáf(x_k)‚Äñ¬≤
  --
  -- From convexity: f(x_k) - f(x*) ‚â§ ‚ü®‚àáf(x_k), x_k - x*‚ü©
  -- So: 2Œ∑(f(x_k) - f(x*)) ‚â§ 2Œ∑‚ü®‚àáf(x_k), x_k - x*‚ü©
  --
  -- Rearranging the distance identity:
  -- 2Œ∑(f(x_k) - f(x*)) ‚â§ ‚Äñx_k - x*‚Äñ¬≤ - ‚Äñx_{k+1} - x*‚Äñ¬≤ + Œ∑¬≤‚Äñ‚àáf(x_k)‚Äñ¬≤
  --
  -- Summing over i = 0 to k-1:
  -- 2Œ∑‚àë(f(x_i) - f(x*)) ‚â§ ‚Äñx_0 - x*‚Äñ¬≤ - ‚Äñx_k - x*‚Äñ¬≤ + Œ∑¬≤‚àë‚Äñ‚àáf(x_i)‚Äñ¬≤
  --
  -- From descent_lemma: f(x_{i+1}) ‚â§ f(x_i) - (Œ∑/2)‚Äñ‚àáf(x_i)‚Äñ¬≤
  -- Telescoping: f(x_k) - f(x_0) ‚â§ -(Œ∑/2)‚àë‚Äñ‚àáf(x_i)‚Äñ¬≤
  -- So: (Œ∑/2)‚àë‚Äñ‚àáf(x_i)‚Äñ¬≤ ‚â§ f(x_0) - f(x_k) ‚â§ f(x_0) - f(x*)
  -- Hence: Œ∑¬≤‚àë‚Äñ‚àáf(x_i)‚Äñ¬≤ ‚â§ 2Œ∑(f(x_0) - f(x*))
  --
  -- Substituting:
  -- 2Œ∑‚àë(f(x_i) - f(x*)) ‚â§ ‚Äñx_0 - x*‚Äñ¬≤ + 2Œ∑(f(x_0) - f(x*))
  --
  -- Since f(x_k) - f(x*) ‚â§ (1/k)‚àë(f(x_i) - f(x*)) (minimum ‚â§ average):
  -- 2Œ∑k(f(x_k) - f(x*)) ‚â§ ‚Äñx_0 - x*‚Äñ¬≤ + 2Œ∑(f(x_0) - f(x*))
  --
  -- Note: This gives a slightly weaker bound than claimed. The exact bound
  -- f(x_k) - f(x*) ‚â§ ‚Äñx_0 - x*‚Äñ¬≤/(2Œ∑k) requires showing the last iterate
  -- satisfies the average bound, which holds for convex objectives.
  --
  -- TODO: Complete with induction and Finset.sum machinery

  sorry

/-- Linear convergence for strongly convex smooth functions.
    After k iterations: ‚Äñx_k - x*‚Äñ¬≤ ‚â§ (1 - Œº/L)^k ‚Äñx‚ÇÄ - x*‚Äñ¬≤

## Proof Strategy

For strongly convex and L-smooth functions with step size Œ∑ = 1/L:

1. **Contraction per iteration**: Each gradient descent step contracts the distance to optimum
   by a factor of (1 - Œº/L), i.e., ‚Äñx_{k+1} - x*‚Äñ¬≤ ‚â§ (1 - Œº/L)‚Äñx_k - x*‚Äñ¬≤

2. **Key ingredients**:
   - L-smoothness provides descent lemma: f(x - Œ∑‚àáf(x)) ‚â§ f(x) - (Œ∑/2)‚Äñ‚àáf(x)‚Äñ¬≤
   - Œº-strong convexity ensures: f(x*) + (Œº/2)‚Äñx - x*‚Äñ¬≤ ‚â§ f(x) + ‚ü®‚àáf(x), x* - x‚ü©
   - At optimum: ‚àáf(x*) = 0

3. **Per-step contraction lemma**: From strong convexity and smoothness
   ‚Äñx_{k+1} - x*‚Äñ¬≤ = ‚Äñx_k - Œ∑‚àáf(x_k) - x*‚Äñ¬≤
                     ‚â§ (1 - Œº/L)‚Äñx_k - x*‚Äñ¬≤

4. **Telescoping**: Apply contraction k times to get:
   ‚Äñx_k - x*‚Äñ¬≤ ‚â§ (1 - Œº/L)^k ‚Äñx‚ÇÄ - x*‚Äñ¬≤

This is the classical result for strongly convex optimization.
-/
theorem strongly_convex_linear_convergence (f : E ‚Üí ‚Ñù) (L Œº : ‚Ñù)
    (hL : 0 < L) (hŒº : 0 < Œº) (hŒºL : Œº ‚â§ L)
    (hSmooth : IsLSmooth f L) (hStrong : IsStronglyConvex f Œº)
    (x_star : E) (hMin : gradient f x_star = 0)
    (Œ∑ : ‚Ñù) (hŒ∑ : Œ∑ = 1 / L) (x‚ÇÄ : E) :
    ‚àÄ k : ‚Ñï, ‚ÄñgradientDescentIterates f Œ∑ x‚ÇÄ k - x_star‚Äñ^2 ‚â§
      (1 - Œº / L)^k * ‚Äñx‚ÇÄ - x_star‚Äñ^2 := by
  -- We proceed by induction on k
  intro k
  induction k with
  | zero =>
    -- Base case: k = 0
    -- gradientDescentIterates f Œ∑ x‚ÇÄ 0 = x‚ÇÄ
    -- ‚Äñx‚ÇÄ - x_star‚Äñ¬≤ ‚â§ (1 - Œº/L)^0 * ‚Äñx‚ÇÄ - x_star‚Äñ¬≤
    -- This simplifies to ‚Äñx‚ÇÄ - x_star‚Äñ¬≤ ‚â§ ‚Äñx‚ÇÄ - x_star‚Äñ¬≤
    simp only [gradientDescentIterates, pow_zero, one_mul]
    exact le_refl _
  | succ k ih =>
    -- Inductive case: assume ‚Äñx_k - x*‚Äñ¬≤ ‚â§ (1 - Œº/L)^k ‚Äñx‚ÇÄ - x*‚Äñ¬≤
    -- Need to show: ‚Äñx_{k+1} - x*‚Äñ¬≤ ‚â§ (1 - Œº/L)^{k+1} ‚Äñx‚ÇÄ - x*‚Äñ¬≤
    let x_k := gradientDescentIterates f Œ∑ x‚ÇÄ k
    let x_k1 := gradientDescentIterates f Œ∑ x‚ÇÄ (k + 1)
    -- Key: x_{k+1} = x_k - Œ∑‚àáf(x_k)
    have h_step : x_k1 = x_k - Œ∑ ‚Ä¢ gradient f x_k := rfl
    -- The per-iteration contraction: ‚Äñx_{k+1} - x*‚Äñ¬≤ ‚â§ (1 - Œº/L) ‚Äñx_k - x*‚Äñ¬≤
    --
    -- Proof outline:
    -- 1. Expand: ‚Äñx_{k+1} - x*‚Äñ¬≤ = ‚Äñ(x_k - x*) - Œ∑‚àáf(x_k)‚Äñ¬≤
    --    = ‚Äñx_k - x*‚Äñ¬≤ - 2Œ∑‚ü®‚àáf(x_k), x_k - x*‚ü© + Œ∑¬≤‚Äñ‚àáf(x_k)‚Äñ¬≤
    --
    -- 2. For Œº-strongly convex f with minimum at x*:
    --    ‚ü®‚àáf(x_k), x_k - x*‚ü© ‚â• Œº‚Äñx_k - x*‚Äñ¬≤ + (f(x_k) - f(x*))
    --    (This is the "strong convexity gradient inequality")
    --
    -- 3. For L-smooth f:
    --    ‚Äñ‚àáf(x_k)‚Äñ¬≤ ‚â§ 2L(f(x_k) - f(x*))
    --    (Co-coercivity of gradient)
    -- 4. Combining with Œ∑ = 1/L:
    --    ‚Äñx_{k+1} - x*‚Äñ¬≤ ‚â§ ‚Äñx_k - x*‚Äñ¬≤ - 2Œ∑¬∑Œº‚Äñx_k - x*‚Äñ¬≤
    --                      - 2Œ∑(f(x_k) - f(x*)) + Œ∑¬≤¬∑2L(f(x_k) - f(x*))
    --    = ‚Äñx_k - x*‚Äñ¬≤ - (2Œº/L)‚Äñx_k - x*‚Äñ¬≤
    --    = (1 - 2Œº/L)‚Äñx_k - x*‚Äñ¬≤ ‚â§ (1 - Œº/L)‚Äñx_k - x*‚Äñ¬≤  (since 2Œº/L ‚â• Œº/L)
    -- The formal proof requires the following key lemmas:

    -- Lemma 1: Strong convexity gradient inequality
    -- For Œº-strongly convex f with ‚àáf(x*) = 0:
    -- ‚ü®‚àáf(x), x - x*‚ü© ‚â• Œº‚Äñx - x*‚Äñ¬≤ + (f(x) - f(x*))
    --
    -- This follows from the strong convexity definition:
    -- f(y) ‚â• f(x) + ‚ü®‚àáf(x), y-x‚ü© + (Œº/2)‚Äñx-y‚Äñ¬≤
    -- Setting y = x* and using f(x*) ‚â§ f(x) + ‚ü®‚àáf(x), x*-x‚ü© + (Œº/2)‚Äñx-x*‚Äñ¬≤

    -- Lemma 2: Co-coercivity of L-smooth gradients
    -- For L-smooth f with ‚àáf(x*) = 0:
    -- ‚Äñ‚àáf(x)‚Äñ¬≤ ‚â§ 2L(f(x) - f(x*))
    --
    -- This follows from the descent lemma applied at x:
    -- f(x - (1/L)‚àáf(x)) ‚â§ f(x) - (1/2L)‚Äñ‚àáf(x)‚Äñ¬≤
    -- Since f(x*) is the minimum: f(x*) ‚â§ f(x - (1/L)‚àáf(x))
    -- Therefore: f(x*) ‚â§ f(x) - (1/2L)‚Äñ‚àáf(x)‚Äñ¬≤
    -- Rearranging: ‚Äñ‚àáf(x)‚Äñ¬≤ ‚â§ 2L(f(x) - f(x*))
    have h_contraction : ‚Äñx_k1 - x_star‚Äñ^2 ‚â§ (1 - Œº / L) * ‚Äñx_k - x_star‚Äñ^2 := by
      -- Let g = ‚àáf(x_k)
      let g := gradient f x_k
      -- x_{k+1} - x* = (x_k - x*) - Œ∑¬∑g
      have h_diff : x_k1 - x_star = (x_k - x_star) - Œ∑ ‚Ä¢ g := by
        simp only [h_step]
        abel
      -- ‚Äñx_{k+1} - x*‚Äñ¬≤ = ‚Äñ(x_k - x*) - Œ∑¬∑g‚Äñ¬≤
      --                  = ‚Äñx_k - x*‚Äñ¬≤ - 2Œ∑‚ü®g, x_k - x*‚ü© + Œ∑¬≤‚Äñg‚Äñ¬≤
      have h_expand : ‚Äñx_k1 - x_star‚Äñ^2 =
          ‚Äñx_k - x_star‚Äñ^2 - 2 * Œ∑ * @inner ‚Ñù E _ g (x_k - x_star) + Œ∑^2 * ‚Äñg‚Äñ^2 := by
        rw [h_diff]
        -- Use polarization: ‚Äña - b‚Äñ¬≤ = ‚Äña‚Äñ¬≤ + ‚Äñb‚Äñ¬≤ - 2‚ü®a, b‚ü©
        -- ‚Äña - Œ∑ ‚Ä¢ g‚Äñ¬≤ = ‚Äña‚Äñ¬≤ + ‚ÄñŒ∑ ‚Ä¢ g‚Äñ¬≤ - 2‚ü®a, Œ∑ ‚Ä¢ g‚ü©
        --              = ‚Äña‚Äñ¬≤ + Œ∑¬≤‚Äñg‚Äñ¬≤ - 2Œ∑‚ü®a, g‚ü©
        --              = ‚Äña‚Äñ¬≤ - 2Œ∑‚ü®g, a‚ü© + Œ∑¬≤‚Äñg‚Äñ¬≤ (by inner product symmetry)
        rw [norm_sub_sq_real]
        -- ‚ÄñŒ∑ ‚Ä¢ g‚Äñ¬≤ = (|Œ∑| * ‚Äñg‚Äñ)¬≤ = |Œ∑|¬≤ * ‚Äñg‚Äñ¬≤ = Œ∑¬≤ * ‚Äñg‚Äñ¬≤
        have h_norm_smul_sq : ‚ÄñŒ∑ ‚Ä¢ g‚Äñ^2 = Œ∑^2 * ‚Äñg‚Äñ^2 := by
          rw [norm_smul, Real.norm_eq_abs, mul_pow, sq_abs]
        rw [h_norm_smul_sq]
        -- ‚ü®a, Œ∑ ‚Ä¢ g‚ü© = Œ∑ * ‚ü®a, g‚ü© = Œ∑ * ‚ü®g, a‚ü© (by symmetry)
        rw [inner_smul_right, real_inner_comm]
        ring
      -- Now use h_expand and bound each term. From Œ∑ = 1/L:
      have h_eta : Œ∑ = 1 / L := hŒ∑
      have h_eta_sq : Œ∑^2 = 1 / L^2 := by rw [h_eta]; ring
      -- Use the interpolation condition which combines strong convexity and smoothness
      have h_interp := strong_smooth_interpolation f L Œº hL hŒº hSmooth hStrong x_k x_star hMin
      -- Let inner_val = ‚ü®g, x_k - x*‚ü© for clarity
      let inner_val := @inner ‚Ñù E _ g (x_k - x_star)
      -- From h_expand: ‚Äñx_{k+1} - x*‚Äñ¬≤ = ‚Äñx_k - x*‚Äñ¬≤ - 2Œ∑¬∑inner_val + Œ∑¬≤‚Äñg‚Äñ¬≤
      -- With Œ∑ = 1/L: = ‚Äñx_k - x*‚Äñ¬≤ - (2/L)¬∑inner_val + (1/L¬≤)‚Äñg‚Äñ¬≤
      -- From interpolation: inner_val ‚â• (ŒºL)/(Œº+L)‚Äñx_k - x*‚Äñ¬≤ + 1/(Œº+L)‚Äñg‚Äñ¬≤
      -- So: -(2/L)¬∑inner_val ‚â§ -(2/L)¬∑[(ŒºL)/(Œº+L)‚Äñx_k - x*‚Äñ¬≤ + 1/(Œº+L)‚Äñg‚Äñ¬≤]
      --                      = -(2Œº)/(Œº+L)‚Äñx_k - x*‚Äñ¬≤ - 2/(L(Œº+L))‚Äñg‚Äñ¬≤. Combined:
      -- ‚Äñx_{k+1} - x*‚Äñ¬≤ ‚â§ ‚Äñx_k - x*‚Äñ¬≤ - (2Œº)/(Œº+L)‚Äñx_k - x*‚Äñ¬≤ + [1/L¬≤ - 2/(L(Œº+L))]‚Äñg‚Äñ¬≤
      --
      -- The coefficient of ‚Äñg‚Äñ¬≤:
      -- 1/L¬≤ - 2/(L(Œº+L)) = [(Œº+L) - 2L] / [L¬≤(Œº+L)] = (Œº-L) / [L¬≤(Œº+L)] ‚â§ 0 (since Œº ‚â§ L)
      --
      -- So we can drop the ‚Äñg‚Äñ¬≤ term:
      -- ‚Äñx_{k+1} - x*‚Äñ¬≤ ‚â§ ‚Äñx_k - x*‚Äñ¬≤ - (2Œº)/(Œº+L)‚Äñx_k - x*‚Äñ¬≤
      --                = [1 - 2Œº/(Œº+L)]‚Äñx_k - x*‚Äñ¬≤
      --                = [(Œº+L-2Œº)/(Œº+L)]‚Äñx_k - x*‚Äñ¬≤
      --                = [(L-Œº)/(L+Œº)]‚Äñx_k - x*‚Äñ¬≤
      --
      -- Finally: (L-Œº)/(L+Œº) ‚â§ 1 - Œº/L because:
      -- (L-Œº)/(L+Œº) ‚â§ (L-Œº)/L = 1 - Œº/L iff L+Œº ‚â• L, which is true since Œº > 0
      have h_coeff_neg : 1 / L^2 - 2 / (L * (Œº + L)) ‚â§ 0 := by
        have h3 : 1 / L^2 - 2 / (L * (Œº + L)) = (Œº - L) / (L^2 * (Œº + L)) := by field_simp; ring
        rw [h3]
        apply div_nonpos_of_nonpos_of_nonneg
        ¬∑ linarith  -- Œº - L ‚â§ 0 since Œº ‚â§ L
        ¬∑ apply mul_nonneg (sq_nonneg L)
          linarith  -- Œº + L > 0
      have h_contraction_factor : (L - Œº) / (L + Œº) ‚â§ 1 - Œº / L := by
        have h1 : (L - Œº) / (L + Œº) ‚â§ (L - Œº) / L := by
          apply div_le_div_of_nonneg_left
          ¬∑ linarith  -- L - Œº ‚â• 0
          ¬∑ linarith  -- L > 0
          ¬∑ linarith  -- L + Œº ‚â• L
        have h2 : (L - Œº) / L = 1 - Œº / L := by field_simp
        linarith
      -- Chain h_expand with h_interp and algebraic bounds
      -- Goal: ‚Äñx_k1 - x_star‚Äñ^2 ‚â§ (1 - Œº / L) * ‚Äñx_k - x_star‚Äñ^2
      --
      -- From h_expand (with Œ∑ = 1/L):
      -- ‚Äñx_k1 - x_star‚Äñ^2 = ‚Äñx_k - x_star‚Äñ^2 - (2/L)‚ü®g, x_k - x*‚ü© + (1/L¬≤)‚Äñg‚Äñ¬≤
      --
      -- From h_interp (assuming strong_smooth_interpolation is proved):
      -- ‚ü®g, x_k - x*‚ü© ‚â• (ŒºL)/(Œº+L)‚Äñx_k - x*‚Äñ¬≤ + 1/(Œº+L)‚Äñg‚Äñ¬≤
      --
      -- Substituting:
      -- ‚Äñx_k1 - x_star‚Äñ^2 ‚â§ ‚Äñx_k - x*‚Äñ¬≤ - (2/L)¬∑[(ŒºL)/(Œº+L)‚Äñx_k - x*‚Äñ¬≤ + 1/(Œº+L)‚Äñg‚Äñ¬≤] + (1/L¬≤)‚Äñg‚Äñ¬≤
      --                    = ‚Äñx_k - x*‚Äñ¬≤ - (2Œº)/(Œº+L)‚Äñx_k - x*‚Äñ¬≤ + [1/L¬≤ - 2/(L(Œº+L))]‚Äñg‚Äñ¬≤
      --
      -- By h_coeff_neg, the coefficient of ‚Äñg‚Äñ¬≤ is ‚â§ 0, and ‚Äñg‚Äñ¬≤ ‚â• 0, so:
      -- ‚Äñx_k1 - x_star‚Äñ^2 ‚â§ ‚Äñx_k - x*‚Äñ¬≤ - (2Œº)/(Œº+L)‚Äñx_k - x*‚Äñ¬≤
      --                    = [1 - 2Œº/(Œº+L)]‚Äñx_k - x*‚Äñ¬≤
      --                    = [(L-Œº)/(L+Œº)]‚Äñx_k - x*‚Äñ¬≤
      --
      -- By h_contraction_factor: (L-Œº)/(L+Œº) ‚â§ 1 - Œº/L
      -- First compute the coefficient 1 - 2Œº/(Œº+L) = (L-Œº)/(L+Œº)
      have h_coeff_eq : 1 - 2 * Œº / (Œº + L) = (L - Œº) / (L + Œº) := by
        field_simp
        ring
      -- Combine everything using transitivity. The proof depends on
      -- strong_smooth_interpolation which currently has a sorry.
      -- Key inequality from h_interp:
      have h_inner_bound : inner_val ‚â• (Œº * L) / (Œº + L) * ‚Äñx_k - x_star‚Äñ^2 +
                                        1 / (Œº + L) * ‚Äñg‚Äñ^2 := h_interp
      -- Substitute Œ∑ = 1/L into h_expand
      have h_expand' : ‚Äñx_k1 - x_star‚Äñ^2 =
          ‚Äñx_k - x_star‚Äñ^2 - 2 / L * inner_val + 1 / L^2 * ‚Äñg‚Äñ^2 := by
        rw [h_expand, h_eta]; ring
      -- Apply the bound on inner_val
      have h_step1 : ‚Äñx_k1 - x_star‚Äñ^2 ‚â§
          ‚Äñx_k - x_star‚Äñ^2 - 2 / L * ((Œº * L) / (Œº + L) * ‚Äñx_k - x_star‚Äñ^2 +
                                       1 / (Œº + L) * ‚Äñg‚Äñ^2) + 1 / L^2 * ‚Äñg‚Äñ^2 := by
        rw [h_expand']
        have h_L_pos : 0 < L := hL
        have h_2L_pos : 0 < 2 / L := by positivity
        nlinarith [h_inner_bound, sq_nonneg ‚Äñg‚Äñ, sq_nonneg ‚Äñx_k - x_star‚Äñ]
      -- Simplify to get the coefficient form
      have h_step2 : ‚Äñx_k1 - x_star‚Äñ^2 ‚â§
          ‚Äñx_k - x_star‚Äñ^2 - 2 * Œº / (Œº + L) * ‚Äñx_k - x_star‚Äñ^2 +
          (1 / L^2 - 2 / (L * (Œº + L))) * ‚Äñg‚Äñ^2 := by
        calc ‚Äñx_k1 - x_star‚Äñ^2
            ‚â§ ‚Äñx_k - x_star‚Äñ^2 - 2 / L * ((Œº * L) / (Œº + L) * ‚Äñx_k - x_star‚Äñ^2 +
                                           1 / (Œº + L) * ‚Äñg‚Äñ^2) + 1 / L^2 * ‚Äñg‚Äñ^2 := h_step1
          _ = ‚Äñx_k - x_star‚Äñ^2 - 2 * Œº / (Œº + L) * ‚Äñx_k - x_star‚Äñ^2 +
              (1 / L^2 - 2 / (L * (Œº + L))) * ‚Äñg‚Äñ^2 := by
            have hL_ne : L ‚â† 0 := ne_of_gt hL
            have hŒºL_ne : Œº + L ‚â† 0 := by linarith
            field_simp
            ring
      -- Drop the ‚Äñg‚Äñ¬≤ term (coefficient is ‚â§ 0)
      have h_step3 : ‚Äñx_k1 - x_star‚Äñ^2 ‚â§
          ‚Äñx_k - x_star‚Äñ^2 - 2 * Œº / (Œº + L) * ‚Äñx_k - x_star‚Äñ^2 := by
        have h_g_sq_nonneg : 0 ‚â§ ‚Äñg‚Äñ^2 := sq_nonneg _
        nlinarith [h_step2, h_coeff_neg, h_g_sq_nonneg]
      -- Factor and apply contraction bound
      calc ‚Äñx_k1 - x_star‚Äñ^2
          ‚â§ ‚Äñx_k - x_star‚Äñ^2 - 2 * Œº / (Œº + L) * ‚Äñx_k - x_star‚Äñ^2 := h_step3
        _ = (1 - 2 * Œº / (Œº + L)) * ‚Äñx_k - x_star‚Äñ^2 := by ring
        _ = (L - Œº) / (L + Œº) * ‚Äñx_k - x_star‚Äñ^2 := by rw [h_coeff_eq]
        _ ‚â§ (1 - Œº / L) * ‚Äñx_k - x_star‚Äñ^2 := by
            apply mul_le_mul_of_nonneg_right h_contraction_factor (sq_nonneg _)
    -- Apply contraction and inductive hypothesis
    calc ‚Äñx_k1 - x_star‚Äñ^2
        ‚â§ (1 - Œº / L) * ‚Äñx_k - x_star‚Äñ^2 := h_contraction
      _ ‚â§ (1 - Œº / L) * ((1 - Œº / L)^k * ‚Äñx‚ÇÄ - x_star‚Äñ^2) := by {
          apply mul_le_mul_of_nonneg_left ih
          have h1 : Œº / L ‚â§ 1 := (div_le_one (by linarith : 0 < L)).mpr hŒºL
          linarith
        }
      _ = (1 - Œº / L)^(k + 1) * ‚Äñx‚ÇÄ - x_star‚Äñ^2 := by ring

/-- The descent lemma: one step decreases function value.

The proof follows from L-smoothness:
1. By L-smoothness: f(y) ‚â§ f(x) + ‚ü®‚àáf(x), y-x‚ü© + (L/2)‚Äñy-x‚Äñ¬≤
2. With y = x - Œ∑‚àáf(x), we have y - x = -Œ∑‚àáf(x)
3. So ‚ü®‚àáf(x), y-x‚ü© = -Œ∑‚Äñ‚àáf(x)‚Äñ¬≤
4. And ‚Äñy-x‚Äñ¬≤ = Œ∑¬≤‚Äñ‚àáf(x)‚Äñ¬≤
5. Thus: f(y) ‚â§ f(x) - Œ∑‚Äñ‚àáf(x)‚Äñ¬≤ + (LŒ∑¬≤/2)‚Äñ‚àáf(x)‚Äñ¬≤
6. Since Œ∑ ‚â§ 1/L, we have (LŒ∑¬≤/2) ‚â§ Œ∑/2
7. Therefore: f(y) ‚â§ f(x) - (Œ∑/2)‚Äñ‚àáf(x)‚Äñ¬≤

The key insight is that L-smoothness provides a second-order bound on function values,
which allows us to show descent over a single gradient step.
-/
theorem descent_lemma (f : E ‚Üí ‚Ñù) (L : ‚Ñù) (hL : 0 < L)
    (hSmooth : IsLSmooth f L) (x : E) (Œ∑ : ‚Ñù) (hŒ∑ : 0 < Œ∑) (hŒ∑L : Œ∑ ‚â§ 1 / L) :
    f (gradientDescentStep f Œ∑ x) ‚â§ f x - (Œ∑ / 2) * ‚Äñgradient f x‚Äñ^2 := by
  -- Define y = x - Œ∑‚àáf(x) (the gradient descent step)
  let y := x - Œ∑ ‚Ä¢ gradient f x
  let g := gradient f x
  -- Step 1: Apply the fundamental inequality for L-smooth functions
  have h_fund := lsmooth_fundamental_ineq f L (le_of_lt hL) hSmooth x y
  -- Step 2: Compute y - x = -(Œ∑ ‚Ä¢ ‚àáf(x))
  have h_diff : y - x = -(Œ∑ ‚Ä¢ g) := by simp only [y, g]; abel
  -- Step 3: Compute ‚ü®‚àáf(x), y - x‚ü© = -Œ∑‚Äñ‚àáf(x)‚Äñ¬≤
  have h_inner : @inner ‚Ñù E _ g (y - x) = -Œ∑ * ‚Äñg‚Äñ^2 := by
    rw [h_diff, inner_neg_right, inner_smul_right]
    rw [real_inner_self_eq_norm_sq]
    ring
  -- Step 4: Compute ‚Äñy - x‚Äñ¬≤ = Œ∑¬≤‚Äñ‚àáf(x)‚Äñ¬≤
  have h_norm_sq : ‚Äñy - x‚Äñ^2 = Œ∑^2 * ‚Äñg‚Äñ^2 := by
    rw [h_diff, norm_neg, norm_smul, Real.norm_eq_abs]
    have : |Œ∑|^2 = Œ∑^2 := sq_abs Œ∑
    rw [mul_pow, this]
  -- Step 5: Substitute into the fundamental inequality
  -- f(y) ‚â§ f(x) + ‚ü®‚àáf(x), y - x‚ü© + (L/2)‚Äñy - x‚Äñ¬≤
  --      = f(x) - Œ∑‚Äñ‚àáf(x)‚Äñ¬≤ + (L/2)Œ∑¬≤‚Äñ‚àáf(x)‚Äñ¬≤
  --      = f(x) + (-Œ∑ + LŒ∑¬≤/2)‚Äñ‚àáf(x)‚Äñ¬≤
  calc f y ‚â§ f x + @inner ‚Ñù E _ g (y - x) + (L / 2) * ‚Äñy - x‚Äñ^2 := h_fund
    _ = f x + (-Œ∑ * ‚Äñg‚Äñ^2) + (L / 2) * (Œ∑^2 * ‚Äñg‚Äñ^2) := by rw [h_inner, h_norm_sq]
    _ = f x + (-Œ∑ + L * Œ∑^2 / 2) * ‚Äñg‚Äñ^2 := by ring
    _ ‚â§ f x + (-Œ∑ / 2) * ‚Äñg‚Äñ^2 := by {
        -- Need: -Œ∑ + L*Œ∑¬≤/2 ‚â§ -Œ∑/2
        -- i.e., L*Œ∑¬≤/2 ‚â§ Œ∑/2
        -- i.e., L*Œ∑ ‚â§ 1
        -- which follows from Œ∑ ‚â§ 1/L
        have h_LŒ∑ : L * Œ∑ ‚â§ 1 := by
          calc L * Œ∑ = Œ∑ * L := mul_comm L Œ∑
            _ ‚â§ (1 / L) * L := mul_le_mul_of_nonneg_right hŒ∑L (le_of_lt hL)
            _ = 1 := div_mul_cancel‚ÇÄ 1 (ne_of_gt hL)
        have h_coeff : -Œ∑ + L * Œ∑^2 / 2 ‚â§ -Œ∑ / 2 := by
          have h1 : L * Œ∑^2 / 2 ‚â§ Œ∑ / 2 := by
            have : L * Œ∑^2 ‚â§ Œ∑ := by
              calc L * Œ∑^2 = (L * Œ∑) * Œ∑ := by ring
                _ ‚â§ 1 * Œ∑ := mul_le_mul_of_nonneg_right h_LŒ∑ (le_of_lt hŒ∑)
                _ = Œ∑ := one_mul Œ∑
            linarith
          linarith
        have h_g_sq_nonneg : 0 ‚â§ ‚Äñg‚Äñ^2 := sq_nonneg _
        nlinarith [sq_nonneg ‚Äñg‚Äñ]
      }
    _ = f x - (Œ∑ / 2) * ‚Äñg‚Äñ^2 := by ring

end Gradient
