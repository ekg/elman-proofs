/-
Copyright (c) 2024 Elman Ablation Ladder Project. All rights reserved.
Released under Apache 2.0 license as described in the file LICENSE.
Authors: Elman Ablation Ladder Team
-/

import Mathlib.Analysis.Calculus.Gradient.Basic
import Mathlib.Analysis.Convex.Basic
import Mathlib.Analysis.Calculus.MeanValue
import Mathlib.Analysis.Calculus.Deriv.Basic
import Mathlib.Analysis.Calculus.Deriv.MeanValue
import Mathlib.Analysis.InnerProductSpace.Calculus

/-!
# Gradient Flow and Learning Dynamics

This file formalizes gradient descent as a dynamical system and proves
convergence results relevant to neural network training.

## Main Definitions

* `GradientDescentStep`: One step of gradient descent
* `IsLSmooth`: Function with L-Lipschitz gradient
* `IsStronglyConvex`: Œº-strongly convex function

## Main Theorems

* `gradient_descent_convex`: O(1/k) convergence for convex functions
* `gradient_descent_strongly_convex`: O(c^k) convergence for strongly convex

## Application to RNN Training

For RNN training with loss L(Œ∏):
- If L is L-smooth and Œº-strongly convex, gradient descent converges linearly
- The condition number Œ∫ = L/Œº determines convergence rate

-/

namespace Gradient

variable {E : Type*} [NormedAddCommGroup E] [InnerProductSpace ‚Ñù E] [CompleteSpace E]

/-- A function is L-smooth if its gradient is L-Lipschitz. -/
def IsLSmooth (f : E ‚Üí ‚Ñù) (L : ‚Ñù) : Prop :=
  Differentiable ‚Ñù f ‚àß ‚àÄ x y, ‚Äñgradient f x - gradient f y‚Äñ ‚â§ L * ‚Äñx - y‚Äñ

/-- A function is Œº-strongly convex. -/
def IsStronglyConvex (f : E ‚Üí ‚Ñù) (Œº : ‚Ñù) : Prop :=
  ‚àÄ x y : E, ‚àÄ t : ‚Ñù, 0 ‚â§ t ‚Üí t ‚â§ 1 ‚Üí
    f (t ‚Ä¢ x + (1 - t) ‚Ä¢ y) ‚â§ t * f x + (1 - t) * f y - (Œº / 2) * t * (1 - t) * ‚Äñx - y‚Äñ^2

/-- Strong convexity implies ordinary convexity.

    If f is Œº-strongly convex with Œº ‚â• 0, then f is convex on the whole space.
-/
theorem stronglyConvex_implies_convexOn (f : E ‚Üí ‚Ñù) (Œº : ‚Ñù) (hŒº : 0 ‚â§ Œº)
    (hStrong : IsStronglyConvex f Œº) : ConvexOn ‚Ñù Set.univ f := by
  constructor
  ¬∑ exact convex_univ
  ¬∑ intro x _ y _ a b ha hb hab
    -- ConvexOn uses weights a, b with a + b = 1
    -- IsStronglyConvex uses t with (1-t)
    -- We have: a ‚Ä¢ x + b ‚Ä¢ y where a + b = 1
    -- We want to show: f (a ‚Ä¢ x + b ‚Ä¢ y) ‚â§ a * f x + b * f y
    -- Using IsStronglyConvex with t = a:
    -- f (a ‚Ä¢ x + (1 - a) ‚Ä¢ y) ‚â§ a * f x + (1 - a) * f y - (Œº/2) * a * (1 - a) * ‚Äñx - y‚Äñ¬≤
    have hb_eq : b = 1 - a := by linarith
    rw [hb_eq]
    have ha_le_1 : a ‚â§ 1 := by linarith
    have h1_minus_a_nonneg : 0 ‚â§ 1 - a := by linarith
    have hStrong' := hStrong x y a ha ha_le_1
    have h_nonneg : 0 ‚â§ (Œº / 2) * a * (1 - a) * ‚Äñx - y‚Äñ^2 := by
      have h1 : 0 ‚â§ Œº / 2 := by linarith
      have h2 : 0 ‚â§ a * (1 - a) := mul_nonneg ha h1_minus_a_nonneg
      have h3 : 0 ‚â§ (Œº / 2) * (a * (1 - a)) := mul_nonneg h1 h2
      have h4 : 0 ‚â§ ‚Äñx - y‚Äñ ^ 2 := sq_nonneg _
      calc (Œº / 2) * a * (1 - a) * ‚Äñx - y‚Äñ^2
          = (Œº / 2) * (a * (1 - a)) * ‚Äñx - y‚Äñ^2 := by ring
        _ ‚â• 0 := mul_nonneg h3 h4
    -- Convert smul to mul for reals: a ‚Ä¢ r = a * r
    simp only [smul_eq_mul] at *
    linarith

/-- Strong convexity implies a lower bound on the gradient inner product.

    For Œº-strongly convex f with ‚àáf(x*) = 0:
    ‚ü®‚àáf(x), x - x*‚ü© ‚â• (Œº/2)‚Äñx - x*‚Äñ¬≤

    This follows from the first-order characterization of strong convexity.
-/
theorem strong_convex_gradient_lower_bound (f : E ‚Üí ‚Ñù) (Œº : ‚Ñù) (hŒº : 0 < Œº)
    (hStrong : IsStronglyConvex f Œº) (hDiff : Differentiable ‚Ñù f)
    (x x_star : E) (hMin : gradient f x_star = 0) :
    @inner ‚Ñù E _ (gradient f x) (x - x_star) ‚â• (Œº / 2) * ‚Äñx - x_star‚Äñ^2 := by
  /- The proof uses the first-order characterization of strong convexity.

     For Œº-strongly convex f, the definition gives:
     f(t¬∑a + (1-t)¬∑b) ‚â§ t¬∑f(a) + (1-t)¬∑f(b) - (Œº/2)¬∑t¬∑(1-t)¬∑‚Äña - b‚Äñ¬≤

     The first-order characterization (for differentiable f) is:
     f(y) ‚â• f(x) + ‚ü®‚àáf(x), y - x‚ü© + (Œº/2)¬∑‚Äñy - x‚Äñ¬≤

     Setting y = x* (where ‚àáf(x*) = 0):
     f(x*) ‚â• f(x) + ‚ü®‚àáf(x), x* - x‚ü© + (Œº/2)¬∑‚Äñx* - x‚Äñ¬≤

     Rearranging:
     ‚ü®‚àáf(x), x - x*‚ü© = -‚ü®‚àáf(x), x* - x‚ü© ‚â• f(x) - f(x*) + (Œº/2)¬∑‚Äñx - x*‚Äñ¬≤

     Since x* is a critical point (‚àáf(x*) = 0) for a strongly convex function,
     it's the unique global minimum, so f(x) - f(x*) ‚â• 0.

     Therefore: ‚ü®‚àáf(x), x - x*‚ü© ‚â• (Œº/2)¬∑‚Äñx - x*‚Äñ¬≤

     The key step requiring formalization is deriving the first-order
     characterization from the definition of IsStronglyConvex.
     This typically requires taking limits as t ‚Üí 0 in the definition.
  -/

  -- The formal proof requires:
  -- 1. Deriving first-order characterization from IsStronglyConvex
  -- 2. Using that ‚àáf(x*) = 0 implies x* is global minimum for strongly convex f
  -- 3. Combining the bounds

  -- Key derivation: From the strong convexity definition with a = x, b = x*, t ‚àà (0,1]:
  -- f(t‚Ä¢x + (1-t)‚Ä¢x*) ‚â§ t‚Ä¢f(x) + (1-t)‚Ä¢f(x*) - (Œº/2)‚Ä¢t‚Ä¢(1-t)‚Ä¢‚Äñx - x*‚Äñ¬≤
  --
  -- Rearranging: f(x* + t(x - x*)) ‚â§ f(x*) + t(f(x) - f(x*)) - (Œº/2)t(1-t)‚Äñx - x*‚Äñ¬≤
  --
  -- Taking the derivative w.r.t. t at t = 0 (using differentiability):
  -- LHS derivative: ‚ü®‚àáf(x*), x - x*‚ü© = 0 (since ‚àáf(x*) = 0)
  -- RHS derivative: f(x) - f(x*) - (Œº/2)(1)‚Äñx - x*‚Äñ¬≤ = f(x) - f(x*) - (Œº/2)‚Äñx - x*‚Äñ¬≤
  --
  -- Wait, this gives information at x*, not at x. Let me use a = x*, b = x instead:
  -- f(t‚Ä¢x* + (1-t)‚Ä¢x) ‚â§ t‚Ä¢f(x*) + (1-t)‚Ä¢f(x) - (Œº/2)‚Ä¢t‚Ä¢(1-t)‚Ä¢‚Äñx* - x‚Äñ¬≤
  --
  -- Rewrite LHS: f(x + t(x* - x)) = f(x - t(x - x*))
  --
  -- Taking derivative w.r.t. t at t = 0:
  -- LHS: ‚ü®‚àáf(x), x* - x‚ü© = -‚ü®‚àáf(x), x - x*‚ü©
  -- RHS: (d/dt)[t‚Ä¢f(x*) + (1-t)‚Ä¢f(x) - (Œº/2)‚Ä¢t‚Ä¢(1-t)‚Ä¢‚Äñx - x*‚Äñ¬≤] at t=0
  --     = f(x*) - f(x) - (Œº/2)‚Ä¢(1-2t)‚Ä¢‚Äñx - x*‚Äñ¬≤ at t=0
  --     = f(x*) - f(x) - (Œº/2)‚Äñx - x*‚Äñ¬≤
  --
  -- The strong convexity inequality gives: LHS ‚â§ RHS (as t ‚Üí 0‚Å∫)
  -- -‚ü®‚àáf(x), x - x*‚ü© ‚â§ f(x*) - f(x) - (Œº/2)‚Äñx - x*‚Äñ¬≤
  -- ‚ü®‚àáf(x), x - x*‚ü© ‚â• f(x) - f(x*) + (Œº/2)‚Äñx - x*‚Äñ¬≤
  --
  -- Since x* is a critical point of strongly convex f, it's the global minimum:
  -- f(x) - f(x*) ‚â• 0
  --
  -- Therefore: ‚ü®‚àáf(x), x - x*‚ü© ‚â• (Œº/2)‚Äñx - x*‚Äñ¬≤

  -- The formal proof requires taking limits as t ‚Üí 0 in the strong convexity
  -- definition and using differentiability. This involves:
  -- 1. Showing the function g(t) = f(x + t(x* - x)) is differentiable at t = 0
  -- 2. Computing g'(0) = ‚ü®‚àáf(x), x* - x‚ü©
  -- 3. Bounding g(t) using strong convexity
  -- 4. Taking the limit to get the first-order condition

  -- Define the direction and path
  let d := x_star - x
  let g := fun t : ‚Ñù => f (x + t ‚Ä¢ d)
  -- The upper bound from strong convexity: h(t) = (1-t)f(x) + tf(x*) - (Œº/2)t(1-t)‚Äñd‚Äñ¬≤
  let h := fun t : ‚Ñù => (1 - t) * f x + t * f x_star - (Œº / 2) * t * (1 - t) * ‚Äñd‚Äñ^2
  -- Strong convexity gives g(t) ‚â§ h(t) for t ‚àà [0, 1]
  have h_ineq : ‚àÄ t, 0 ‚â§ t ‚Üí t ‚â§ 1 ‚Üí g t ‚â§ h t := by
    intro t ht0 ht1
    have hconv := hStrong x_star x t ht0 ht1
    -- t‚Ä¢x* + (1-t)‚Ä¢x = x + t‚Ä¢(x* - x) = x + t‚Ä¢d
    have heq : t ‚Ä¢ x_star + (1 - t) ‚Ä¢ x = x + t ‚Ä¢ d := by
      simp only [d]; rw [smul_sub]; ring_nf; module
    simp only [g, h, heq] at hconv ‚ä¢
    have hnorm : ‚Äñx_star - x‚Äñ = ‚Äñd‚Äñ := by simp only [d]
    rw [hnorm] at hconv
    linarith
  -- At t = 0: g(0) = h(0) = f(x)
  have hg0 : g 0 = f x := by simp only [g, zero_smul, add_zero]
  have hh0 : h 0 = f x := by simp only [h]; ring
  -- Compute h'(0) = f(x*) - f(x) - (Œº/2)‚Äñd‚Äñ¬≤
  have h_deriv : HasDerivAt h (f x_star - f x - (Œº / 2) * ‚Äñd‚Äñ^2) 0 := by
    -- h(t) = (1-t)f(x) + tf(x*) - (Œº/2)t(1-t)‚Äñd‚Äñ¬≤
    -- Rewrite as: h(t) = f(x) + t*(f(x*) - f(x)) - (Œº/2)*‚Äñd‚Äñ¬≤*(t - t¬≤)
    -- h'(t) = f(x*) - f(x) - (Œº/2)*‚Äñd‚Äñ¬≤*(1 - 2t)
    -- h'(0) = f(x*) - f(x) - (Œº/2)*‚Äñd‚Äñ¬≤
    have h1 : HasDerivAt (fun t : ‚Ñù => (1 - t) * f x) (-f x) 0 := by
      have hid : HasDerivAt (fun t : ‚Ñù => 1 - t) (-1) 0 :=
        (hasDerivAt_const (0 : ‚Ñù) (1 : ‚Ñù)).sub (hasDerivAt_id (0 : ‚Ñù)) |>.congr_deriv (by ring)
      convert hid.mul_const (f x) using 1; ring
    have h2 : HasDerivAt (fun t : ‚Ñù => t * f x_star) (f x_star) 0 := by
      convert (hasDerivAt_id (0 : ‚Ñù)).mul_const (f x_star) using 1; ring
    have h3 : HasDerivAt (fun t : ‚Ñù => (Œº / 2) * t * (1 - t) * ‚Äñd‚Äñ^2) ((Œº / 2) * ‚Äñd‚Äñ^2) 0 := by
      -- (Œº/2)*t*(1-t)*‚Äñd‚Äñ¬≤ has derivative (Œº/2)*‚Äñd‚Äñ¬≤*(1 - 2t) at t
      -- At t = 0: (Œº/2)*‚Äñd‚Äñ¬≤
      have hpoly : HasDerivAt (fun t : ‚Ñù => t * (1 - t)) 1 0 := by
        have h1' := hasDerivAt_id (0 : ‚Ñù)
        have h2' : HasDerivAt (fun t : ‚Ñù => 1 - t) (-1) 0 :=
          (hasDerivAt_const (0 : ‚Ñù) (1 : ‚Ñù)).sub (hasDerivAt_id (0 : ‚Ñù)) |>.congr_deriv (by ring)
        have hprod := h1'.mul h2'
        convert hprod using 2 <;> simp
      convert hpoly.const_mul ((Œº / 2) * ‚Äñd‚Äñ^2) using 1
      ¬∑ ext t; ring
      ¬∑ ring
    convert (h1.add h2).sub h3 using 1; ring
  -- Compute g'(0) = ‚ü®‚àáf(x), d‚ü©
  have g_deriv : HasDerivAt g (@inner ‚Ñù E _ (gradient f x) d) 0 := by
    have hŒ≥ : HasDerivAt (fun t : ‚Ñù => x + t ‚Ä¢ d) d 0 := by
      have h1 : HasDerivAt (fun _ : ‚Ñù => x) 0 0 := hasDerivAt_const 0 x
      have h2 : HasDerivAt (fun t : ‚Ñù => t ‚Ä¢ d) ((1 : ‚Ñù) ‚Ä¢ d) 0 :=
        (hasDerivAt_id 0).smul_const d
      have hsum := h1.add h2
      simp only [zero_add, one_smul] at hsum
      exact hsum
    have hf_grad : HasGradientAt f (gradient f x) x := (hDiff x).hasGradientAt
    have hf_fderiv : HasFDerivAt f (innerSL (ùïú := ‚Ñù) (gradient f x)) x := hf_grad.hasFDerivAt
    have hf_fderiv' : HasFDerivAt f (innerSL (ùïú := ‚Ñù) (gradient f x)) (x + (0 : ‚Ñù) ‚Ä¢ d) := by
      simp only [zero_smul, add_zero]; exact hf_fderiv
    have hcomp := hf_fderiv'.comp_hasDerivAt (0 : ‚Ñù) hŒ≥
    simp only [Function.comp_apply, innerSL_apply_apply, zero_smul, add_zero] at hcomp
    exact hcomp
  -- Key lemma: if g(0) = h(0) and g(t) ‚â§ h(t) for t ‚àà (0, 1], then g'(0) ‚â§ h'(0)
  -- This follows from: (g(t) - g(0))/t ‚â§ (h(t) - h(0))/t for t > 0
  -- Taking limit as t ‚Üí 0‚Å∫ gives g'(0) ‚â§ h'(0)
  have h_deriv_ineq : @inner ‚Ñù E _ (gradient f x) d ‚â§ f x_star - f x - (Œº / 2) * ‚Äñd‚Äñ^2 := by
    by_contra hcontra
    push_neg at hcontra
    -- Let Œ¥ = g'(0) - h'(0) > 0
    let Œ¥ := @inner ‚Ñù E _ (gradient f x) d - (f x_star - f x - (Œº / 2) * ‚Äñd‚Äñ^2)
    have hŒ¥_pos : Œ¥ > 0 := by simp only [Œ¥]; linarith
    -- From HasDerivAt, the difference quotient converges to the derivative
    -- For g: (g(t) - g(0))/t ‚Üí g'(0) as t ‚Üí 0
    -- For h: (h(t) - h(0))/t ‚Üí h'(0) as t ‚Üí 0
    -- So (g(t) - h(t))/t ‚Üí g'(0) - h'(0) = Œ¥ > 0
    have h_gh_deriv : HasDerivAt (fun t => g t - h t) Œ¥ 0 := HasDerivAt.sub g_deriv h_deriv
    have h_gh_0 : (fun t => g t - h t) 0 = 0 := by simp only [hg0, hh0, sub_self]
    -- HasDerivAt gives: (g-h)(t) = (g-h)(0) + Œ¥*t + o(t) = Œ¥*t + o(t)
    -- For small t > 0: (g-h)(t) ‚âà Œ¥*t > 0 since Œ¥ > 0
    rw [hasDerivAt_iff_isLittleO] at h_gh_deriv
    -- h_gh_deriv : (fun t => (g-h)(0+t) - (g-h)(0) - (t-0)‚Ä¢Œ¥) =o[ùìù 0] (fun t => t-0)
    -- Use IsLittleO.def to get: for c = Œ¥/2 > 0, eventually ‚Äñ...‚Äñ ‚â§ c * ‚Äñt - 0‚Äñ
    have hŒµ_half : 0 < Œ¥ / 2 := by linarith
    have h_bound_evt := h_gh_deriv.def hŒµ_half
    -- h_bound_evt : ‚àÄ·∂† t in ùìù 0, ‚Äñ(g t - h t) - (g 0 - h 0) - (t - 0) ‚Ä¢ Œ¥‚Äñ ‚â§ (Œ¥/2) * ‚Äñt - 0‚Äñ
    simp only [h_gh_0, sub_zero, smul_eq_mul] at h_bound_evt
    -- h_bound_evt : ‚àÄ·∂† t in ùìù 0, ‚Äñg t - h t - t * Œ¥‚Äñ ‚â§ (Œ¥/2) * ‚Äñt‚Äñ
    rw [Filter.eventually_iff_exists_mem] at h_bound_evt
    obtain ‚ü®s, hs_mem, hs_bound‚ü© := h_bound_evt
    rw [Metric.mem_nhds_iff] at hs_mem
    obtain ‚ü®Œµ, hŒµ_pos, hŒµ_sub‚ü© := hs_mem
    -- Pick t = min(Œµ/2, 1/2) > 0
    let t := min (Œµ / 2) (1 / 2)
    have ht_pos : 0 < t := by positivity
    have ht_lt_Œµ : t < Œµ := by simp only [t]; linarith [min_le_left (Œµ / 2) (1 / 2)]
    have ht_le_1 : t ‚â§ 1 := by simp only [t]; linarith [min_le_right (Œµ / 2) (1 / 2)]
    have ht_in_ball : t ‚àà Metric.ball 0 Œµ := by
      simp only [Metric.mem_ball, dist_zero_right, Real.norm_eq_abs, abs_of_pos ht_pos]
      exact ht_lt_Œµ
    have ht_in_s : t ‚àà s := hŒµ_sub ht_in_ball
    -- hs_bound says: ‚Äñ(g-h)(t) - t*Œ¥‚Äñ ‚â§ (Œ¥/2) * ‚Äñt‚Äñ
    have h_bound := hs_bound t ht_in_s
    simp only [Real.norm_eq_abs, abs_of_pos ht_pos] at h_bound
    -- h_bound : |g t - h t - t * Œ¥| ‚â§ (Œ¥ / 2) * t
    -- |f(t) - t*Œ¥| ‚â§ (Œ¥/2)*t means f(t) ‚â• t*Œ¥ - (Œ¥/2)*t = (Œ¥/2)*t > 0
    have h_lower : g t - h t ‚â• t * Œ¥ - (Œ¥ / 2) * t := by
      have h1 : -((Œ¥ / 2) * t) ‚â§ (g t - h t) - t * Œ¥ := by
        have := neg_abs_le (g t - h t - t * Œ¥)
        linarith
      linarith
    have h_diff_pos : g t - h t > 0 := by
      have : t * Œ¥ - (Œ¥ / 2) * t = (Œ¥ / 2) * t := by ring
      rw [this] at h_lower
      have : (Œ¥ / 2) * t > 0 := mul_pos (by linarith) ht_pos
      linarith
    -- But h_ineq says g(t) ‚â§ h(t), contradiction
    have h_le := h_ineq t (le_of_lt ht_pos) ht_le_1
    linarith
  -- Now: ‚ü®‚àáf(x), d‚ü© ‚â§ f(x*) - f(x) - (Œº/2)‚Äñd‚Äñ¬≤
  -- Since d = x* - x, we have ‚ü®‚àáf(x), x - x*‚ü© = -‚ü®‚àáf(x), d‚ü©
  have h_inner_neg : @inner ‚Ñù E _ (gradient f x) (x - x_star) =
      -@inner ‚Ñù E _ (gradient f x) d := by
    simp only [d, ‚Üê inner_neg_right, neg_sub]
  rw [h_inner_neg]
  -- Need: -‚ü®‚àáf(x), d‚ü© ‚â• (Œº/2)‚Äñx - x*‚Äñ¬≤
  -- From h_deriv_ineq: ‚ü®‚àáf(x), d‚ü© ‚â§ f(x*) - f(x) - (Œº/2)‚Äñd‚Äñ¬≤
  -- So: -‚ü®‚àáf(x), d‚ü© ‚â• f(x) - f(x*) + (Œº/2)‚Äñd‚Äñ¬≤
  -- Need to show f(x) - f(x*) ‚â• 0, i.e., x* is global minimum
  have h_min : f x_star ‚â§ f x := by
    -- Use derivative limit argument at x* with ‚àáf(x*) = 0
    -- Define path from x* to x: p(t) = f(x* + t(x - x*))
    -- Strong convexity gives p(t) ‚â§ RHS, and taking derivative limit at t = 0
    -- with p'(0) = ‚ü®‚àáf(x*), x - x*‚ü© = 0 gives the desired inequality.
    let e := x - x_star
    let p := fun t : ‚Ñù => f (x_star + t ‚Ä¢ e)
    let q := fun t : ‚Ñù => t * f x + (1 - t) * f x_star - (Œº / 2) * t * (1 - t) * ‚Äñe‚Äñ^2
    -- Strong convexity gives p(t) ‚â§ q(t) for t ‚àà [0, 1]
    have hpq_ineq : ‚àÄ t, 0 ‚â§ t ‚Üí t ‚â§ 1 ‚Üí p t ‚â§ q t := by
      intro t ht0 ht1
      have hconv := hStrong x x_star t ht0 ht1
      have heq : t ‚Ä¢ x + (1 - t) ‚Ä¢ x_star = x_star + t ‚Ä¢ e := by
        simp only [e]; rw [smul_sub]; ring_nf; module
      simp only [p, q, heq] at hconv ‚ä¢
      have hnorm : ‚Äñx - x_star‚Äñ = ‚Äñe‚Äñ := by simp only [e]
      rw [hnorm] at hconv
      linarith
    -- At t = 0: p(0) = q(0) = f(x*)
    have hp0 : p 0 = f x_star := by simp only [p, zero_smul, add_zero]
    have hq0 : q 0 = f x_star := by simp only [q]; ring
    -- Compute q'(0) = f(x) - f(x*) - (Œº/2)‚Äñe‚Äñ¬≤
    have q_deriv : HasDerivAt q (f x - f x_star - (Œº / 2) * ‚Äñe‚Äñ^2) 0 := by
      have h1 : HasDerivAt (fun t : ‚Ñù => t * f x) (f x) 0 := by
        convert (hasDerivAt_id (0 : ‚Ñù)).mul_const (f x) using 1; ring
      have h2 : HasDerivAt (fun t : ‚Ñù => (1 - t) * f x_star) (-f x_star) 0 := by
        have hid : HasDerivAt (fun t : ‚Ñù => 1 - t) (-1) 0 :=
          (hasDerivAt_const (0 : ‚Ñù) (1 : ‚Ñù)).sub (hasDerivAt_id (0 : ‚Ñù)) |>.congr_deriv (by ring)
        convert hid.mul_const (f x_star) using 1; ring
      have h3 : HasDerivAt (fun t : ‚Ñù => (Œº / 2) * t * (1 - t) * ‚Äñe‚Äñ^2) ((Œº / 2) * ‚Äñe‚Äñ^2) 0 := by
        have hpoly : HasDerivAt (fun t : ‚Ñù => t * (1 - t)) 1 0 := by
          have ha : HasDerivAt (fun t : ‚Ñù => t) 1 0 := hasDerivAt_id (0 : ‚Ñù)
          have hb : HasDerivAt (fun t : ‚Ñù => 1 - t) (-1) 0 :=
            (hasDerivAt_const (0 : ‚Ñù) (1 : ‚Ñù)).sub (hasDerivAt_id (0 : ‚Ñù)) |>.congr_deriv (by ring)
          exact (ha.mul hb).congr_deriv (by simp [id])
        convert hpoly.const_mul ((Œº / 2) * ‚Äñe‚Äñ^2) using 1
        ¬∑ ext t; ring
        ¬∑ ring
      convert (h1.add h2).sub h3 using 1 <;> ring
    -- Compute p'(0) = ‚ü®‚àáf(x*), e‚ü© = 0 (since ‚àáf(x*) = 0)
    have p_deriv : HasDerivAt p 0 0 := by
      have hŒ≥ : HasDerivAt (fun t : ‚Ñù => x_star + t ‚Ä¢ e) e 0 := by
        have h1 : HasDerivAt (fun _ : ‚Ñù => x_star) 0 0 := hasDerivAt_const 0 x_star
        have h2 : HasDerivAt (fun t : ‚Ñù => t ‚Ä¢ e) ((1 : ‚Ñù) ‚Ä¢ e) 0 :=
          (hasDerivAt_id 0).smul_const e
        have hsum := h1.add h2
        simp only [zero_add, one_smul] at hsum
        exact hsum
      have hf_grad : HasGradientAt f (gradient f x_star) x_star := (hDiff x_star).hasGradientAt
      have hf_fderiv : HasFDerivAt f (innerSL (ùïú := ‚Ñù) (gradient f x_star)) x_star :=
        hf_grad.hasFDerivAt
      have hf_fderiv' : HasFDerivAt f (innerSL (ùïú := ‚Ñù) (gradient f x_star)) (x_star + (0 : ‚Ñù) ‚Ä¢ e) := by
        simp only [zero_smul, add_zero]; exact hf_fderiv
      have hcomp := hf_fderiv'.comp_hasDerivAt (0 : ‚Ñù) hŒ≥
      simp only [Function.comp_apply, innerSL_apply_apply, zero_smul, add_zero, hMin, inner_zero_left] at hcomp
      exact hcomp
    -- Key: if p(0) = q(0), p(t) ‚â§ q(t) for t > 0, and both differentiable at 0, then p'(0) ‚â§ q'(0)
    have hderiv_ineq : 0 ‚â§ f x - f x_star - (Œº / 2) * ‚Äñe‚Äñ^2 := by
      by_contra hcontra
      push_neg at hcontra
      -- Let Œ¥ = p'(0) - q'(0) = 0 - q'(0) = -(f x - f x_star - (Œº/2)‚Äñe‚Äñ¬≤) > 0
      let Œ¥ := -(f x - f x_star - (Œº / 2) * ‚Äñe‚Äñ^2)
      have hŒ¥_pos : Œ¥ > 0 := by simp only [Œ¥]; linarith
      have h_pq_deriv : HasDerivAt (fun t => p t - q t) Œ¥ 0 := by
        have := HasDerivAt.sub p_deriv q_deriv
        convert this using 2
        simp only [Œ¥]; ring
      have h_pq_0 : (fun t => p t - q t) 0 = 0 := by simp only [hp0, hq0, sub_self]
      -- Use isLittleO characterization instead of tendsto_slope (which gives nhdsWithin)
      rw [hasDerivAt_iff_isLittleO] at h_pq_deriv
      have hŒµ_half : 0 < Œ¥ / 2 := by linarith
      have h_bound_evt := h_pq_deriv.def hŒµ_half
      simp only [h_pq_0, sub_zero, smul_eq_mul] at h_bound_evt
      rw [Filter.eventually_iff_exists_mem] at h_bound_evt
      obtain ‚ü®s, hs_mem, hs_bound‚ü© := h_bound_evt
      rw [Metric.mem_nhds_iff] at hs_mem
      obtain ‚ü®Œµ, hŒµ_pos, hŒµ_sub‚ü© := hs_mem
      let t := min (Œµ / 2) (1 / 2)
      have ht_pos : 0 < t := by positivity
      have ht_lt_Œµ : t < Œµ := by simp only [t]; linarith [min_le_left (Œµ / 2) (1 / 2)]
      have ht_le_1 : t ‚â§ 1 := by simp only [t]; linarith [min_le_right (Œµ / 2) (1 / 2)]
      have ht_in_ball : t ‚àà Metric.ball 0 Œµ := by
        simp only [Metric.mem_ball, dist_zero_right, Real.norm_eq_abs, abs_of_pos ht_pos]
        exact ht_lt_Œµ
      have ht_in_s : t ‚àà s := hŒµ_sub ht_in_ball
      have h_bound := hs_bound t ht_in_s
      simp only [Real.norm_eq_abs, abs_of_pos ht_pos] at h_bound
      -- h_bound : ‚Äñp t - q t - t * Œ¥‚Äñ ‚â§ (Œ¥/2) * t
      -- This means: -(Œ¥/2)*t ‚â§ (p t - q t) - t*Œ¥ ‚â§ (Œ¥/2)*t
      -- So: t*Œ¥ - (Œ¥/2)*t ‚â§ p t - q t, i.e., (Œ¥/2)*t ‚â§ p t - q t
      have h_lower : p t - q t ‚â• t * Œ¥ - (Œ¥ / 2) * t := by
        have h1 : -((Œ¥ / 2) * t) ‚â§ (p t - q t) - t * Œ¥ := by
          have := neg_abs_le (p t - q t - t * Œ¥)
          linarith
        linarith
      have h_diff_pos : p t - q t > 0 := by
        have : t * Œ¥ - (Œ¥ / 2) * t = (Œ¥ / 2) * t := by ring
        rw [this] at h_lower
        have : (Œ¥ / 2) * t > 0 := mul_pos (by linarith) ht_pos
        linarith
      have h_le := hpq_ineq t (le_of_lt ht_pos) ht_le_1
      linarith
    -- From 0 ‚â§ f(x) - f(x*) - (Œº/2)‚Äñe‚Äñ¬≤, we get f(x*) ‚â§ f(x) - (Œº/2)‚Äñe‚Äñ¬≤ ‚â§ f(x)
    have h_e_sq_nonneg : 0 ‚â§ (Œº / 2) * ‚Äñe‚Äñ^2 := by positivity
    linarith
  have h_d_norm : ‚Äñd‚Äñ = ‚Äñx - x_star‚Äñ := by simp only [d, norm_sub_rev]
  rw [h_d_norm] at h_deriv_ineq
  linarith

/-- Gradient monotonicity for strongly convex functions (full Œº, not Œº/2).

    For Œº-strongly convex f with ‚àáf(x*) = 0:
    ‚ü®‚àáf(x), x - x*‚ü© ‚â• Œº‚Äñx - x*‚Äñ¬≤

    This is twice as strong as `strong_convex_gradient_lower_bound` and comes from
    adding the first-order conditions at both x and x*.

    Proof:
    1. First-order at x: ‚ü®‚àáf(x), x* - x‚ü© ‚â§ f(x*) - f(x) - (Œº/2)‚Äñx - x*‚Äñ¬≤
       ‚Üí ‚ü®‚àáf(x), x - x*‚ü© ‚â• f(x) - f(x*) + (Œº/2)‚Äñx - x*‚Äñ¬≤
    2. First-order at x* with ‚àáf(x*) = 0: 0 ‚â§ f(x) - f(x*) - (Œº/2)‚Äñx - x*‚Äñ¬≤
       ‚Üí f(x) - f(x*) ‚â• (Œº/2)‚Äñx - x*‚Äñ¬≤
    3. Combining: ‚ü®‚àáf(x), x - x*‚ü© ‚â• (Œº/2)‚Äñx - x*‚Äñ¬≤ + (Œº/2)‚Äñx - x*‚Äñ¬≤ = Œº‚Äñx - x*‚Äñ¬≤
-/
theorem strong_convex_gradient_monotonicity (f : E ‚Üí ‚Ñù) (Œº : ‚Ñù) (hŒº : 0 < Œº)
    (hStrong : IsStronglyConvex f Œº) (hDiff : Differentiable ‚Ñù f)
    (x x_star : E) (hMin : gradient f x_star = 0) :
    @inner ‚Ñù E _ (gradient f x) (x - x_star) ‚â• Œº * ‚Äñx - x_star‚Äñ^2 := by
  -- The proof combines two first-order conditions from strong convexity.
  -- Define d = x* - x and e = x - x*
  let d := x_star - x
  let e := x - x_star

  -- Part 1: First-order condition at x gives:
  -- ‚ü®‚àáf(x), x - x*‚ü© ‚â• f(x) - f(x*) + (Œº/2)‚Äñx - x*‚Äñ¬≤
  -- (This is derived in strong_convex_gradient_lower_bound as h_deriv_ineq)

  -- We'll derive both bounds together using the same technique.

  -- Step A: Derive ‚ü®‚àáf(x), d‚ü© ‚â§ f(x*) - f(x) - (Œº/2)‚Äñd‚Äñ¬≤ via derivative limit
  let g := fun t : ‚Ñù => f (x + t ‚Ä¢ d)
  let h := fun t : ‚Ñù => (1 - t) * f x + t * f x_star - (Œº / 2) * t * (1 - t) * ‚Äñd‚Äñ^2
  have h_ineq : ‚àÄ t, 0 ‚â§ t ‚Üí t ‚â§ 1 ‚Üí g t ‚â§ h t := by
    intro t ht0 ht1
    have hconv := hStrong x_star x t ht0 ht1
    have heq : t ‚Ä¢ x_star + (1 - t) ‚Ä¢ x = x + t ‚Ä¢ d := by
      simp only [d]; rw [smul_sub]; ring_nf; module
    simp only [g, h, heq] at hconv ‚ä¢
    have hnorm : ‚Äñx_star - x‚Äñ = ‚Äñd‚Äñ := by simp only [d]
    rw [hnorm] at hconv
    linarith
  have hg0 : g 0 = f x := by simp only [g, zero_smul, add_zero]
  have hh0 : h 0 = f x := by simp only [h]; ring
  have h_deriv : HasDerivAt h (f x_star - f x - (Œº / 2) * ‚Äñd‚Äñ^2) 0 := by
    have h1 : HasDerivAt (fun t : ‚Ñù => (1 - t) * f x) (-f x) 0 := by
      have hid : HasDerivAt (fun t : ‚Ñù => 1 - t) (-1) 0 :=
        (hasDerivAt_const (0 : ‚Ñù) (1 : ‚Ñù)).sub (hasDerivAt_id (0 : ‚Ñù)) |>.congr_deriv (by ring)
      convert hid.mul_const (f x) using 1; ring
    have h2 : HasDerivAt (fun t : ‚Ñù => t * f x_star) (f x_star) 0 := by
      convert (hasDerivAt_id (0 : ‚Ñù)).mul_const (f x_star) using 1; ring
    have h3 : HasDerivAt (fun t : ‚Ñù => (Œº / 2) * t * (1 - t) * ‚Äñd‚Äñ^2) ((Œº / 2) * ‚Äñd‚Äñ^2) 0 := by
      have hpoly : HasDerivAt (fun t : ‚Ñù => t * (1 - t)) 1 0 := by
        have h1' := hasDerivAt_id (0 : ‚Ñù)
        have h2' : HasDerivAt (fun t : ‚Ñù => 1 - t) (-1) 0 :=
          (hasDerivAt_const (0 : ‚Ñù) (1 : ‚Ñù)).sub (hasDerivAt_id (0 : ‚Ñù)) |>.congr_deriv (by ring)
        have hprod := h1'.mul h2'
        convert hprod using 2 <;> simp
      convert hpoly.const_mul ((Œº / 2) * ‚Äñd‚Äñ^2) using 1
      ¬∑ ext t; ring
      ¬∑ ring
    convert (h1.add h2).sub h3 using 1; ring
  have g_deriv : HasDerivAt g (@inner ‚Ñù E _ (gradient f x) d) 0 := by
    have hŒ≥ : HasDerivAt (fun t : ‚Ñù => x + t ‚Ä¢ d) d 0 := by
      have h1 : HasDerivAt (fun _ : ‚Ñù => x) 0 0 := hasDerivAt_const 0 x
      have h2 : HasDerivAt (fun t : ‚Ñù => t ‚Ä¢ d) ((1 : ‚Ñù) ‚Ä¢ d) 0 :=
        (hasDerivAt_id 0).smul_const d
      have hsum := h1.add h2
      simp only [zero_add, one_smul] at hsum
      exact hsum
    have hf_grad : HasGradientAt f (gradient f x) x := (hDiff x).hasGradientAt
    have hf_fderiv : HasFDerivAt f (innerSL (ùïú := ‚Ñù) (gradient f x)) x := hf_grad.hasFDerivAt
    have hf_fderiv' : HasFDerivAt f (innerSL (ùïú := ‚Ñù) (gradient f x)) (x + (0 : ‚Ñù) ‚Ä¢ d) := by
      simp only [zero_smul, add_zero]; exact hf_fderiv
    have hcomp := hf_fderiv'.comp_hasDerivAt (0 : ‚Ñù) hŒ≥
    simp only [Function.comp_apply, innerSL_apply_apply, zero_smul, add_zero] at hcomp
    exact hcomp
  -- Derivative limit argument: g(0) = h(0), g ‚â§ h on (0,1], so g'(0) ‚â§ h'(0)
  have h_deriv_ineq : @inner ‚Ñù E _ (gradient f x) d ‚â§ f x_star - f x - (Œº / 2) * ‚Äñd‚Äñ^2 := by
    by_contra hcontra
    push_neg at hcontra
    let Œ¥ := @inner ‚Ñù E _ (gradient f x) d - (f x_star - f x - (Œº / 2) * ‚Äñd‚Äñ^2)
    have hŒ¥_pos : Œ¥ > 0 := by simp only [Œ¥]; linarith
    have h_gh_deriv : HasDerivAt (fun t => g t - h t) Œ¥ 0 := HasDerivAt.sub g_deriv h_deriv
    have h_gh_0 : (fun t => g t - h t) 0 = 0 := by simp only [hg0, hh0, sub_self]
    rw [hasDerivAt_iff_isLittleO] at h_gh_deriv
    have hŒµ_half : 0 < Œ¥ / 2 := by linarith
    have h_bound_evt := h_gh_deriv.def hŒµ_half
    simp only [h_gh_0, sub_zero, smul_eq_mul] at h_bound_evt
    rw [Filter.eventually_iff_exists_mem] at h_bound_evt
    obtain ‚ü®s, hs_mem, hs_bound‚ü© := h_bound_evt
    rw [Metric.mem_nhds_iff] at hs_mem
    obtain ‚ü®Œµ, hŒµ_pos, hŒµ_sub‚ü© := hs_mem
    let t := min (Œµ / 2) (1 / 2)
    have ht_pos : 0 < t := by positivity
    have ht_lt_Œµ : t < Œµ := by simp only [t]; linarith [min_le_left (Œµ / 2) (1 / 2)]
    have ht_le_1 : t ‚â§ 1 := by simp only [t]; linarith [min_le_right (Œµ / 2) (1 / 2)]
    have ht_in_ball : t ‚àà Metric.ball 0 Œµ := by
      simp only [Metric.mem_ball, dist_zero_right, Real.norm_eq_abs, abs_of_pos ht_pos]
      exact ht_lt_Œµ
    have ht_in_s : t ‚àà s := hŒµ_sub ht_in_ball
    have h_bound := hs_bound t ht_in_s
    simp only [Real.norm_eq_abs, abs_of_pos ht_pos] at h_bound
    have h_lower : g t - h t ‚â• t * Œ¥ - (Œ¥ / 2) * t := by
      have h1 : -((Œ¥ / 2) * t) ‚â§ (g t - h t) - t * Œ¥ := by
        have := neg_abs_le (g t - h t - t * Œ¥)
        linarith
      linarith
    have h_diff_pos : g t - h t > 0 := by
      have : t * Œ¥ - (Œ¥ / 2) * t = (Œ¥ / 2) * t := by ring
      rw [this] at h_lower
      have : (Œ¥ / 2) * t > 0 := mul_pos (by linarith) ht_pos
      linarith
    have h_le := h_ineq t (le_of_lt ht_pos) ht_le_1
    linarith

  -- Step B: Derive f(x) - f(x*) ‚â• (Œº/2)‚Äñe‚Äñ¬≤ via derivative limit at x*
  let p := fun t : ‚Ñù => f (x_star + t ‚Ä¢ e)
  let q := fun t : ‚Ñù => t * f x + (1 - t) * f x_star - (Œº / 2) * t * (1 - t) * ‚Äñe‚Äñ^2
  have hpq_ineq : ‚àÄ t, 0 ‚â§ t ‚Üí t ‚â§ 1 ‚Üí p t ‚â§ q t := by
    intro t ht0 ht1
    have hconv := hStrong x x_star t ht0 ht1
    have heq : t ‚Ä¢ x + (1 - t) ‚Ä¢ x_star = x_star + t ‚Ä¢ e := by
      simp only [e]; rw [smul_sub]; ring_nf; module
    simp only [p, q, heq] at hconv ‚ä¢
    have hnorm : ‚Äñx - x_star‚Äñ = ‚Äñe‚Äñ := by simp only [e]
    rw [hnorm] at hconv
    linarith
  have hp0 : p 0 = f x_star := by simp only [p, zero_smul, add_zero]
  have hq0 : q 0 = f x_star := by simp only [q]; ring
  have q_deriv : HasDerivAt q (f x - f x_star - (Œº / 2) * ‚Äñe‚Äñ^2) 0 := by
    have h1 : HasDerivAt (fun t : ‚Ñù => t * f x) (f x) 0 := by
      convert (hasDerivAt_id (0 : ‚Ñù)).mul_const (f x) using 1; ring
    have h2 : HasDerivAt (fun t : ‚Ñù => (1 - t) * f x_star) (-f x_star) 0 := by
      have hid : HasDerivAt (fun t : ‚Ñù => 1 - t) (-1) 0 :=
        (hasDerivAt_const (0 : ‚Ñù) (1 : ‚Ñù)).sub (hasDerivAt_id (0 : ‚Ñù)) |>.congr_deriv (by ring)
      convert hid.mul_const (f x_star) using 1; ring
    have h3 : HasDerivAt (fun t : ‚Ñù => (Œº / 2) * t * (1 - t) * ‚Äñe‚Äñ^2) ((Œº / 2) * ‚Äñe‚Äñ^2) 0 := by
      have hpoly : HasDerivAt (fun t : ‚Ñù => t * (1 - t)) 1 0 := by
        have ha : HasDerivAt (fun t : ‚Ñù => t) 1 0 := hasDerivAt_id (0 : ‚Ñù)
        have hb : HasDerivAt (fun t : ‚Ñù => 1 - t) (-1) 0 :=
          (hasDerivAt_const (0 : ‚Ñù) (1 : ‚Ñù)).sub (hasDerivAt_id (0 : ‚Ñù)) |>.congr_deriv (by ring)
        exact (ha.mul hb).congr_deriv (by simp [id])
      convert hpoly.const_mul ((Œº / 2) * ‚Äñe‚Äñ^2) using 1
      ¬∑ ext t; ring
      ¬∑ ring
    convert (h1.add h2).sub h3 using 1 <;> ring
  have p_deriv : HasDerivAt p 0 0 := by
    have hŒ≥ : HasDerivAt (fun t : ‚Ñù => x_star + t ‚Ä¢ e) e 0 := by
      have h1 : HasDerivAt (fun _ : ‚Ñù => x_star) 0 0 := hasDerivAt_const 0 x_star
      have h2 : HasDerivAt (fun t : ‚Ñù => t ‚Ä¢ e) ((1 : ‚Ñù) ‚Ä¢ e) 0 :=
        (hasDerivAt_id 0).smul_const e
      have hsum := h1.add h2
      simp only [zero_add, one_smul] at hsum
      exact hsum
    have hf_grad : HasGradientAt f (gradient f x_star) x_star := (hDiff x_star).hasGradientAt
    have hf_fderiv : HasFDerivAt f (innerSL (ùïú := ‚Ñù) (gradient f x_star)) x_star :=
      hf_grad.hasFDerivAt
    have hf_fderiv' : HasFDerivAt f (innerSL (ùïú := ‚Ñù) (gradient f x_star)) (x_star + (0 : ‚Ñù) ‚Ä¢ e) := by
      simp only [zero_smul, add_zero]; exact hf_fderiv
    have hcomp := hf_fderiv'.comp_hasDerivAt (0 : ‚Ñù) hŒ≥
    simp only [Function.comp_apply, innerSL_apply_apply, zero_smul, add_zero, hMin, inner_zero_left] at hcomp
    exact hcomp
  -- p'(0) = 0 ‚â§ q'(0) = f(x) - f(x*) - (Œº/2)‚Äñe‚Äñ¬≤ gives f(x) - f(x*) ‚â• (Œº/2)‚Äñe‚Äñ¬≤
  have h_func_bound : 0 ‚â§ f x - f x_star - (Œº / 2) * ‚Äñe‚Äñ^2 := by
    by_contra hcontra
    push_neg at hcontra
    let Œ¥ := -(f x - f x_star - (Œº / 2) * ‚Äñe‚Äñ^2)
    have hŒ¥_pos : Œ¥ > 0 := by simp only [Œ¥]; linarith
    have h_pq_deriv : HasDerivAt (fun t => p t - q t) Œ¥ 0 := by
      have := HasDerivAt.sub p_deriv q_deriv
      convert this using 2
      simp only [Œ¥]; ring
    have h_pq_0 : (fun t => p t - q t) 0 = 0 := by simp only [hp0, hq0, sub_self]
    rw [hasDerivAt_iff_isLittleO] at h_pq_deriv
    have hŒµ_half : 0 < Œ¥ / 2 := by linarith
    have h_bound_evt := h_pq_deriv.def hŒµ_half
    simp only [h_pq_0, sub_zero, smul_eq_mul] at h_bound_evt
    rw [Filter.eventually_iff_exists_mem] at h_bound_evt
    obtain ‚ü®s, hs_mem, hs_bound‚ü© := h_bound_evt
    rw [Metric.mem_nhds_iff] at hs_mem
    obtain ‚ü®Œµ, hŒµ_pos, hŒµ_sub‚ü© := hs_mem
    let t := min (Œµ / 2) (1 / 2)
    have ht_pos : 0 < t := by positivity
    have ht_lt_Œµ : t < Œµ := by simp only [t]; linarith [min_le_left (Œµ / 2) (1 / 2)]
    have ht_le_1 : t ‚â§ 1 := by simp only [t]; linarith [min_le_right (Œµ / 2) (1 / 2)]
    have ht_in_ball : t ‚àà Metric.ball 0 Œµ := by
      simp only [Metric.mem_ball, dist_zero_right, Real.norm_eq_abs, abs_of_pos ht_pos]
      exact ht_lt_Œµ
    have ht_in_s : t ‚àà s := hŒµ_sub ht_in_ball
    have h_bound := hs_bound t ht_in_s
    simp only [Real.norm_eq_abs, abs_of_pos ht_pos] at h_bound
    have h_lower : p t - q t ‚â• t * Œ¥ - (Œ¥ / 2) * t := by
      have h1 : -((Œ¥ / 2) * t) ‚â§ (p t - q t) - t * Œ¥ := by
        have := neg_abs_le (p t - q t - t * Œ¥)
        linarith
      linarith
    have h_diff_pos : p t - q t > 0 := by
      have : t * Œ¥ - (Œ¥ / 2) * t = (Œ¥ / 2) * t := by ring
      rw [this] at h_lower
      have : (Œ¥ / 2) * t > 0 := mul_pos (by linarith) ht_pos
      linarith
    have h_le := hpq_ineq t (le_of_lt ht_pos) ht_le_1
    linarith

  -- Step C: Combine the two bounds
  -- From h_deriv_ineq: ‚ü®‚àáf(x), d‚ü© ‚â§ f(x*) - f(x) - (Œº/2)‚Äñd‚Äñ¬≤
  -- So: ‚ü®‚àáf(x), x - x*‚ü© = -‚ü®‚àáf(x), d‚ü© ‚â• f(x) - f(x*) + (Œº/2)‚Äñd‚Äñ¬≤
  have h_inner_neg : @inner ‚Ñù E _ (gradient f x) (x - x_star) =
      -@inner ‚Ñù E _ (gradient f x) d := by
    simp only [d, ‚Üê inner_neg_right, neg_sub]
  have h_d_norm : ‚Äñd‚Äñ = ‚Äñx - x_star‚Äñ := by simp only [d, norm_sub_rev]
  have h_e_norm : ‚Äñe‚Äñ = ‚Äñx - x_star‚Äñ := by simp only [e]

  -- From h_deriv_ineq: -‚ü®‚àáf(x), d‚ü© ‚â• f(x) - f(x*) + (Œº/2)‚Äñd‚Äñ¬≤
  have h_inner_lb : @inner ‚Ñù E _ (gradient f x) (x - x_star) ‚â•
      f x - f x_star + (Œº / 2) * ‚Äñx - x_star‚Äñ^2 := by
    rw [h_inner_neg]
    simp only [h_d_norm] at h_deriv_ineq
    linarith

  -- From h_func_bound: f(x) - f(x*) ‚â• (Œº/2)‚Äñe‚Äñ¬≤ = (Œº/2)‚Äñx - x*‚Äñ¬≤
  have h_func_lb : f x - f x_star ‚â• (Œº / 2) * ‚Äñx - x_star‚Äñ^2 := by
    rw [h_e_norm] at h_func_bound
    linarith

  -- Combine: ‚ü®‚àáf(x), x - x*‚ü© ‚â• (Œº/2)‚Äñx - x*‚Äñ¬≤ + (Œº/2)‚Äñx - x*‚Äñ¬≤ = Œº‚Äñx - x*‚Äñ¬≤
  calc @inner ‚Ñù E _ (gradient f x) (x - x_star)
      ‚â• f x - f x_star + (Œº / 2) * ‚Äñx - x_star‚Äñ^2 := h_inner_lb
    _ ‚â• (Œº / 2) * ‚Äñx - x_star‚Äñ^2 + (Œº / 2) * ‚Äñx - x_star‚Äñ^2 := by linarith [h_func_lb]
    _ = Œº * ‚Äñx - x_star‚Äñ^2 := by ring

-- strong_smooth_interpolation is defined below after lsmooth_cocoercivity

/-- Fundamental inequality for L-smooth functions:
    f(y) ‚â§ f(x) + ‚ü®‚àáf(x), y - x‚ü© + (L/2)‚Äñy - x‚Äñ¬≤

    ## Mathematical Proof

    This follows from integrating the gradient along the line from x to y
    and using the Lipschitz condition on the gradient.

    **Step 1: Define the path**

    Let Œ≥(t) = x + t(y - x) for t ‚àà [0, 1].
    Then Œ≥(0) = x and Œ≥(1) = y.

    **Step 2: Apply the Fundamental Theorem of Calculus**

    Define g(t) = f(Œ≥(t)). By chain rule: g'(t) = ‚ü®‚àáf(Œ≥(t)), y - x‚ü©.

    Therefore: f(y) - f(x) = g(1) - g(0) = ‚à´‚ÇÄ¬π g'(t) dt = ‚à´‚ÇÄ¬π ‚ü®‚àáf(Œ≥(t)), y - x‚ü© dt.

    **Step 3: Decompose and bound**

    f(y) - f(x) - ‚ü®‚àáf(x), y - x‚ü©
      = ‚à´‚ÇÄ¬π ‚ü®‚àáf(Œ≥(t)), y - x‚ü© dt - ‚ü®‚àáf(x), y - x‚ü©
      = ‚à´‚ÇÄ¬π ‚ü®‚àáf(Œ≥(t)) - ‚àáf(x), y - x‚ü© dt

    By Cauchy-Schwarz:
      |‚ü®‚àáf(Œ≥(t)) - ‚àáf(x), y - x‚ü©| ‚â§ ‚Äñ‚àáf(Œ≥(t)) - ‚àáf(x)‚Äñ ¬∑ ‚Äñy - x‚Äñ

    By L-smoothness (gradient is L-Lipschitz):
      ‚Äñ‚àáf(Œ≥(t)) - ‚àáf(x)‚Äñ ‚â§ L ¬∑ ‚ÄñŒ≥(t) - x‚Äñ = L ¬∑ t ¬∑ ‚Äñy - x‚Äñ

    Therefore:
      ‚ü®‚àáf(Œ≥(t)) - ‚àáf(x), y - x‚ü© ‚â§ L ¬∑ t ¬∑ ‚Äñy - x‚Äñ¬≤

    **Step 4: Integrate**

    f(y) - f(x) - ‚ü®‚àáf(x), y - x‚ü© ‚â§ ‚à´‚ÇÄ¬π L ¬∑ t ¬∑ ‚Äñy - x‚Äñ¬≤ dt
                                   = L ¬∑ ‚Äñy - x‚Äñ¬≤ ¬∑ ‚à´‚ÇÄ¬π t dt
                                   = L ¬∑ ‚Äñy - x‚Äñ¬≤ ¬∑ (1/2)
                                   = (L/2) ¬∑ ‚Äñy - x‚Äñ¬≤

    **Lean Formalization Requirements**

    1. `MeasureTheory.integral_Icc_eq_integral_Ioc` - integration on [0,1]
    2. `HasDerivAt.integral_eq_sub` - FTC for path integrals
    3. `MeasureTheory.integral_mono` - for bounding integrals
    4. `integral_id` or similar for ‚à´‚ÇÄ¬π t dt = 1/2
-/
theorem lsmooth_fundamental_ineq (f : E ‚Üí ‚Ñù) (L : ‚Ñù) (hL : 0 ‚â§ L)
    (hSmooth : IsLSmooth f L) (x y : E) :
    f y ‚â§ f x + @inner ‚Ñù E _ (gradient f x) (y - x) + (L / 2) * ‚Äñy - x‚Äñ^2 := by
  obtain ‚ü®hDiff, hLip‚ü© := hSmooth
  -- Special case: if x = y, the inequality is trivially true
  by_cases hxy : x = y
  ¬∑ simp only [hxy, sub_self, inner_zero_right, norm_zero, sq, mul_zero, add_zero, le_refl]
  -- Special case: if L = 0, gradient is constant, so f is affine
  by_cases hL0 : L = 0
  ¬∑ -- When L = 0, ‚àáf is constant (0-Lipschitz means constant)
    -- So f(y) = f(x) + ‚ü®‚àáf(x), y - x‚ü© for all x, y
    simp only [hL0, zero_div, zero_mul, add_zero]
    -- For constant gradient, f is affine: f(y) - f(x) = ‚ü®‚àáf(x), y - x‚ü©
    -- From 0-Lipschitz: ‚Äñ‚àáf(x) - ‚àáf(y)‚Äñ ‚â§ 0 * ‚Äñx - y‚Äñ = 0. So ‚àáf(x) = ‚àáf(y) for all x, y.
    -- When gradient is constant, by the MVT: f(y) - f(x) = ‚ü®‚àáf(Œæ), y - x‚ü© for some Œæ.
    -- Since ‚àáf is constant, ‚àáf(Œæ) = ‚àáf(x), so f(y) - f(x) = ‚ü®‚àáf(x), y - x‚ü©
    have h_grad_const : ‚àÄ z, gradient f z = gradient f x := by
      intro z
      have h0 : ‚Äñgradient f z - gradient f x‚Äñ ‚â§ 0 * ‚Äñz - x‚Äñ := by
        rw [‚Üê hL0]
        exact hLip z x
      simp only [zero_mul, norm_le_zero_iff] at h0
      exact sub_eq_zero.mp h0
    -- For the formal proof, we use that zero Frechet derivative implies constant.
    -- Define h(z) = f(z) - ‚ü®‚àáf(x), z‚ü©. Then fderiv h z = 0 (gradient is constant).
    -- Zero fderiv on convex set implies h is constant, so h(y) = h(x).
    let g := gradient f x
    let h := fun z => f z - @inner ‚Ñù E _ g z
    have hh_diff : Differentiable ‚Ñù h := by
      intro z
      apply DifferentiableAt.sub (hDiff z)
      exact (innerSL (ùïú := ‚Ñù) g).differentiableAt
    -- h has zero Frechet derivative everywhere
    have h_fderiv_zero : ‚àÄ z, fderiv ‚Ñù h z = 0 := by
      intro z
      have hf_diff : DifferentiableAt ‚Ñù f z := hDiff z
      have hg_diff : DifferentiableAt ‚Ñù (fun w => @inner ‚Ñù E _ g w) z :=
        (innerSL (ùïú := ‚Ñù) g).differentiableAt
      -- fderiv of f z = innerSL (gradient f z)
      have h_fderiv_f : fderiv ‚Ñù f z = innerSL (ùïú := ‚Ñù) (gradient f z) := by
        have hgrad := hf_diff.hasGradientAt
        exact hgrad.hasFDerivAt.fderiv
      -- fderiv of (inner g ¬∑) = innerSL g
      have h_fderiv_inner : fderiv ‚Ñù (fun w => @inner ‚Ñù E _ g w) z = innerSL (ùïú := ‚Ñù) g :=
        (innerSL (ùïú := ‚Ñù) g).fderiv
      -- fderiv of h = fderiv f - fderiv inner
      have h1 : fderiv ‚Ñù h z = fderiv ‚Ñù f z - fderiv ‚Ñù (fun w => @inner ‚Ñù E _ g w) z := by
        exact fderiv_sub hf_diff hg_diff
      rw [h1, h_fderiv_f, h_fderiv_inner, h_grad_const z]
      exact sub_self _
    -- h is constant: use that zero derivative on convex set implies constant
    have h_const : h y = h x := by
      have hconvex : Convex ‚Ñù (Set.univ : Set E) := convex_univ
      have hdiff_on : DifferentiableOn ‚Ñù h Set.univ := hh_diff.differentiableOn
      have hfderiv_on : ‚àÄ z ‚àà Set.univ, fderivWithin ‚Ñù h Set.univ z = 0 := by
        intro z _
        rw [fderivWithin_univ]
        exact h_fderiv_zero z
      exact Convex.is_const_of_fderivWithin_eq_zero hconvex hdiff_on hfderiv_on
        (Set.mem_univ x) (Set.mem_univ y)
    -- Expand h(y) = h(x): f(y) - ‚ü®g, y‚ü© = f(x) - ‚ü®g, x‚ü©, so f(y) = f(x) + ‚ü®g, y - x‚ü©
    simp only [h] at h_const
    have h_inner_sub : @inner ‚Ñù E _ g y - @inner ‚Ñù E _ g x = @inner ‚Ñù E _ g (y - x) := by
      rw [inner_sub_right]
    linarith [h_const, h_inner_sub]
  -- Main case: L > 0
  have hL_pos : 0 < L := lt_of_le_of_ne hL (Ne.symm hL0)
  /- The proof uses integration along the line segment from x to y.

     Define Œ≥(t) = x + t(y - x) for t ‚àà [0, 1].
     Define g(t) = f(Œ≥(t)).

     Then g'(t) = ‚ü®‚àáf(Œ≥(t)), y - x‚ü©.

     By the fundamental theorem of calculus:
     f(y) - f(x) = g(1) - g(0) = ‚à´‚ÇÄ¬π g'(t) dt = ‚à´‚ÇÄ¬π ‚ü®‚àáf(Œ≥(t)), y - x‚ü© dt

     Therefore:
     f(y) - f(x) - ‚ü®‚àáf(x), y - x‚ü© = ‚à´‚ÇÄ¬π ‚ü®‚àáf(Œ≥(t)) - ‚àáf(x), y - x‚ü© dt

     By Cauchy-Schwarz and L-Lipschitz gradient:
     ‚ü®‚àáf(Œ≥(t)) - ‚àáf(x), y - x‚ü© ‚â§ ‚Äñ‚àáf(Œ≥(t)) - ‚àáf(x)‚Äñ ¬∑ ‚Äñy - x‚Äñ
                                 ‚â§ L ¬∑ ‚ÄñŒ≥(t) - x‚Äñ ¬∑ ‚Äñy - x‚Äñ
                                 = L ¬∑ t ¬∑ ‚Äñy - x‚Äñ¬≤

     Integrating:
     f(y) - f(x) - ‚ü®‚àáf(x), y - x‚ü© ‚â§ ‚à´‚ÇÄ¬π L ¬∑ t ¬∑ ‚Äñy - x‚Äñ¬≤ dt
                                   = L ¬∑ ‚Äñy - x‚Äñ¬≤ ¬∑ [t¬≤/2]‚ÇÄ¬π
                                   = (L/2) ¬∑ ‚Äñy - x‚Äñ¬≤

     This requires Mathlib's MeasureTheory.integral machinery and
     careful handling of the FTC for paths in Hilbert spaces.

     **Mathlib theorems needed**:
     - `MeasureTheory.integral_Icc` for ‚à´‚ÇÄ¬π ... dt
     - `HasDerivAt.integral_eq_sub` for FTC
     - `real_inner_le_norm` for Cauchy-Schwarz
     - `intervalIntegral.integral_mono` for bounding integrals

     **Alternative approach via second derivative**:
     Define g(t) = f(x + t(y-x)). Then:
     - g'(t) = ‚ü®‚àáf(x + t(y-x)), y - x‚ü©
     - g''(t) = ‚ü®Hf(x + t(y-x))(y-x), y - x‚ü© where Hf is the Hessian
     - For L-smooth f, the Hessian satisfies ‚ÄñHf‚Äñ ‚â§ L, so g''(t) ‚â§ L‚Äñy-x‚Äñ¬≤

     Integrating g''(t) twice:
     - g'(t) ‚â§ g'(0) + L¬∑t¬∑‚Äñy-x‚Äñ¬≤
     - g(t) ‚â§ g(0) + g'(0)¬∑t + (L/2)¬∑t¬≤¬∑‚Äñy-x‚Äñ¬≤

     At t = 1:
     - f(y) ‚â§ f(x) + ‚ü®‚àáf(x), y-x‚ü© + (L/2)‚Äñy-x‚Äñ¬≤
  -/

  /- **Proof Strategy using Monotonicity (avoids MeasureTheory integration)**
     Define:
     - Œ≥(t) = x + t ‚Ä¢ (y - x) for t ‚àà [0, 1]
     - g(t) = f(Œ≥(t)) - t * ‚ü®‚àáf(x), y - x‚ü©
     - K = L * ‚Äñy - x‚Äñ¬≤
     - h(t) = g(t) - (K/2) * t¬≤
     Then:
     - g'(t) = ‚ü®‚àáf(Œ≥(t)) - ‚àáf(x), y - x‚ü© (after simplification)
     - g'(t) ‚â§ L * t * ‚Äñy - x‚Äñ¬≤ = K * t (by Lipschitz + Cauchy-Schwarz)
     - h'(t) = g'(t) - K * t ‚â§ 0
     - By antitoneOn_of_deriv_nonpos: h(1) ‚â§ h(0)
     - Expanding: g(1) - K/2 ‚â§ g(0)
     - So: f(y) - ‚ü®‚àáf(x), y-x‚ü© - (L/2)‚Äñy-x‚Äñ¬≤ ‚â§ f(x)
     - Rearranging: f(y) ‚â§ f(x) + ‚ü®‚àáf(x), y-x‚ü© + (L/2)‚Äñy-x‚Äñ¬≤
  -/
  -- Define the path Œ≥(t) = x + t ‚Ä¢ (y - x)
  let Œ≥ := fun t : ‚Ñù => x + t ‚Ä¢ (y - x)
  -- Define K = L * ‚Äñy - x‚Äñ¬≤
  let K := L * ‚Äñy - x‚Äñ^2
  -- Define inner_val = ‚ü®‚àáf(x), y - x‚ü©
  let inner_val := @inner ‚Ñù E _ (gradient f x) (y - x)
  -- Define g(t) = f(Œ≥(t)) - t * inner_val : measures deviation from linear model
  let g := fun t : ‚Ñù => f (Œ≥ t) - t * inner_val
  -- Define h(t) = g(t) - (K/2) * t¬≤ : we'll show h is antitone
  let h := fun t : ‚Ñù => g t - (K / 2) * t^2
  -- Key boundary values
  have hŒ≥0 : Œ≥ 0 = x := by simp only [Œ≥, zero_smul, add_zero]
  have hŒ≥1 : Œ≥ 1 = y := by simp only [Œ≥, one_smul, add_sub_cancel]
  have hg0 : g 0 = f x := by simp only [g, hŒ≥0, zero_mul, sub_zero]
  have hg1 : g 1 = f y - inner_val := by simp only [g, hŒ≥1, one_mul]
  have hh0 : h 0 = f x := by simp only [h, hg0, sq, mul_zero, sub_zero]
  have hh1 : h 1 = f y - inner_val - K / 2 := by
    simp only [h, hg1, one_pow, mul_one]
  -- Œ≥(t) - x = t ‚Ä¢ (y - x) for the Lipschitz bound
  have hŒ≥_diff : ‚àÄ t, Œ≥ t - x = t ‚Ä¢ (y - x) := by
    intro t; simp only [Œ≥, add_sub_cancel_left]
  -- ‚ÄñŒ≥(t) - x‚Äñ = |t| * ‚Äñy - x‚Äñ
  have hŒ≥_norm : ‚àÄ t, ‚ÄñŒ≥ t - x‚Äñ = |t| * ‚Äñy - x‚Äñ := by
    intro t; rw [hŒ≥_diff, norm_smul, Real.norm_eq_abs]
  -- For t ‚àà [0, 1], |t| = t
  have h_abs_t : ‚àÄ t : ‚Ñù, 0 ‚â§ t ‚Üí t ‚â§ 1 ‚Üí |t| = t := fun t ht _ => abs_of_nonneg ht
  -- The key bound: ‚ü®‚àáf(Œ≥(t)) - ‚àáf(x), y - x‚ü© ‚â§ L * t * ‚Äñy - x‚Äñ¬≤ for t ‚àà [0, 1]
  -- This uses: Cauchy-Schwarz, then L-Lipschitz of gradient, then ‚ÄñŒ≥(t) - x‚Äñ = t * ‚Äñy - x‚Äñ
  have h_grad_bound : ‚àÄ t, 0 ‚â§ t ‚Üí t ‚â§ 1 ‚Üí
      @inner ‚Ñù E _ (gradient f (Œ≥ t) - gradient f x) (y - x) ‚â§ L * t * ‚Äñy - x‚Äñ^2 := by
    intro t ht0 ht1
    have hCS : @inner ‚Ñù E _ (gradient f (Œ≥ t) - gradient f x) (y - x) ‚â§
        ‚Äñgradient f (Œ≥ t) - gradient f x‚Äñ * ‚Äñy - x‚Äñ := real_inner_le_norm _ _
    have hLip : ‚Äñgradient f (Œ≥ t) - gradient f x‚Äñ ‚â§ L * ‚ÄñŒ≥ t - x‚Äñ := hLip (Œ≥ t) x
    have hNorm : ‚ÄñŒ≥ t - x‚Äñ = t * ‚Äñy - x‚Äñ := by rw [hŒ≥_norm, h_abs_t t ht0 ht1]
    calc @inner ‚Ñù E _ (gradient f (Œ≥ t) - gradient f x) (y - x)
        ‚â§ ‚Äñgradient f (Œ≥ t) - gradient f x‚Äñ * ‚Äñy - x‚Äñ := hCS
      _ ‚â§ (L * ‚ÄñŒ≥ t - x‚Äñ) * ‚Äñy - x‚Äñ := by nlinarith [norm_nonneg (y - x)]
      _ = L * (t * ‚Äñy - x‚Äñ) * ‚Äñy - x‚Äñ := by rw [hNorm]
      _ = L * t * ‚Äñy - x‚Äñ^2 := by ring
  -- Step 1: h is continuous on [0, 1]
  -- Œ≥ is continuous
  have hŒ≥_cont : Continuous Œ≥ := by
    simp only [Œ≥]
    exact continuous_const.add (continuous_id.smul continuous_const)
  -- f ‚àò Œ≥ is continuous
  have hfŒ≥_cont : Continuous (f ‚àò Œ≥) := hDiff.continuous.comp hŒ≥_cont
  -- g is continuous
  have hg_cont : Continuous g := by
    simp only [g]
    exact hfŒ≥_cont.sub (continuous_id.mul continuous_const)
  -- h is continuous
  have hh_cont : Continuous h := by
    simp only [h]
    exact hg_cont.sub (continuous_const.mul (continuous_pow 2))
  have h_cont : ContinuousOn h (Set.Icc 0 1) := hh_cont.continuousOn
  -- Step 2: h is differentiable on interior (0, 1)
  -- The derivative of h at t is: ‚ü®‚àáf(Œ≥(t)), y-x‚ü© - inner_val - K*t
  --                            = ‚ü®‚àáf(Œ≥(t)) - ‚àáf(x), y-x‚ü© - K*t
  -- We use the chain rule: deriv (f ‚àò Œ≥) t = fderiv f (Œ≥ t) (deriv Œ≥ t)
  --                                        = ‚ü®‚àáf(Œ≥(t)), y - x‚ü©
  -- Since Œ≥(t) = x + t ‚Ä¢ (y - x), we have deriv Œ≥ t = y - x (constant)
  have h_deriv : ‚àÄ t ‚àà Set.Ioo (0 : ‚Ñù) 1,
      HasDerivAt h (@inner ‚Ñù E _ (gradient f (Œ≥ t) - gradient f x) (y - x) - K * t) t := by
    intro t _ht
    -- Œ≥ has derivative y - x
    have hŒ≥_deriv : HasDerivAt Œ≥ (y - x) t := by
      have h1 : HasDerivAt (fun s : ‚Ñù => x) 0 t := hasDerivAt_const t x
      have h2 : HasDerivAt (fun s : ‚Ñù => s ‚Ä¢ (y - x)) ((1 : ‚Ñù) ‚Ä¢ (y - x)) t := by
        exact (hasDerivAt_id t).smul_const (y - x)
      have h3 := h1.add h2
      simp only [zero_add, one_smul] at h3
      convert h3 using 1
    -- f ‚àò Œ≥ has derivative ‚ü®‚àáf(Œ≥(t)), y - x‚ü©
    have hfŒ≥_deriv : HasDerivAt (f ‚àò Œ≥) (@inner ‚Ñù E _ (gradient f (Œ≥ t)) (y - x)) t := by
      have hf_grad : HasGradientAt f (gradient f (Œ≥ t)) (Œ≥ t) := (hDiff (Œ≥ t)).hasGradientAt
      have hf_fderiv : HasFDerivAt f (innerSL (ùïú := ‚Ñù) (gradient f (Œ≥ t))) (Œ≥ t) :=
        hf_grad.hasFDerivAt
      have := hf_fderiv.comp_hasDerivAt t hŒ≥_deriv
      simp only [innerSL_apply_apply] at this
      exact this
    -- (t ‚Ü¶ t * inner_val) has derivative inner_val
    have h_lin_deriv : HasDerivAt (fun s => s * inner_val) inner_val t := by
      have := (hasDerivAt_id t).mul_const inner_val
      simp only [one_mul] at this
      exact this
    -- g = (f ‚àò Œ≥) - (t ‚Ü¶ t * inner_val) has derivative ‚ü®‚àáf(Œ≥(t)), y-x‚ü© - inner_val
    have hg_deriv : HasDerivAt g (@inner ‚Ñù E _ (gradient f (Œ≥ t)) (y - x) - inner_val) t := by
      exact hfŒ≥_deriv.sub h_lin_deriv
    -- Rewrite using inner_sub_left: ‚ü®a, v‚ü© - ‚ü®b, v‚ü© = ‚ü®a - b, v‚ü©
    have h_inner_eq : @inner ‚Ñù E _ (gradient f (Œ≥ t)) (y - x) - inner_val =
        @inner ‚Ñù E _ (gradient f (Œ≥ t) - gradient f x) (y - x) := by
      simp only [inner_val, inner_sub_left]
    rw [h_inner_eq] at hg_deriv
    -- (t ‚Ü¶ (K/2) * t¬≤) has derivative K * t
    have h_quad_deriv : HasDerivAt (fun s => (K / 2) * s^2) (K * t) t := by
      have h1 := hasDerivAt_pow 2 t
      have h2 := h1.const_mul (K / 2)
      simp only [Nat.cast_ofNat] at h2
      convert h2 using 1
      ring
    -- h = g - (t ‚Ü¶ (K/2) * t¬≤)
    exact hg_deriv.sub h_quad_deriv
  -- Step 3: deriv h t ‚â§ 0 on (0, 1)
  have h_deriv_nonpos : ‚àÄ t ‚àà Set.Ioo (0 : ‚Ñù) 1, deriv h t ‚â§ 0 := by
    intro t ht
    have hd := h_deriv t ht
    rw [hd.deriv]
    have hbound := h_grad_bound t (le_of_lt ht.1) (le_of_lt ht.2)
    linarith
  -- Step 4: Apply antitone result
  -- interior of Icc 0 1 = Ioo 0 1
  have h_interior : interior (Set.Icc (0 : ‚Ñù) 1) = Set.Ioo 0 1 := interior_Icc
  have h_diff_on : DifferentiableOn ‚Ñù h (interior (Set.Icc (0 : ‚Ñù) 1)) := by
    rw [h_interior]
    intro t ht
    exact (h_deriv t ht).differentiableAt.differentiableWithinAt
  have h_deriv_le : ‚àÄ t ‚àà interior (Set.Icc (0 : ‚Ñù) 1), deriv h t ‚â§ 0 := by
    rw [h_interior]
    exact h_deriv_nonpos
  have h_mono := Convex.image_sub_le_mul_sub_of_deriv_le (convex_Icc (0 : ‚Ñù) 1) h_cont h_diff_on
    h_deriv_le 0 (Set.left_mem_Icc.mpr zero_le_one) 1 (Set.right_mem_Icc.mpr zero_le_one)
    zero_le_one
  -- h(1) - h(0) ‚â§ 0 * (1 - 0) = 0
  simp only [zero_mul, sub_zero] at h_mono
  -- h(1) ‚â§ h(0) means f(y) - inner_val - K/2 ‚â§ f(x)
  rw [hh1, hh0] at h_mono
  -- Conclude: f(y) ‚â§ f(x) + inner_val + K/2
  simp only [inner_val, K] at h_mono
  linarith

/-- First-order optimality for convex functions: if ‚àáf(x*) = 0 and f is convex and differentiable,
    then x* is a global minimizer.

    This is proved using a derivative limit argument: define p(t) = f(x* + t(y - x*)).
    Convexity gives p(t) ‚â§ (1-t)p(0) + tp(1). At t = 0, p(0) = p(0), so equality holds.
    p'(0) = ‚ü®‚àáf(x*), y - x*‚ü© = 0. For the convex bound p(t) ‚â§ (1-t)p(0) + tp(1) with
    derivative 0 at t = 0, we must have p(0) ‚â§ p(1), i.e., f(x*) ‚â§ f(y). -/
lemma convex_first_order_optimality (f : E ‚Üí ‚Ñù) (hConvex : ConvexOn ‚Ñù Set.univ f)
    (hDiff : Differentiable ‚Ñù f) (x_star : E) (hMin : gradient f x_star = 0) :
    ‚àÄ y, f x_star ‚â§ f y := by
  -- First-order optimality: for convex differentiable f, ‚àáf(x*) = 0 implies x* is a global minimizer.
  -- The proof uses convexity along paths and derivative comparison at t = 0.
  intro y
  -- Define p(t) = f(x* + t(y - x*)) and q(t) = (1-t)f(x*) + tf(y)
  let e := y - x_star
  let p := fun t : ‚Ñù => f (x_star + t ‚Ä¢ e)
  let q := fun t : ‚Ñù => (1 - t) * f x_star + t * f y
  -- By convexity: p(t) ‚â§ q(t) for t ‚àà [0, 1]
  have hpq_ineq : ‚àÄ t, 0 ‚â§ t ‚Üí t ‚â§ 1 ‚Üí p t ‚â§ q t := by
    intro t ht0 ht1
    have h1mt : 0 ‚â§ 1 - t := by linarith
    have hsum : (1 - t) + t = 1 := by ring
    have hconv := hConvex.2 (Set.mem_univ x_star) (Set.mem_univ y) h1mt ht0 hsum
    have heq : (1 - t) ‚Ä¢ x_star + t ‚Ä¢ y = x_star + t ‚Ä¢ e := by
      simp only [e, smul_sub]
      ring_nf
      module
    simp only [p, q, heq, smul_eq_mul] at hconv ‚ä¢
    linarith
  -- At t = 0: p(0) = q(0) = f(x*)
  have hp0 : p 0 = f x_star := by simp only [p, zero_smul, add_zero]
  have hq0 : q 0 = f x_star := by simp only [q]; ring
  -- p'(0) = ‚ü®‚àáf(x*), e‚ü© = 0
  have p_deriv : HasDerivAt p 0 0 := by
    have hŒ≥ : HasDerivAt (fun t : ‚Ñù => x_star + t ‚Ä¢ e) e 0 := by
      have h1 : HasDerivAt (fun _ : ‚Ñù => x_star) 0 0 := hasDerivAt_const 0 x_star
      have h2 : HasDerivAt (fun t : ‚Ñù => t ‚Ä¢ e) ((1 : ‚Ñù) ‚Ä¢ e) 0 := (hasDerivAt_id 0).smul_const e
      have hsum := h1.add h2
      simp only [zero_add, one_smul] at hsum
      exact hsum
    have hf_grad : HasGradientAt f (gradient f x_star) x_star := (hDiff x_star).hasGradientAt
    have hf_fderiv : HasFDerivAt f (innerSL (ùïú := ‚Ñù) (gradient f x_star)) x_star := hf_grad.hasFDerivAt
    have hf_fderiv' : HasFDerivAt f (innerSL (ùïú := ‚Ñù) (gradient f x_star)) (x_star + (0 : ‚Ñù) ‚Ä¢ e) := by
      simp only [zero_smul, add_zero]; exact hf_fderiv
    have hcomp := hf_fderiv'.comp_hasDerivAt (0 : ‚Ñù) hŒ≥
    simp only [Function.comp_apply, innerSL_apply_apply, zero_smul, add_zero, hMin, inner_zero_left] at hcomp
    exact hcomp
  -- q'(0) = f(y) - f(x*)
  have q_deriv : HasDerivAt q (f y - f x_star) 0 := by
    have h1 : HasDerivAt (fun t : ‚Ñù => (1 - t) * f x_star) (-f x_star) 0 := by
      have hid : HasDerivAt (fun t : ‚Ñù => 1 - t) (-1) 0 := by
        have := (hasDerivAt_const (0 : ‚Ñù) (1 : ‚Ñù)).sub (hasDerivAt_id (0 : ‚Ñù))
        convert this using 1
        ring
      have := hid.mul_const (f x_star)
      convert this using 1
      ring
    have h2 : HasDerivAt (fun t : ‚Ñù => t * f y) (f y) 0 := by
      have := (hasDerivAt_id (0 : ‚Ñù)).mul_const (f y)
      convert this using 1
      ring
    have h3 := h1.add h2
    convert h3 using 1
    ring
  -- Proof by contradiction: assume f(x*) > f(y)
  by_contra hcontra
  push_neg at hcontra
  let Œ¥ := f x_star - f y
  have hŒ¥_pos : Œ¥ > 0 := by simp only [Œ¥]; linarith
  -- (p - q)'(0) = 0 - (f(y) - f(x*)) = Œ¥ > 0
  have h_pq_deriv : HasDerivAt (fun t => p t - q t) Œ¥ 0 := by
    have hsub := HasDerivAt.sub p_deriv q_deriv
    convert hsub using 1
    simp only [Œ¥]
    ring
  have h_pq_0 : (fun t => p t - q t) 0 = 0 := by simp only [hp0, hq0, sub_self]
  -- Use isLittleO characterization of derivative
  rw [hasDerivAt_iff_isLittleO] at h_pq_deriv
  have hŒµ_half : 0 < Œ¥ / 2 := by linarith
  have h_bound_evt := h_pq_deriv.def hŒµ_half
  simp only [h_pq_0, sub_zero, smul_eq_mul] at h_bound_evt
  rw [Filter.eventually_iff_exists_mem] at h_bound_evt
  obtain ‚ü®s, hs_mem, hs_bound‚ü© := h_bound_evt
  rw [Metric.mem_nhds_iff] at hs_mem
  obtain ‚ü®Œµ, hŒµ_pos, hŒµ_sub‚ü© := hs_mem
  -- Choose t in (0, min(Œµ/2, 1/2)]
  let t := min (Œµ / 2) (1 / 2)
  have ht_pos : 0 < t := by positivity
  have ht_lt_Œµ : t < Œµ := by simp only [t]; linarith [min_le_left (Œµ / 2) (1 / 2)]
  have ht_le_1 : t ‚â§ 1 := by simp only [t]; linarith [min_le_right (Œµ / 2) (1 / 2)]
  have ht_in_ball : t ‚àà Metric.ball 0 Œµ := by
    simp only [Metric.mem_ball, dist_zero_right, Real.norm_eq_abs, abs_of_pos ht_pos]
    exact ht_lt_Œµ
  have ht_in_s : t ‚àà s := hŒµ_sub ht_in_ball
  have h_bound := hs_bound t ht_in_s
  simp only [Real.norm_eq_abs, abs_of_pos ht_pos] at h_bound
  -- From derivative approximation: |(p(t) - q(t)) - t*Œ¥| ‚â§ (Œ¥/2)*t
  -- So p(t) - q(t) ‚â• t*Œ¥ - (Œ¥/2)*t = (Œ¥/2)*t > 0
  have h_lower : p t - q t ‚â• t * Œ¥ - (Œ¥ / 2) * t := by
    have h1 : -((Œ¥ / 2) * t) ‚â§ (p t - q t) - t * Œ¥ := by
      have := neg_abs_le (p t - q t - t * Œ¥)
      linarith
    linarith
  have h_diff_pos : p t - q t > 0 := by
    have : t * Œ¥ - (Œ¥ / 2) * t = (Œ¥ / 2) * t := by ring
    rw [this] at h_lower
    have : (Œ¥ / 2) * t > 0 := mul_pos (by linarith) ht_pos
    linarith
  -- But convexity says p(t) ‚â§ q(t), contradiction
  have h_le := hpq_ineq t (le_of_lt ht_pos) ht_le_1
  linarith

/-- Co-coercivity of L-smooth gradients (Baillon-Haddad theorem).

    For L-smooth f: ‚ü®‚àáf(x) - ‚àáf(y), x - y‚ü© ‚â• (1/L)‚Äñ‚àáf(x) - ‚àáf(y)‚Äñ¬≤

    With y = x* where ‚àáf(x*) = 0:
    ‚ü®‚àáf(x), x - x*‚ü© ‚â• (1/L)‚Äñ‚àáf(x)‚Äñ¬≤

    Equivalently: ‚Äñ‚àáf(x)‚Äñ¬≤ ‚â§ L‚ü®‚àáf(x), x - x*‚ü© -/
theorem lsmooth_cocoercivity (f : E ‚Üí ‚Ñù) (L : ‚Ñù) (hL : 0 < L)
    (hSmooth : IsLSmooth f L) (hConvex : ConvexOn ‚Ñù Set.univ f)
    (x x_star : E) (hMin : gradient f x_star = 0) :
    ‚Äñgradient f x‚Äñ^2 ‚â§ L * @inner ‚Ñù E _ (gradient f x) (x - x_star) := by
  -- The proof uses the tilted function technique combining:
  -- 1. L-smooth descent: f(x - (1/L)g) ‚â§ f(x) - (1/2L)‚Äñg‚Äñ¬≤
  -- 2. First-order optimality for the tilted function h(z) = f(z) - ‚ü®g, z‚ü©
  -- Adding the two bounds gives (1/L)‚Äñg‚Äñ¬≤ ‚â§ ‚ü®g, x - x*‚ü©
  have hDiff : Differentiable ‚Ñù f := hSmooth.1
  let g := gradient f x

  -- Step 1: x* minimizes f (since ‚àáf(x*) = 0 and f is convex)
  have h_xstar_min : ‚àÄ y, f x_star ‚â§ f y := convex_first_order_optimality f hConvex hDiff x_star hMin

  -- Step 2: Apply lsmooth_fundamental_ineq to get descent at x
  have h_fund_f := lsmooth_fundamental_ineq f L (le_of_lt hL) hSmooth x (x - (1 / L) ‚Ä¢ g)
  have h_descent_f : f (x - (1 / L) ‚Ä¢ g) ‚â§ f x - (1 / (2 * L)) * ‚Äñg‚Äñ^2 := by
    have h_diff : (x - (1 / L) ‚Ä¢ g) - x = -((1 / L) ‚Ä¢ g) := by simp [sub_eq_add_neg, add_comm]
    have h_inner : @inner ‚Ñù E _ g ((x - (1 / L) ‚Ä¢ g) - x) = -(1 / L) * ‚Äñg‚Äñ^2 := by
      rw [h_diff]
      simp only [inner_neg_right, inner_smul_right, real_inner_self_eq_norm_sq]
      ring
    have h_norm : ‚Äñ(x - (1 / L) ‚Ä¢ g) - x‚Äñ^2 = (1 / L)^2 * ‚Äñg‚Äñ^2 := by
      rw [h_diff, norm_neg, norm_smul, Real.norm_eq_abs, abs_of_pos (by positivity : 1/L > 0)]
      ring
    calc f (x - (1 / L) ‚Ä¢ g) ‚â§ f x + @inner ‚Ñù E _ g ((x - (1 / L) ‚Ä¢ g) - x) +
                                 (L / 2) * ‚Äñ(x - (1 / L) ‚Ä¢ g) - x‚Äñ^2 := h_fund_f
      _ = f x + (-(1 / L) * ‚Äñg‚Äñ^2) + (L / 2) * ((1 / L)^2 * ‚Äñg‚Äñ^2) := by rw [h_inner, h_norm]
      _ = f x - (1 / (2 * L)) * ‚Äñg‚Äñ^2 := by field_simp; ring

  -- Bound A: (1/2L)‚Äñg‚Äñ¬≤ ‚â§ f(x) - f(x*)
  have h_bound_A : (1 / (2 * L)) * ‚Äñg‚Äñ^2 ‚â§ f x - f x_star := by
    have := h_xstar_min (x - (1 / L) ‚Ä¢ g)
    linarith

  -- Step 3: Apply fundamental ineq at x_star
  have h_fund_xstar := lsmooth_fundamental_ineq f L (le_of_lt hL) hSmooth x_star (x_star + (1 / L) ‚Ä¢ g)
  have h_fund_xstar_bound : f (x_star + (1 / L) ‚Ä¢ g) ‚â§ f x_star + (1 / (2 * L)) * ‚Äñg‚Äñ^2 := by
    have h_diff : (x_star + (1 / L) ‚Ä¢ g) - x_star = (1 / L) ‚Ä¢ g := by abel
    have h_inner : @inner ‚Ñù E _ (gradient f x_star) ((x_star + (1 / L) ‚Ä¢ g) - x_star) = 0 := by
      rw [hMin, inner_zero_left]
    have h_norm : ‚Äñ(x_star + (1 / L) ‚Ä¢ g) - x_star‚Äñ^2 = (1 / L)^2 * ‚Äñg‚Äñ^2 := by
      rw [h_diff, norm_smul, Real.norm_eq_abs, abs_of_pos (by positivity : 1/L > 0)]
      ring
    calc f (x_star + (1 / L) ‚Ä¢ g) ‚â§ f x_star + @inner ‚Ñù E _ (gradient f x_star)
          ((x_star + (1 / L) ‚Ä¢ g) - x_star) + (L / 2) * ‚Äñ(x_star + (1 / L) ‚Ä¢ g) - x_star‚Äñ^2 := h_fund_xstar
      _ = f x_star + 0 + (L / 2) * ((1 / L)^2 * ‚Äñg‚Äñ^2) := by rw [h_inner, h_norm]
      _ = f x_star + (1 / (2 * L)) * ‚Äñg‚Äñ^2 := by field_simp; ring

  -- Step 4: Tilted function h(z) = f(z) - ‚ü®g, z‚ü© is convex
  have h_convex : ConvexOn ‚Ñù Set.univ (fun z => f z - @inner ‚Ñù E _ g z) := by
    have h_linear_concave : ConcaveOn ‚Ñù Set.univ (fun z => @inner ‚Ñù E _ g z) := by
      constructor
      ¬∑ exact convex_univ
      ¬∑ intro z _ w _ a b ha hb hab
        simp only [inner_add_right, inner_smul_right, smul_eq_mul]
        linarith
    exact hConvex.sub h_linear_concave

  -- ‚àáh(x) = 0
  have h_grad_h_x : gradient (fun z => f z - @inner ‚Ñù E _ g z) x = 0 := by
    have hf_diff : DifferentiableAt ‚Ñù f x := hDiff x
    have hg_diff : DifferentiableAt ‚Ñù (fun z => @inner ‚Ñù E _ g z) x :=
      (innerSL (ùïú := ‚Ñù) g).differentiableAt
    -- The gradient of z ‚Ü¶ ‚ü®g, z‚ü© is g
    have hg_grad : HasGradientAt (fun z => @inner ‚Ñù E _ g z) g x := by
      rw [hasGradientAt_iff_hasFDerivAt]
      have h1 := (innerSL (ùïú := ‚Ñù) g).hasFDerivAt (x := x)
      simp only [InnerProductSpace.toDual] at h1 ‚ä¢
      convert h1 using 1
    -- HasGradientAt (f - inner g) (g - g) x
    have hf_grad : HasGradientAt f g x := hf_diff.hasGradientAt
    have h_sub : HasGradientAt (fun z => f z - @inner ‚Ñù E _ g z) (g - g) x := by
      have h1 := hasGradientAt_iff_hasFDerivAt.mp hf_grad
      have h2 := hasGradientAt_iff_hasFDerivAt.mp hg_grad
      have h3 := h1.sub h2
      rw [hasGradientAt_iff_hasFDerivAt]
      convert h3 using 1
      simp only [map_sub]
    rw [sub_self] at h_sub
    exact h_sub.gradient

  -- h is differentiable
  have h_diff_h : Differentiable ‚Ñù (fun z => f z - @inner ‚Ñù E _ g z) := by
    intro z
    exact (hDiff z).sub (innerSL (ùïú := ‚Ñù) g).differentiableAt

  -- x minimizes h via first-order optimality
  have h_x_min_h : ‚àÄ y, (f x - @inner ‚Ñù E _ g x) ‚â§ (f y - @inner ‚Ñù E _ g y) :=
    convex_first_order_optimality (fun z => f z - @inner ‚Ñù E _ g z) h_convex h_diff_h x h_grad_h_x

  -- h(x) ‚â§ h(x_star + (1/L)g)
  have h_hx_le := h_x_min_h (x_star + (1 / L) ‚Ä¢ g)

  -- Expand ‚ü®g, x_star + (1/L)g‚ü©
  have h_inner_xstar'_g : @inner ‚Ñù E _ g (x_star + (1 / L) ‚Ä¢ g) =
      @inner ‚Ñù E _ g x_star + (1 / L) * ‚Äñg‚Äñ^2 := by
    simp only [inner_add_right, inner_smul_right, real_inner_self_eq_norm_sq]

  -- Bound B: (1/2L)‚Äñg‚Äñ¬≤ ‚â§ f(x_star) - f(x) + ‚ü®g, x - x_star‚ü©
  have h_bound_B : (1 / (2 * L)) * ‚Äñg‚Äñ^2 ‚â§ f x_star - f x + @inner ‚Ñù E _ g (x - x_star) := by
    -- From h(x) ‚â§ h(x_star + (1/L)g):
    -- f(x) - ‚ü®g, x‚ü© ‚â§ f(x_star + (1/L)g) - ‚ü®g, x_star + (1/L)g‚ü©
    --              ‚â§ [f(x_star) + (1/2L)‚Äñg‚Äñ¬≤] - [‚ü®g, x_star‚ü© + (1/L)‚Äñg‚Äñ¬≤]
    --              = f(x_star) - ‚ü®g, x_star‚ü© - (1/2L)‚Äñg‚Äñ¬≤
    -- Rearranging: (1/2L)‚Äñg‚Äñ¬≤ ‚â§ f(x_star) - f(x) + ‚ü®g, x‚ü© - ‚ü®g, x_star‚ü©
    --                        = f(x_star) - f(x) + ‚ü®g, x - x_star‚ü©
    have h4 : @inner ‚Ñù E _ g (x - x_star) = @inner ‚Ñù E _ g x - @inner ‚Ñù E _ g x_star :=
      inner_sub_right g x x_star
    -- Substitute step3 into step1
    have step1' : f x - @inner ‚Ñù E _ g x ‚â§ f (x_star + (1 / L) ‚Ä¢ g) -
        (@inner ‚Ñù E _ g x_star + (1 / L) * ‚Äñg‚Äñ^2) := by
      rw [‚Üê h_inner_xstar'_g]
      exact h_hx_le
    -- Combine with step2
    have step2' : f (x_star + (1 / L) ‚Ä¢ g) - (@inner ‚Ñù E _ g x_star + (1 / L) * ‚Äñg‚Äñ^2) ‚â§
        f x_star + (1 / (2 * L)) * ‚Äñg‚Äñ^2 - (@inner ‚Ñù E _ g x_star + (1 / L) * ‚Äñg‚Äñ^2) := by
      linarith [h_fund_xstar_bound]
    -- Chain inequalities
    have step3' : f x - @inner ‚Ñù E _ g x ‚â§
        f x_star - @inner ‚Ñù E _ g x_star - (1 / (2 * L)) * ‚Äñg‚Äñ^2 := by
      have := le_trans step1' step2'
      have eq1 : f x_star + (1 / (2 * L)) * ‚Äñg‚Äñ^2 - (@inner ‚Ñù E _ g x_star + (1 / L) * ‚Äñg‚Äñ^2) =
          f x_star - @inner ‚Ñù E _ g x_star - (1 / (2 * L)) * ‚Äñg‚Äñ^2 := by ring
      linarith
    -- Rearrange
    have step4 : (1 / (2 * L)) * ‚Äñg‚Äñ^2 ‚â§ f x_star - f x + @inner ‚Ñù E _ g x - @inner ‚Ñù E _ g x_star := by
      linarith
    linarith

  -- Add bounds A and B: (1/L)‚Äñg‚Äñ¬≤ ‚â§ ‚ü®g, x - x*‚ü©
  have h_combined : (1 / L) * ‚Äñg‚Äñ^2 ‚â§ @inner ‚Ñù E _ g (x - x_star) := by
    have h_add := add_le_add h_bound_A h_bound_B
    -- h_add: (1/(2L))‚Äñg‚Äñ¬≤ + (1/(2L))‚Äñg‚Äñ¬≤ ‚â§ (f x - f x_star) + (f x_star - f x + ‚ü®g, x - x_star‚ü©)
    -- LHS = (1/L)‚Äñg‚Äñ¬≤, RHS = ‚ü®g, x - x_star‚ü©
    have lhs_eq : (1 / (2 * L)) * ‚Äñg‚Äñ^2 + (1 / (2 * L)) * ‚Äñg‚Äñ^2 = (1 / L) * ‚Äñg‚Äñ^2 := by field_simp; ring
    have rhs_eq : (f x - f x_star) + (f x_star - f x + @inner ‚Ñù E _ g (x - x_star)) =
        @inner ‚Ñù E _ g (x - x_star) := by ring
    linarith

  -- Multiply by L
  calc ‚Äñg‚Äñ^2 = L * ((1 / L) * ‚Äñg‚Äñ^2) := by field_simp
    _ ‚â§ L * @inner ‚Ñù E _ g (x - x_star) := by
        apply mul_le_mul_of_nonneg_left h_combined (le_of_lt hL)

/-- Interpolation condition for strongly convex AND smooth functions.

    For Œº-strongly convex and L-smooth f with ‚àáf(x*) = 0:
    ‚ü®‚àáf(x), x - x*‚ü© ‚â• (ŒºL)/(Œº+L) ‚Äñx - x*‚Äñ¬≤ + 1/(Œº+L) ‚Äñ‚àáf(x)‚Äñ¬≤

    This is stronger than using strong convexity or smoothness alone.
    It's the key to achieving the optimal (1 - Œº/L) contraction rate.

    **Proof**: Uses the auxiliary function h(x) = f(x) - (Œº/2)‚Äñx - x*‚Äñ¬≤ which is
    convex and (L-Œº)-smooth. Applying cocoercivity to h gives the bound. -/
theorem strong_smooth_interpolation (f : E ‚Üí ‚Ñù) (L Œº : ‚Ñù) (hL : 0 < L) (hŒº : 0 < Œº)
    (hSmooth : IsLSmooth f L) (hStrong : IsStronglyConvex f Œº)
    (x x_star : E) (hMin : gradient f x_star = 0) :
    @inner ‚Ñù E _ (gradient f x) (x - x_star) ‚â•
      (Œº * L) / (Œº + L) * ‚Äñx - x_star‚Äñ^2 + 1 / (Œº + L) * ‚Äñgradient f x‚Äñ^2 := by
  have hDiff : Differentiable ‚Ñù f := hSmooth.1

  -- Let g = ‚àáf(x) and d = x - x*
  let g := gradient f x
  let d := x - x_star

  -- Strong convexity implies convexity
  have hConvex : ConvexOn ‚Ñù Set.univ f :=
    stronglyConvex_implies_convexOn f Œº (le_of_lt hŒº) hStrong

  -- From strong convexity: ‚ü®g, d‚ü© ‚â• Œº‚Äñd‚Äñ¬≤
  have h_strong : @inner ‚Ñù E _ g d ‚â• Œº * ‚Äñd‚Äñ^2 := by
    have := strong_convex_gradient_monotonicity f Œº hŒº hStrong hDiff x x_star hMin
    convert this using 2 <;> rfl

  -- From cocoercivity: ‚Äñg‚Äñ¬≤ ‚â§ L‚ü®g, d‚ü©
  have h_cocoer : ‚Äñg‚Äñ^2 ‚â§ L * @inner ‚Ñù E _ g d := by
    have := lsmooth_cocoercivity f L hL hSmooth hConvex x x_star hMin
    convert this using 2 <;> rfl

  have h_sum_pos : 0 < Œº + L := by linarith

  -- The key bound comes from the auxiliary function h(x) = f(x) - (Œº/2)‚Äñx - x*‚Äñ¬≤
  -- which is convex and (L-Œº)-smooth.
  -- Cocoercivity for h gives: ‚Äñ‚àáh(x)‚Äñ¬≤ ‚â§ (L-Œº)‚ü®‚àáh(x), x - x*‚ü©
  -- where ‚àáh(x) = ‚àáf(x) - Œº(x - x*) = g - Œºd
  --
  -- Expanding: ‚Äñg - Œºd‚Äñ¬≤ ‚â§ (L-Œº)‚ü®g - Œºd, d‚ü©
  -- After algebra: ‚Äñg‚Äñ¬≤ + ŒºL‚Äñd‚Äñ¬≤ ‚â§ (L+Œº)‚ü®g, d‚ü©

  have h_aux_cocoer : ‚Äñg - Œº ‚Ä¢ d‚Äñ^2 ‚â§ (L - Œº) * @inner ‚Ñù E _ (g - Œº ‚Ä¢ d) d := by
    -- First handle d = 0 case (when x = x*)
    by_cases hd_zero : d = 0
    ¬∑ -- When d = 0, we have x = x*, so g = ‚àáf(x*) = 0 by hMin
      have hg_zero : g = 0 := by simp only [g, d] at hd_zero ‚ä¢; simp [sub_eq_zero.mp hd_zero, hMin]
      simp only [hd_zero, hg_zero, smul_zero, sub_zero, norm_zero, sq, mul_zero,
                 inner_zero_right, mul_zero, le_refl]
    -- Now assume d ‚â† 0
    -- Case split: L = Œº vs L ‚â† Œº
    by_cases hLŒº : L = Œº
    ¬∑ -- Case L = Œº: RHS = 0, need LHS ‚â§ 0, which holds since LHS ‚â• 0 means LHS = 0
      -- Actually we just need ‚Äñg - Œºd‚Äñ¬≤ ‚â§ 0 which is true iff ‚Äñg - Œºd‚Äñ¬≤ = 0
      -- But wait, we're showing ‚â§, and RHS = 0, so we need LHS ‚â§ 0
      -- Since LHS = ‚Äñg - Œºd‚Äñ¬≤ ‚â• 0 always, we need LHS = 0
      -- This is NOT generally true for L = Œº case!
      -- Actually the RHS is 0, and LHS ‚â• 0, so we need LHS ‚â§ 0
      -- The only way this works is if LHS = 0.
      -- For L = Œº, the bound is tight only at the optimum.
      -- Let's reconsider: we can use that ‚Äñg - Œºd‚Äñ¬≤ ‚â• 0 and show RHS ‚â• LHS
      -- When L = Œº, RHS = 0, so we need ‚Äñg - Œºd‚Äñ¬≤ ‚â§ 0, forcing equality.
      -- This requires g = Œºd, which is the gradient condition at optimum.
      -- But x is arbitrary! So this case needs the strong condition.
      -- Actually, for L = Œº (condition number 1), f(x) = (Œº/2)‚Äñx - x*‚Äñ¬≤ + const
      -- So ‚àáf(x) = Œº(x - x*) = Œºd, hence g = Œºd, hence g - Œºd = 0.
      have h_grad_eq : g = Œº ‚Ä¢ d := by
        -- When L = Œº, the gradient is forced to be exactly linear
        -- From L-smooth: ‚Äñ‚àáf(x) - ‚àáf(x*)‚Äñ ‚â§ Œº‚Äñx - x*‚Äñ (using hLŒº : L = Œº)
        -- From strong convexity gradient monotonicity: ‚ü®‚àáf(x) - ‚àáf(x*), x - x*‚ü© ‚â• Œº‚Äñx - x*‚Äñ¬≤
        -- With ‚àáf(x*) = 0: ‚Äñg‚Äñ ‚â§ Œº‚Äñd‚Äñ and ‚ü®g, d‚ü© ‚â• Œº‚Äñd‚Äñ¬≤
        -- Cauchy-Schwarz: ‚ü®g, d‚ü© ‚â§ ‚Äñg‚Äñ ¬∑ ‚Äñd‚Äñ ‚â§ Œº‚Äñd‚Äñ¬≤
        -- So ‚ü®g, d‚ü© = Œº‚Äñd‚Äñ¬≤ and ‚Äñg‚Äñ = Œº‚Äñd‚Äñ, forcing g = Œºd
        have hg_bound : ‚Äñg‚Äñ ‚â§ Œº * ‚Äñd‚Äñ := by
          have := hSmooth.2 x x_star
          simp only [g, d, hMin, sub_zero] at this
          rw [hLŒº] at this
          exact this
        have h_inner_eq : @inner ‚Ñù E _ g d = Œº * ‚Äñd‚Äñ^2 := by
          -- From h_strong: ‚ü®g, d‚ü© ‚â• Œº‚Äñd‚Äñ¬≤
          -- From Cauchy-Schwarz and hg_bound: ‚ü®g, d‚ü© ‚â§ ‚Äñg‚Äñ¬∑‚Äñd‚Äñ ‚â§ Œº‚Äñd‚Äñ¬≤
          have h_upper : @inner ‚Ñù E _ g d ‚â§ Œº * ‚Äñd‚Äñ^2 := by
            calc @inner ‚Ñù E _ g d ‚â§ ‚Äñg‚Äñ * ‚Äñd‚Äñ := real_inner_le_norm g d
              _ ‚â§ Œº * ‚Äñd‚Äñ * ‚Äñd‚Äñ := by apply mul_le_mul_of_nonneg_right hg_bound (norm_nonneg d)
              _ = Œº * ‚Äñd‚Äñ^2 := by ring
          linarith [h_strong]
        -- Equality in Cauchy-Schwarz means g and d are parallel: g = c ‚Ä¢ d for some c
        -- Combined with ‚ü®g, d‚ü© = Œº‚Äñd‚Äñ¬≤, if d ‚â† 0 then c = Œº
        by_cases hd_case : d = 0
        ¬∑ -- d = 0 contradicts hd_zero
          exact absurd hd_case hd_zero
        ¬∑ -- d ‚â† 0: prove g = Œº ‚Ä¢ d
          have hd_pos' : ‚Äñd‚Äñ > 0 := norm_pos_iff.mpr hd_case
          -- From equality in Cauchy-Schwarz: g = (‚ü®g,d‚ü©/‚Äñd‚Äñ¬≤) ‚Ä¢ d
          have h_cs_eq : @inner ‚Ñù E _ g d = ‚Äñg‚Äñ * ‚Äñd‚Äñ := by
            -- ‚ü®g, d‚ü© = Œº‚Äñd‚Äñ¬≤ and ‚Äñg‚Äñ ‚â§ Œº‚Äñd‚Äñ
            -- Cauchy-Schwarz: ‚ü®g, d‚ü© ‚â§ ‚Äñg‚Äñ ¬∑ ‚Äñd‚Äñ
            -- If ‚ü®g, d‚ü© < ‚Äñg‚Äñ ¬∑ ‚Äñd‚Äñ then ‚ü®g, d‚ü© < Œº‚Äñd‚Äñ ¬∑ ‚Äñd‚Äñ = Œº‚Äñd‚Äñ¬≤
            -- But ‚ü®g, d‚ü© = Œº‚Äñd‚Äñ¬≤, contradiction
            have h1 : @inner ‚Ñù E _ g d ‚â§ ‚Äñg‚Äñ * ‚Äñd‚Äñ := real_inner_le_norm g d
            have h2 : ‚Äñg‚Äñ * ‚Äñd‚Äñ ‚â§ Œº * ‚Äñd‚Äñ * ‚Äñd‚Äñ := by
              apply mul_le_mul_of_nonneg_right hg_bound (norm_nonneg d)
            have h3 : Œº * ‚Äñd‚Äñ * ‚Äñd‚Äñ = Œº * ‚Äñd‚Äñ^2 := by ring
            rw [h_inner_eq]
            by_contra h_ne
            have h_lt : Œº * ‚Äñd‚Äñ^2 < ‚Äñg‚Äñ * ‚Äñd‚Äñ := by
              push_neg at h_ne
              rcases (ne_iff_lt_or_gt.mp h_ne) with h_lt | h_gt
              ¬∑ exact h_lt
              ¬∑ linarith [h1]
            have : Œº * ‚Äñd‚Äñ^2 < Œº * ‚Äñd‚Äñ^2 := by
              calc Œº * ‚Äñd‚Äñ^2 < ‚Äñg‚Äñ * ‚Äñd‚Äñ := h_lt
                _ ‚â§ Œº * ‚Äñd‚Äñ * ‚Äñd‚Äñ := h2
                _ = Œº * ‚Äñd‚Äñ^2 := h3
            linarith
          -- Now g is parallel to d with positive coefficient
          have h_norm_eq : ‚Äñg‚Äñ = Œº * ‚Äñd‚Äñ := by
            have h1 : @inner ‚Ñù E _ g d = ‚Äñg‚Äñ * ‚Äñd‚Äñ := h_cs_eq
            rw [h_inner_eq] at h1
            have h2 : Œº * ‚Äñd‚Äñ^2 = ‚Äñg‚Äñ * ‚Äñd‚Äñ := h1
            have h3 : Œº * ‚Äñd‚Äñ^2 / ‚Äñd‚Äñ = ‚Äñg‚Äñ * ‚Äñd‚Äñ / ‚Äñd‚Äñ := by rw [h2]
            simp only [sq, mul_div_assoc, div_self (ne_of_gt hd_pos'), mul_one] at h3
            linarith
          -- g and Œº‚Ä¢d have same norm and same inner product with d
          -- This means g = Œº‚Ä¢d (parallel with same magnitude and direction)
          have h_same_inner : @inner ‚Ñù E _ g d = @inner ‚Ñù E _ (Œº ‚Ä¢ d) d := by
            rw [h_inner_eq, inner_smul_left, real_inner_self_eq_norm_sq]
            simp only [conj_trivial]
          have h_same_norm : ‚Äñg‚Äñ = ‚ÄñŒº ‚Ä¢ d‚Äñ := by
            rw [h_norm_eq, norm_smul, Real.norm_eq_abs, abs_of_pos hŒº]
          -- The difference g - Œº‚Ä¢d has norm 0
          have h_diff_zero : ‚Äñg - Œº ‚Ä¢ d‚Äñ = 0 := by
            have h1 : ‚Äñg - Œº ‚Ä¢ d‚Äñ^2 = ‚Äñg‚Äñ^2 - 2 * @inner ‚Ñù E _ g (Œº ‚Ä¢ d) + ‚ÄñŒº ‚Ä¢ d‚Äñ^2 := by
              rw [sub_eq_add_neg, norm_add_sq_real]
              simp only [norm_neg, inner_neg_right]
              ring
            have h2 : @inner ‚Ñù E _ g (Œº ‚Ä¢ d) = Œº * @inner ‚Ñù E _ g d := by
              simp only [inner_smul_right, conj_trivial]
            rw [h2, h_inner_eq, h_same_norm] at h1
            simp only [norm_smul, Real.norm_eq_abs, abs_of_pos hŒº] at h1
            have h3 : (Œº * ‚Äñd‚Äñ)^2 - 2 * (Œº * (Œº * ‚Äñd‚Äñ^2)) + (Œº * ‚Äñd‚Äñ)^2 = 0 := by ring
            have h4 : ‚Äñg - Œº ‚Ä¢ d‚Äñ^2 = 0 := by linarith [h1, h3]
            have h5 : ‚Äñg - Œº ‚Ä¢ d‚Äñ^2 = ‚Äñg - Œº ‚Ä¢ d‚Äñ * ‚Äñg - Œº ‚Ä¢ d‚Äñ := sq _
            rw [h5] at h4
            exact mul_self_eq_zero.mp h4
          exact sub_eq_zero.mp (norm_eq_zero.mp h_diff_zero)
      rw [h_grad_eq, sub_self]
      simp only [norm_zero, sq, mul_zero, inner_zero_left, mul_zero, le_refl]
    ¬∑ -- Case L ‚â† Œº: Need to show L > Œº first
      have hL_ge_Œº : L ‚â• Œº := by
        -- Use L-smooth upper bound and Œº-strong convex lower bound at x*
        -- Upper: f(y) ‚â§ f(x*) + ‚ü®‚àáf(x*), y-x*‚ü© + (L/2)‚Äñy-x*‚Äñ¬≤ = f(x*) + (L/2)‚Äñy-x*‚Äñ¬≤
        -- Lower (from x* minimality): f(y) ‚â• f(x*)
        -- Combined: 0 ‚â§ (L/2)‚Äñy-x*‚Äñ¬≤ for all y, which is always true.
        -- We need a tighter lower bound from strong convexity.
        --
        -- Actually, use the gradient bounds we already have:
        -- h_cocoer: ‚Äñg‚Äñ¬≤ ‚â§ L‚ü®g, d‚ü©  and  h_strong: ‚ü®g, d‚ü© ‚â• Œº‚Äñd‚Äñ¬≤
        -- If d ‚â† 0 and g ‚â† 0:
        --   From h_strong and Cauchy-Schwarz: Œº‚Äñd‚Äñ¬≤ ‚â§ ‚ü®g,d‚ü© ‚â§ ‚Äñg‚Äñ¬∑‚Äñd‚Äñ, so Œº‚Äñd‚Äñ ‚â§ ‚Äñg‚Äñ
        --   From h_cocoer and Cauchy-Schwarz: ‚Äñg‚Äñ¬≤ ‚â§ L‚ü®g,d‚ü© ‚â§ L‚Äñg‚Äñ¬∑‚Äñd‚Äñ, so ‚Äñg‚Äñ ‚â§ L‚Äñd‚Äñ
        --   Combined: Œº‚Äñd‚Äñ ‚â§ ‚Äñg‚Äñ ‚â§ L‚Äñd‚Äñ, hence Œº ‚â§ L
        -- We know d ‚â† 0 from hd_zero above
        have hd : d ‚â† 0 := hd_zero
        have hd_pos : ‚Äñd‚Äñ > 0 := norm_pos_iff.mpr hd
        -- From h_strong: Œº‚Äñd‚Äñ¬≤ ‚â§ ‚ü®g, d‚ü©
        -- From Cauchy-Schwarz: ‚ü®g, d‚ü© ‚â§ ‚Äñg‚Äñ ¬∑ ‚Äñd‚Äñ
        have h1 : Œº * ‚Äñd‚Äñ^2 ‚â§ ‚Äñg‚Äñ * ‚Äñd‚Äñ := by
          calc Œº * ‚Äñd‚Äñ^2 ‚â§ @inner ‚Ñù E _ g d := h_strong
            _ ‚â§ ‚Äñg‚Äñ * ‚Äñd‚Äñ := real_inner_le_norm g d
        -- Divide by ‚Äñd‚Äñ > 0: Œº * ‚Äñd‚Äñ ‚â§ ‚Äñg‚Äñ
        have h2 : Œº * ‚Äñd‚Äñ ‚â§ ‚Äñg‚Äñ := by
          have hd_ne : ‚Äñd‚Äñ ‚â† 0 := ne_of_gt hd_pos
          have h1' : Œº * ‚Äñd‚Äñ^2 / ‚Äñd‚Äñ ‚â§ ‚Äñg‚Äñ * ‚Äñd‚Äñ / ‚Äñd‚Äñ :=
            div_le_div_of_nonneg_right h1 (le_of_lt hd_pos)
          simp only [sq, mul_div_assoc, div_self hd_ne, mul_one] at h1'
          linarith
        -- From h_cocoer: ‚Äñg‚Äñ¬≤ ‚â§ L‚ü®g, d‚ü© ‚â§ L‚Äñg‚Äñ¬∑‚Äñd‚Äñ
        by_cases hg : g = 0
        ¬∑ -- If g = 0, then from h_strong: 0 ‚â• Œº‚Äñd‚Äñ¬≤ > 0, contradiction with d ‚â† 0
          simp only [hg, inner_zero_left] at h_strong
          have : Œº * ‚Äñd‚Äñ^2 > 0 := mul_pos hŒº (sq_pos_of_pos hd_pos)
          linarith
        ¬∑ -- g ‚â† 0
          have hg_pos : ‚Äñg‚Äñ > 0 := norm_pos_iff.mpr hg
          have hg_ne : ‚Äñg‚Äñ ‚â† 0 := ne_of_gt hg_pos
          have h3 : ‚Äñg‚Äñ^2 ‚â§ L * (‚Äñg‚Äñ * ‚Äñd‚Äñ) := by
            calc ‚Äñg‚Äñ^2 ‚â§ L * @inner ‚Ñù E _ g d := h_cocoer
              _ ‚â§ L * (‚Äñg‚Äñ * ‚Äñd‚Äñ) := by
                  apply mul_le_mul_of_nonneg_left (real_inner_le_norm g d) (le_of_lt hL)
          -- ‚Äñg‚Äñ¬≤ ‚â§ L¬∑‚Äñg‚Äñ¬∑‚Äñd‚Äñ, divide by ‚Äñg‚Äñ > 0
          have h4 : ‚Äñg‚Äñ ‚â§ L * ‚Äñd‚Äñ := by
            -- ‚Äñg‚Äñ¬≤ ‚â§ L * ‚Äñg‚Äñ * ‚Äñd‚Äñ, so ‚Äñg‚Äñ ‚â§ L * ‚Äñd‚Äñ (dividing by ‚Äñg‚Äñ > 0)
            have h3' : ‚Äñg‚Äñ * ‚Äñg‚Äñ ‚â§ L * (‚Äñg‚Äñ * ‚Äñd‚Äñ) := by
              simp only [sq] at h3; exact h3
            have key : ‚Äñg‚Äñ ‚â§ L * ‚Äñd‚Äñ := by
              by_contra h_neg
              push_neg at h_neg
              -- h_neg : L * ‚Äñd‚Äñ < ‚Äñg‚Äñ
              -- Then L * (‚Äñg‚Äñ * ‚Äñd‚Äñ) < ‚Äñg‚Äñ * ‚Äñg‚Äñ, contradicting h3'
              have : L * (‚Äñg‚Äñ * ‚Äñd‚Äñ) = ‚Äñg‚Äñ * (L * ‚Äñd‚Äñ) := by ring
              rw [this] at h3'
              have h4' : ‚Äñg‚Äñ * (L * ‚Äñd‚Äñ) < ‚Äñg‚Äñ * ‚Äñg‚Äñ := by
                apply mul_lt_mul_of_pos_left h_neg hg_pos
              linarith
            exact key
          -- Combine: Œº¬∑‚Äñd‚Äñ ‚â§ ‚Äñg‚Äñ ‚â§ L¬∑‚Äñd‚Äñ, so Œº ‚â§ L
          have h5 : Œº * ‚Äñd‚Äñ ‚â§ L * ‚Äñd‚Äñ := le_trans h2 h4
          have h6 : (Œº * ‚Äñd‚Äñ) / ‚Äñd‚Äñ ‚â§ (L * ‚Äñd‚Äñ) / ‚Äñd‚Äñ :=
            div_le_div_of_nonneg_right h5 (le_of_lt hd_pos)
          simp only [mul_div_assoc, div_self (ne_of_gt hd_pos), mul_one] at h6
          linarith
      have hL_gt_Œº : L > Œº := lt_of_le_of_ne hL_ge_Œº (Ne.symm hLŒº)

      -- Define auxiliary function h(z) = f(z) - (Œº/2)‚Äñz - x*‚Äñ¬≤
      let h := fun z : E => f z - (Œº / 2) * ‚Äñz - x_star‚Äñ^2

      -- The key insight: for the auxiliary function h:
      -- 1. ‚àáh(z) = ‚àáf(z) - Œº(z - x*), so ‚àáh(x) = g - Œºd
      -- 2. ‚àáh(x*) = ‚àáf(x*) - 0 = 0
      -- 3. h is convex (from Œº-strong convexity of f)
      -- 4. h is (L-Œº)-smooth (from L-smoothness of f)
      --
      -- Applying lsmooth_cocoercivity to h gives:
      -- ‚Äñ‚àáh(x)‚Äñ¬≤ ‚â§ (L-Œº)‚ü®‚àáh(x), x - x*‚ü©
      -- which is: ‚Äñg - Œºd‚Äñ¬≤ ‚â§ (L-Œº)‚ü®g - Œºd, d‚ü©

      -- h is differentiable
      have h_diff : Differentiable ‚Ñù h := by
        intro z
        apply DifferentiableAt.sub (hDiff z)
        apply DifferentiableAt.const_mul
        have h1 : DifferentiableAt ‚Ñù (fun w => w - x_star) z :=
          differentiableAt_id.sub (differentiableAt_const x_star)
        exact h1.norm_sq (ùïú := ‚Ñù)

      -- ‚àáh(z) = ‚àáf(z) - Œº(z - x*)
      have h_grad : ‚àÄ z, gradient h z = gradient f z - Œº ‚Ä¢ (z - x_star) := by
        intro z
        -- Differentiability facts
        have h_shift_diff : DifferentiableAt ‚Ñù (fun w => w - x_star) z :=
          differentiableAt_id.sub (differentiableAt_const _)
        have h_norm_sq_diff : DifferentiableAt ‚Ñù (fun w => ‚Äñw - x_star‚Äñ^2) z :=
          h_shift_diff.norm_sq (ùïú := ‚Ñù)
        have h_scaled_diff : DifferentiableAt ‚Ñù (fun w => (Œº / 2) * ‚Äñw - x_star‚Äñ^2) z :=
          (differentiableAt_const _).mul h_norm_sq_diff
        -- Key fact: gradient of (Œº/2)‚Äñw - x*‚Äñ¬≤ is Œº(w - x*)
        -- fderiv ‚Ñù (fun w => ‚Äñw - x*‚Äñ¬≤) z = 2 ‚Ä¢ innerSL ‚Ñù (z - x*)
        -- So gradient = (toDual ‚Ñù E).symm(2 ‚Ä¢ innerSL ‚Ñù (z - x*)) = 2(z - x*)
        -- And gradient of (Œº/2)‚Äñw - x*‚Äñ¬≤ = (Œº/2) * 2(z - x*) = Œº(z - x*)
        have h_grad_norm_sq : gradient (fun w => ‚Äñw - x_star‚Äñ^2) z = (2 : ‚Ñù) ‚Ä¢ (z - x_star) := by
          simp only [gradient]
          have hfd : HasFDerivAt (fun w : E => w - x_star) (ContinuousLinearMap.id ‚Ñù E) z := by
            have hsub := (hasFDerivAt_id (ùïú := ‚Ñù) z).sub (hasFDerivAt_const (ùïú := ‚Ñù) x_star z)
            simp only [ContinuousLinearMap.sub_apply, ContinuousLinearMap.id_apply,
                       ContinuousLinearMap.zero_apply, sub_zero] at hsub
            exact hsub
          have h_comp : (innerSL ‚Ñù (z - x_star : E)).comp (ContinuousLinearMap.id ‚Ñù E) =
              innerSL ‚Ñù (z - x_star) := by ext; simp
          have hfd_norm : HasFDerivAt (fun w => ‚Äñw - x_star‚Äñ^2) (2 ‚Ä¢ innerSL ‚Ñù (z - x_star)) z := by
            have := hfd.norm_sq
            simp only [h_comp] at this
            exact this
          rw [hfd_norm.fderiv]
          -- (toDual ‚Ñù E).symm (2 ‚Ä¢ innerSL ‚Ñù (z - x*)) = 2 ‚Ä¢ (z - x*)
          -- Convert ‚Ñï-smul to ‚Ñù-smul
          rw [‚Üê Nat.cast_smul_eq_nsmul ‚Ñù (2 : ‚Ñï) (innerSL ‚Ñù (z - x_star))]
          rw [LinearIsometryEquiv.map_smul]
          congr 1
          -- innerSL ‚Ñù v = toDual ‚Ñù E v, so symm gives v back
          have : innerSL ‚Ñù (z - x_star) = InnerProductSpace.toDual ‚Ñù E (z - x_star) := rfl
          rw [this, LinearIsometryEquiv.symm_apply_apply]
        -- First prove HasGradientAt for the scaled norm squared term
        have h_grad_scaled_at : HasGradientAt (fun w => (Œº / 2) * ‚Äñw - x_star‚Äñ^2) (Œº ‚Ä¢ (z - x_star)) z := by
          -- Build from HasFDerivAt
          have hfd : HasFDerivAt (fun w : E => w - x_star) (ContinuousLinearMap.id ‚Ñù E) z := by
            have hsub := (hasFDerivAt_id (ùïú := ‚Ñù) z).sub (hasFDerivAt_const (ùïú := ‚Ñù) x_star z)
            simp only [ContinuousLinearMap.sub_apply, ContinuousLinearMap.id_apply,
                       ContinuousLinearMap.zero_apply, sub_zero] at hsub
            exact hsub
          have h_comp : (innerSL ‚Ñù (z - x_star : E)).comp (ContinuousLinearMap.id ‚Ñù E) =
              innerSL ‚Ñù (z - x_star) := by ext; simp
          have hfd_norm : HasFDerivAt (fun w => ‚Äñw - x_star‚Äñ^2) (2 ‚Ä¢ innerSL ‚Ñù (z - x_star)) z := by
            have := hfd.norm_sq
            simp only [h_comp] at this
            exact this
          -- (Œº/2) * ‚Äñw - x*‚Äñ¬≤ has derivative (Œº/2) ‚Ä¢ (2 ‚Ä¢ innerSL ‚Ñù (z - x*)) = Œº ‚Ä¢ innerSL ‚Ñù (z - x*)
          have hfd_scaled : HasFDerivAt (fun w => (Œº / 2) * ‚Äñw - x_star‚Äñ^2)
              ((Œº / 2) ‚Ä¢ (2 ‚Ä¢ innerSL ‚Ñù (z - x_star))) z := by
            have hconst : HasFDerivAt (fun _ : E => Œº / 2) 0 z := hasFDerivAt_const (ùïú := ‚Ñù) _ _
            have hmul := hconst.mul hfd_norm
            -- hmul has type: HasFDerivAt ((fun x ‚Ü¶ Œº/2) * (fun w ‚Ü¶ ‚Äñw - x*‚Äñ¬≤)) (... + ‚Äñz-x*‚Äñ¬≤ ‚Ä¢ 0) z
            -- Simplify: ‚Äñz-x*‚Äñ¬≤ ‚Ä¢ 0 = 0, and (fun x ‚Ü¶ c) * g = fun x ‚Ü¶ c * g x
            simp only [smul_zero, add_zero] at hmul
            convert hmul using 2
          -- Simplify (Œº/2) ‚Ä¢ (2 ‚Ä¢ innerSL) = Œº ‚Ä¢ innerSL
          -- Note: 2 ‚Ä¢ is nsmul (‚Ñï), so first convert to ‚Ñù-smul
          have h_smul_simp : (Œº / 2) ‚Ä¢ (2 ‚Ä¢ innerSL ‚Ñù (z - x_star)) = Œº ‚Ä¢ innerSL ‚Ñù (z - x_star) := by
            rw [‚Üê Nat.cast_smul_eq_nsmul ‚Ñù (2 : ‚Ñï) (innerSL ‚Ñù (z - x_star))]
            rw [smul_smul]; congr 1; ring
          rw [h_smul_simp] at hfd_scaled
          -- Convert to HasGradientAt: need to show (toDual ‚Ñù E).symm (Œº ‚Ä¢ innerSL ‚Ñù (z - x*)) = Œº ‚Ä¢ (z - x*)
          rw [hasFDerivAt_iff_hasGradientAt] at hfd_scaled
          convert hfd_scaled using 1
          rw [LinearIsometryEquiv.map_smul]
          congr 1
          have : innerSL ‚Ñù (z - x_star) = InnerProductSpace.toDual ‚Ñù E (z - x_star) := rfl
          rw [this, LinearIsometryEquiv.symm_apply_apply]
        have h_grad_scaled : gradient (fun w => (Œº / 2) * ‚Äñw - x_star‚Äñ^2) z = Œº ‚Ä¢ (z - x_star) :=
          h_grad_scaled_at.gradient
        -- Now combine: h = f - (Œº/2)‚Äñ¬∑-x*‚Äñ¬≤, so gradient h = gradient f - gradient((Œº/2)‚Äñ¬∑-x*‚Äñ¬≤)
        have h_grad_h_at : HasGradientAt h (gradient f z - Œº ‚Ä¢ (z - x_star)) z := by
          have hf_at := (hDiff z).hasGradientAt
          -- Use HasFDerivAt.sub then convert back to HasGradientAt
          have h_fderiv_f := hf_at.hasFDerivAt
          have h_fderiv_scaled := h_grad_scaled_at.hasFDerivAt
          have h_fderiv_sub := h_fderiv_f.sub h_fderiv_scaled
          -- h_fderiv_sub : HasFDerivAt (f - (Œº/2)*‚Äñ¬∑-x*‚Äñ¬≤) (toDual(‚àáf z) - toDual(Œº(z-x*))) z
          rw [hasFDerivAt_iff_hasGradientAt] at h_fderiv_sub
          convert h_fderiv_sub using 1
          rw [LinearIsometryEquiv.map_sub, LinearIsometryEquiv.symm_apply_apply,
              LinearIsometryEquiv.symm_apply_apply]
        simp only [h]
        exact h_grad_h_at.gradient

      -- ‚àáh(x*) = 0
      have h_grad_xstar : gradient h x_star = 0 := by
        rw [h_grad x_star]
        simp only [sub_self, smul_zero, sub_zero, hMin]

      -- h is convex (from strong convexity of f)
      -- The proof uses the identity:
      -- a‚Äñz-x*‚Äñ¬≤ + (1-a)‚Äñw-x*‚Äñ¬≤ = ‚Äñaz+(1-a)w-x*‚Äñ¬≤ + a(1-a)‚Äñz-w‚Äñ¬≤
      have h_convex : ConvexOn ‚Ñù Set.univ h := by
        constructor
        ¬∑ exact convex_univ
        ¬∑ intro z _ w _ a b ha hb hab
          simp only [h, smul_eq_mul]
          have hb_eq : b = 1 - a := by linarith
          rw [hb_eq]
          -- The convex combination identity for norms
          have h_convex_identity : a * ‚Äñz - x_star‚Äñ^2 + (1 - a) * ‚Äñw - x_star‚Äñ^2 =
              ‚Äña ‚Ä¢ z + (1 - a) ‚Ä¢ w - x_star‚Äñ^2 + a * (1 - a) * ‚Äñz - w‚Äñ^2 := by
            -- Let u = z - x*, v = w - x*
            set u := z - x_star
            set v := w - x_star
            have hsum : a ‚Ä¢ z + (1 - a) ‚Ä¢ w - x_star = a ‚Ä¢ u + (1 - a) ‚Ä¢ v := by
              have : a ‚Ä¢ x_star + (1 - a) ‚Ä¢ x_star = x_star := by
                rw [‚Üê add_smul]; simp only [add_sub_cancel, one_smul]
              calc a ‚Ä¢ z + (1 - a) ‚Ä¢ w - x_star
                  = a ‚Ä¢ z - a ‚Ä¢ x_star + ((1 - a) ‚Ä¢ w - (1 - a) ‚Ä¢ x_star) := by module
                _ = a ‚Ä¢ (z - x_star) + (1 - a) ‚Ä¢ (w - x_star) := by simp only [smul_sub]
                _ = a ‚Ä¢ u + (1 - a) ‚Ä¢ v := by simp only [u, v]
            have hdiff : z - w = u - v := by simp only [u, v]; module
            rw [hsum, hdiff]
            have expand_lhs : ‚Äña ‚Ä¢ u + (1 - a) ‚Ä¢ v‚Äñ^2 =
                a^2 * ‚Äñu‚Äñ^2 + (1 - a)^2 * ‚Äñv‚Äñ^2 + 2 * a * (1 - a) * @inner ‚Ñù E _ u v := by
              rw [norm_add_sq_real, norm_smul, norm_smul, Real.norm_eq_abs, Real.norm_eq_abs,
                  abs_of_nonneg ha, abs_of_nonneg (by linarith : 0 ‚â§ 1 - a),
                  inner_smul_left, inner_smul_right]
              simp only [conj_trivial]
              ring
            have expand_diff : ‚Äñu - v‚Äñ^2 = ‚Äñu‚Äñ^2 + ‚Äñv‚Äñ^2 - 2 * @inner ‚Ñù E _ u v := by
              rw [norm_sub_sq_real]; ring
            rw [expand_lhs, expand_diff]
            ring
          have h_from_sc := hStrong z w a ha (by linarith : a ‚â§ 1)
          -- The strong convexity gives:
          -- f(az+(1-a)w) ‚â§ a*f(z) + (1-a)*f(w) - (Œº/2)*a*(1-a)*‚Äñz-w‚Äñ¬≤
          -- Combined with h_convex_identity, this implies convexity of h
          -- Expand and simplify using h_convex_identity and h_from_sc
          have h_expand : a * (f z - Œº / 2 * ‚Äñz - x_star‚Äñ^2) + (1 - a) * (f w - Œº / 2 * ‚Äñw - x_star‚Äñ^2) =
              a * f z + (1 - a) * f w - Œº / 2 * (a * ‚Äñz - x_star‚Äñ^2 + (1 - a) * ‚Äñw - x_star‚Äñ^2) := by
            ring
          have h_rhs : f (a ‚Ä¢ z + (1 - a) ‚Ä¢ w) - Œº / 2 * ‚Äña ‚Ä¢ z + (1 - a) ‚Ä¢ w - x_star‚Äñ^2 =
              f (a ‚Ä¢ z + (1 - a) ‚Ä¢ w) - Œº / 2 * (a * ‚Äñz - x_star‚Äñ^2 + (1 - a) * ‚Äñw - x_star‚Äñ^2)
              + Œº / 2 * a * (1 - a) * ‚Äñz - w‚Äñ^2 := by
            rw [h_convex_identity]; ring
          rw [h_expand, h_rhs]
          have h_ineq : f (a ‚Ä¢ z + (1 - a) ‚Ä¢ w) + Œº / 2 * a * (1 - a) * ‚Äñz - w‚Äñ^2 ‚â§
              a * f z + (1 - a) * f w := by linarith
          linarith

      -- Prove cocoercivity for h directly, following the technique from lsmooth_cocoercivity
      -- The key steps are:
      -- 1. x* minimizes h (since ‚àáh(x*) = 0 and h is convex)
      -- 2. Use descent lemma at x and x* to bound function differences
      -- 3. Use tilted function technique to relate to inner product

      have hL_sub_Œº_pos : 0 < L - Œº := sub_pos.mpr hL_gt_Œº
      let g' := gradient h x

      -- Step 1: x* minimizes h (since ‚àáh(x*) = 0 and h is convex)
      have h_xstar_min : ‚àÄ y, h x_star ‚â§ h y :=
        convex_first_order_optimality h h_convex h_diff x_star h_grad_xstar

      -- h satisfies the descent lemma (fundamental inequality) with constant (L-Œº)
      have h_descent : ‚àÄ u v, h v ‚â§ h u + @inner ‚Ñù E _ (gradient h u) (v - u) +
          ((L - Œº) / 2) * ‚Äñv - u‚Äñ^2 := by
        intro u v
        have hf_desc := lsmooth_fundamental_ineq f L (le_of_lt hL) hSmooth u v
        -- Expand: h(v) = f(v) - (Œº/2)‚Äñv - x*‚Äñ¬≤
        -- Need to show: h(v) ‚â§ h(u) + ‚ü®‚àáh(u), v-u‚ü© + ((L-Œº)/2)‚Äñv-u‚Äñ¬≤
        -- where ‚àáh(u) = ‚àáf(u) - Œº(u - x*)

        -- Key identity: ‚Äñv - x*‚Äñ¬≤ = ‚Äñ(v-u) + (u-x*)‚Äñ¬≤ = ‚Äñv-u‚Äñ¬≤ + ‚Äñu-x*‚Äñ¬≤ + 2‚ü®u-x*, v-u‚ü©
        have h_norm_expand : ‚Äñv - x_star‚Äñ^2 =
            ‚Äñv - u‚Äñ^2 + ‚Äñu - x_star‚Äñ^2 + 2 * @inner ‚Ñù E _ (u - x_star) (v - u) := by
          have hvu : v - x_star = (v - u) + (u - x_star) := by abel
          rw [hvu, norm_add_sq_real]
          -- norm_add_sq_real gives ‚ü®v-u, u-x*‚ü©, need to swap to ‚ü®u-x*, v-u‚ü©
          rw [real_inner_comm (v - u) (u - x_star)]
          ring

        -- Expand inner product: ‚ü®‚àáf(u) - Œº(u-x*), v-u‚ü© = ‚ü®‚àáf(u), v-u‚ü© - Œº‚ü®u-x*, v-u‚ü©
        have h_inner_expand : @inner ‚Ñù E _ (gradient f u - Œº ‚Ä¢ (u - x_star)) (v - u) =
            @inner ‚Ñù E _ (gradient f u) (v - u) - Œº * @inner ‚Ñù E _ (u - x_star) (v - u) := by
          rw [inner_sub_left, inner_smul_left]
          simp only [conj_trivial]

        -- Target: h(v) ‚â§ h(u) + ‚ü®‚àáh(u), v-u‚ü© + ((L-Œº)/2)‚Äñv-u‚Äñ¬≤
        -- h(v) = f(v) - (Œº/2)‚Äñv-x*‚Äñ¬≤
        -- h(u) = f(u) - (Œº/2)‚Äñu-x*‚Äñ¬≤
        -- ‚àáh(u) = ‚àáf(u) - Œº(u-x*)
        simp only [h]
        rw [h_grad u, h_inner_expand, h_norm_expand]
        -- Now: f(v) - (Œº/2)(‚Äñv-u‚Äñ¬≤ + ‚Äñu-x*‚Äñ¬≤ + 2‚ü®u-x*,v-u‚ü©)
        --    ‚â§ f(u) - (Œº/2)‚Äñu-x*‚Äñ¬≤ + (‚ü®‚àáf(u),v-u‚ü© - Œº‚ü®u-x*,v-u‚ü©) + ((L-Œº)/2)‚Äñv-u‚Äñ¬≤
        -- Rearranging: f(v) ‚â§ f(u) + ‚ü®‚àáf(u),v-u‚ü© + ((L-Œº)/2 + Œº/2)‚Äñv-u‚Äñ¬≤ = f(u) + ‚ü®‚àáf(u),v-u‚ü© + (L/2)‚Äñv-u‚Äñ¬≤
        -- Which follows from hf_desc
        linarith

      -- Step 2: Apply descent at x: h(x - (1/(L-Œº))g') ‚â§ h(x) - (1/(2(L-Œº)))‚Äñg'‚Äñ¬≤
      have h_descent_x : h (x - (1 / (L - Œº)) ‚Ä¢ g') ‚â§ h x - (1 / (2 * (L - Œº))) * ‚Äñg'‚Äñ^2 := by
        have hd := h_descent x (x - (1 / (L - Œº)) ‚Ä¢ g')
        have h_diff_eq : (x - (1 / (L - Œº)) ‚Ä¢ g') - x = -((1 / (L - Œº)) ‚Ä¢ g') := by simp [sub_eq_add_neg, add_comm]
        have h_inner : @inner ‚Ñù E _ g' ((x - (1 / (L - Œº)) ‚Ä¢ g') - x) = -(1 / (L - Œº)) * ‚Äñg'‚Äñ^2 := by
          rw [h_diff_eq]
          simp only [inner_neg_right, inner_smul_right, real_inner_self_eq_norm_sq]
          ring
        have h_norm : ‚Äñ(x - (1 / (L - Œº)) ‚Ä¢ g') - x‚Äñ^2 = (1 / (L - Œº))^2 * ‚Äñg'‚Äñ^2 := by
          rw [h_diff_eq, norm_neg, norm_smul, Real.norm_eq_abs, abs_of_pos (by positivity : 1/(L-Œº) > 0)]
          ring
        calc h (x - (1 / (L - Œº)) ‚Ä¢ g') ‚â§ h x + @inner ‚Ñù E _ g' ((x - (1 / (L - Œº)) ‚Ä¢ g') - x) +
                             ((L - Œº) / 2) * ‚Äñ(x - (1 / (L - Œº)) ‚Ä¢ g') - x‚Äñ^2 := hd
          _ = h x + (-(1 / (L - Œº)) * ‚Äñg'‚Äñ^2) + ((L - Œº) / 2) * ((1 / (L - Œº))^2 * ‚Äñg'‚Äñ^2) := by
              rw [h_inner, h_norm]
          _ = h x - (1 / (2 * (L - Œº))) * ‚Äñg'‚Äñ^2 := by field_simp; ring

      -- Bound A: (1/(2(L-Œº)))‚Äñg'‚Äñ¬≤ ‚â§ h(x) - h(x*)
      have h_bound_A : (1 / (2 * (L - Œº))) * ‚Äñg'‚Äñ^2 ‚â§ h x - h x_star := by
        have := h_xstar_min (x - (1 / (L - Œº)) ‚Ä¢ g')
        linarith

      -- Step 3: Apply descent at x*: h(x* + (1/(L-Œº))g') ‚â§ h(x*) + (1/(2(L-Œº)))‚Äñg'‚Äñ¬≤
      have h_descent_xstar : h (x_star + (1 / (L - Œº)) ‚Ä¢ g') ‚â§ h x_star + (1 / (2 * (L - Œº))) * ‚Äñg'‚Äñ^2 := by
        have hd := h_descent x_star (x_star + (1 / (L - Œº)) ‚Ä¢ g')
        have h_diff_eq : (x_star + (1 / (L - Œº)) ‚Ä¢ g') - x_star = (1 / (L - Œº)) ‚Ä¢ g' := by abel
        have h_inner : @inner ‚Ñù E _ (gradient h x_star) ((x_star + (1 / (L - Œº)) ‚Ä¢ g') - x_star) = 0 := by
          rw [h_grad_xstar, inner_zero_left]
        have h_norm : ‚Äñ(x_star + (1 / (L - Œº)) ‚Ä¢ g') - x_star‚Äñ^2 = (1 / (L - Œº))^2 * ‚Äñg'‚Äñ^2 := by
          rw [h_diff_eq, norm_smul, Real.norm_eq_abs, abs_of_pos (by positivity : 1/(L-Œº) > 0)]
          ring
        calc h (x_star + (1 / (L - Œº)) ‚Ä¢ g') ‚â§ h x_star + @inner ‚Ñù E _ (gradient h x_star)
              ((x_star + (1 / (L - Œº)) ‚Ä¢ g') - x_star) + ((L - Œº) / 2) * ‚Äñ(x_star + (1 / (L - Œº)) ‚Ä¢ g') - x_star‚Äñ^2 := hd
          _ = h x_star + 0 + ((L - Œº) / 2) * ((1 / (L - Œº))^2 * ‚Äñg'‚Äñ^2) := by rw [h_inner, h_norm]
          _ = h x_star + (1 / (2 * (L - Œº))) * ‚Äñg'‚Äñ^2 := by field_simp; ring

      -- Step 4: Tilted function œÜ(z) = h(z) - ‚ü®g', z‚ü© is convex
      have œÜ_convex : ConvexOn ‚Ñù Set.univ (fun z => h z - @inner ‚Ñù E _ g' z) := by
        have h_linear_concave : ConcaveOn ‚Ñù Set.univ (fun z => @inner ‚Ñù E _ g' z) := by
          constructor
          ¬∑ exact convex_univ
          ¬∑ intro z _ w _ a b ha hb hab
            simp only [inner_add_right, inner_smul_right, smul_eq_mul]
            linarith
        exact h_convex.sub h_linear_concave

      -- ‚àáœÜ(x) = 0
      have œÜ_grad_x : gradient (fun z => h z - @inner ‚Ñù E _ g' z) x = 0 := by
        have hh_diff : DifferentiableAt ‚Ñù h x := h_diff x
        have hg'_diff : DifferentiableAt ‚Ñù (fun z => @inner ‚Ñù E _ g' z) x :=
          (innerSL (ùïú := ‚Ñù) g').differentiableAt
        have hg'_grad : HasGradientAt (fun z => @inner ‚Ñù E _ g' z) g' x := by
          rw [hasGradientAt_iff_hasFDerivAt]
          have h1 := (innerSL (ùïú := ‚Ñù) g').hasFDerivAt (x := x)
          simp only [InnerProductSpace.toDual] at h1 ‚ä¢
          convert h1 using 1
        have hh_grad : HasGradientAt h g' x := hh_diff.hasGradientAt
        have h_sub : HasGradientAt (fun z => h z - @inner ‚Ñù E _ g' z) (g' - g') x := by
          have h1 := hasGradientAt_iff_hasFDerivAt.mp hh_grad
          have h2 := hasGradientAt_iff_hasFDerivAt.mp hg'_grad
          have h3 := h1.sub h2
          rw [hasGradientAt_iff_hasFDerivAt]
          convert h3 using 1
          simp only [map_sub]
        rw [sub_self] at h_sub
        exact h_sub.gradient

      -- œÜ is differentiable
      have œÜ_diff : Differentiable ‚Ñù (fun z => h z - @inner ‚Ñù E _ g' z) := by
        intro z
        exact (h_diff z).sub (innerSL (ùïú := ‚Ñù) g').differentiableAt

      -- x minimizes œÜ via first-order optimality
      have h_x_min_œÜ : ‚àÄ y, (h x - @inner ‚Ñù E _ g' x) ‚â§ (h y - @inner ‚Ñù E _ g' y) :=
        convex_first_order_optimality (fun z => h z - @inner ‚Ñù E _ g' z) œÜ_convex œÜ_diff x œÜ_grad_x

      -- œÜ(x) ‚â§ œÜ(x* + (1/(L-Œº))g')
      have h_œÜx_le := h_x_min_œÜ (x_star + (1 / (L - Œº)) ‚Ä¢ g')

      -- Expand ‚ü®g', x* + (1/(L-Œº))g'‚ü©
      have h_inner_xstar'_g' : @inner ‚Ñù E _ g' (x_star + (1 / (L - Œº)) ‚Ä¢ g') =
          @inner ‚Ñù E _ g' x_star + (1 / (L - Œº)) * ‚Äñg'‚Äñ^2 := by
        simp only [inner_add_right, inner_smul_right, real_inner_self_eq_norm_sq]

      -- Bound B: (1/(2(L-Œº)))‚Äñg'‚Äñ¬≤ ‚â§ h(x*) - h(x) + ‚ü®g', x - x*‚ü©
      have h_bound_B : (1 / (2 * (L - Œº))) * ‚Äñg'‚Äñ^2 ‚â§ h x_star - h x + @inner ‚Ñù E _ g' (x - x_star) := by
        have h4 : @inner ‚Ñù E _ g' (x - x_star) = @inner ‚Ñù E _ g' x - @inner ‚Ñù E _ g' x_star :=
          inner_sub_right g' x x_star
        have step1' : h x - @inner ‚Ñù E _ g' x ‚â§ h (x_star + (1 / (L - Œº)) ‚Ä¢ g') -
            (@inner ‚Ñù E _ g' x_star + (1 / (L - Œº)) * ‚Äñg'‚Äñ^2) := by
          rw [‚Üê h_inner_xstar'_g']
          exact h_œÜx_le
        have step2' : h (x_star + (1 / (L - Œº)) ‚Ä¢ g') - (@inner ‚Ñù E _ g' x_star + (1 / (L - Œº)) * ‚Äñg'‚Äñ^2) ‚â§
            h x_star + (1 / (2 * (L - Œº))) * ‚Äñg'‚Äñ^2 - (@inner ‚Ñù E _ g' x_star + (1 / (L - Œº)) * ‚Äñg'‚Äñ^2) := by
          linarith [h_descent_xstar]
        have step3' : h x - @inner ‚Ñù E _ g' x ‚â§
            h x_star - @inner ‚Ñù E _ g' x_star - (1 / (2 * (L - Œº))) * ‚Äñg'‚Äñ^2 := by
          have := le_trans step1' step2'
          -- Simplify RHS of step2': h(x*) + (1/(2(L-Œº)))‚Äñg'‚Äñ¬≤ - (‚ü®g', x*‚ü© + (1/(L-Œº))‚Äñg'‚Äñ¬≤)
          -- = h(x*) - ‚ü®g', x*‚ü© + (1/(2(L-Œº)) - 1/(L-Œº))‚Äñg'‚Äñ¬≤
          -- = h(x*) - ‚ü®g', x*‚ü© - (1/(2(L-Œº)))‚Äñg'‚Äñ¬≤
          have h_rhs_simp : h x_star + (1 / (2 * (L - Œº))) * ‚Äñg'‚Äñ^2 -
              (@inner ‚Ñù E _ g' x_star + (1 / (L - Œº)) * ‚Äñg'‚Äñ^2) =
              h x_star - @inner ‚Ñù E _ g' x_star - (1 / (2 * (L - Œº))) * ‚Äñg'‚Äñ^2 := by
            have hne : L - Œº ‚â† 0 := ne_of_gt hL_sub_Œº_pos
            field_simp
            ring
          linarith [this, h_rhs_simp]
        have step4 : (1 / (2 * (L - Œº))) * ‚Äñg'‚Äñ^2 ‚â§ h x_star - h x + @inner ‚Ñù E _ g' x - @inner ‚Ñù E _ g' x_star := by
          linarith
        linarith

      -- Add bounds A and B: (1/(L-Œº))‚Äñg'‚Äñ¬≤ ‚â§ ‚ü®g', x - x*‚ü©
      have h_combined : (1 / (L - Œº)) * ‚Äñg'‚Äñ^2 ‚â§ @inner ‚Ñù E _ g' (x - x_star) := by
        have h_add := add_le_add h_bound_A h_bound_B
        have lhs_eq : (1 / (2 * (L - Œº))) * ‚Äñg'‚Äñ^2 + (1 / (2 * (L - Œº))) * ‚Äñg'‚Äñ^2 = (1 / (L - Œº)) * ‚Äñg'‚Äñ^2 := by field_simp; ring
        have rhs_eq : (h x - h x_star) + (h x_star - h x + @inner ‚Ñù E _ g' (x - x_star)) =
            @inner ‚Ñù E _ g' (x - x_star) := by ring
        linarith

      -- Multiply by (L-Œº): ‚Äñg'‚Äñ¬≤ ‚â§ (L-Œº)‚ü®g', x - x*‚ü©
      have h_cocoer_h : ‚Äñg'‚Äñ^2 ‚â§ (L - Œº) * @inner ‚Ñù E _ g' (x - x_star) := by
        calc ‚Äñg'‚Äñ^2 = (L - Œº) * ((1 / (L - Œº)) * ‚Äñg'‚Äñ^2) := by field_simp
          _ ‚â§ (L - Œº) * @inner ‚Ñù E _ g' (x - x_star) := by
              apply mul_le_mul_of_nonneg_left h_combined (le_of_lt hL_sub_Œº_pos)

      -- Convert g' = ‚àáh(x) = g - Œºd to the target form
      simp only [g'] at h_cocoer_h
      rw [h_grad x] at h_cocoer_h
      simp only [g, d] at h_cocoer_h ‚ä¢
      exact h_cocoer_h

  -- Expand LHS: ‚Äñg - Œºd‚Äñ¬≤ = ‚Äñg‚Äñ¬≤ - 2Œº‚ü®g,d‚ü© + Œº¬≤‚Äñd‚Äñ¬≤
  have h_expand_lhs : ‚Äñg - Œº ‚Ä¢ d‚Äñ^2 = ‚Äñg‚Äñ^2 - 2 * Œº * @inner ‚Ñù E _ g d + Œº^2 * ‚Äñd‚Äñ^2 := by
    rw [sub_eq_add_neg, norm_add_sq_real]
    simp only [norm_neg, inner_neg_right, norm_smul, Real.norm_eq_abs, abs_of_pos hŒº,
               inner_smul_right, real_inner_self_eq_norm_sq]
    ring

  -- Expand RHS: (L-Œº)‚ü®g - Œºd, d‚ü© = (L-Œº)‚ü®g,d‚ü© - (L-Œº)Œº‚Äñd‚Äñ¬≤
  have h_expand_rhs : (L - Œº) * @inner ‚Ñù E _ (g - Œº ‚Ä¢ d) d =
      (L - Œº) * @inner ‚Ñù E _ g d - (L - Œº) * Œº * ‚Äñd‚Äñ^2 := by
    rw [inner_sub_left, inner_smul_left]
    simp only [real_inner_self_eq_norm_sq, conj_trivial]
    ring

  -- From h_aux_cocoer: ‚Äñg‚Äñ¬≤ - 2Œº‚ü®g,d‚ü© + Œº¬≤‚Äñd‚Äñ¬≤ ‚â§ (L-Œº)‚ü®g,d‚ü© - (L-Œº)Œº‚Äñd‚Äñ¬≤
  have h_ineq : ‚Äñg‚Äñ^2 - 2 * Œº * @inner ‚Ñù E _ g d + Œº^2 * ‚Äñd‚Äñ^2 ‚â§
      (L - Œº) * @inner ‚Ñù E _ g d - (L - Œº) * Œº * ‚Äñd‚Äñ^2 := by
    rw [‚Üê h_expand_lhs, ‚Üê h_expand_rhs]
    exact h_aux_cocoer

  -- Rearrange: ‚Äñg‚Äñ¬≤ + ŒºL‚Äñd‚Äñ¬≤ ‚â§ (L+Œº)‚ü®g,d‚ü©
  have h_rearrange : ‚Äñg‚Äñ^2 + Œº * L * ‚Äñd‚Äñ^2 ‚â§ (L + Œº) * @inner ‚Ñù E _ g d := by
    -- From h_ineq: ‚Äñg‚Äñ¬≤ - 2Œº‚ü®g,d‚ü© + Œº¬≤‚Äñd‚Äñ¬≤ ‚â§ (L-Œº)‚ü®g,d‚ü© - (L-Œº)Œº‚Äñd‚Äñ¬≤
    -- Add 2Œº‚ü®g,d‚ü© to both sides:
    -- ‚Äñg‚Äñ¬≤ + Œº¬≤‚Äñd‚Äñ¬≤ ‚â§ (L-Œº+2Œº)‚ü®g,d‚ü© - (L-Œº)Œº‚Äñd‚Äñ¬≤
    -- ‚Äñg‚Äñ¬≤ + Œº¬≤‚Äñd‚Äñ¬≤ ‚â§ (L+Œº)‚ü®g,d‚ü© - (LŒº - Œº¬≤)‚Äñd‚Äñ¬≤
    -- ‚Äñg‚Äñ¬≤ + Œº¬≤‚Äñd‚Äñ¬≤ + LŒº‚Äñd‚Äñ¬≤ - Œº¬≤‚Äñd‚Äñ¬≤ ‚â§ (L+Œº)‚ü®g,d‚ü©
    -- ‚Äñg‚Äñ¬≤ + LŒº‚Äñd‚Äñ¬≤ ‚â§ (L+Œº)‚ü®g,d‚ü©
    linarith

  -- Divide by (L+Œº): ‚ü®g,d‚ü© ‚â• (ŒºL)/(Œº+L)‚Äñd‚Äñ¬≤ + 1/(Œº+L)‚Äñg‚Äñ¬≤
  have h_final : @inner ‚Ñù E _ g d ‚â• (Œº * L) / (Œº + L) * ‚Äñd‚Äñ^2 + 1 / (Œº + L) * ‚Äñg‚Äñ^2 := by
    have h1 : (Œº + L) * @inner ‚Ñù E _ g d ‚â• Œº * L * ‚Äñd‚Äñ^2 + ‚Äñg‚Äñ^2 := by linarith
    have h2 : (Œº * L) / (Œº + L) * ‚Äñd‚Äñ^2 + 1 / (Œº + L) * ‚Äñg‚Äñ^2 =
        (Œº * L * ‚Äñd‚Äñ^2 + ‚Äñg‚Äñ^2) / (Œº + L) := by field_simp
    rw [h2, ge_iff_le]
    -- (ŒºL¬∑‚Äñd‚Äñ¬≤ + ‚Äñg‚Äñ¬≤)/(Œº+L) ‚â§ ‚ü®g,d‚ü© iff ŒºL¬∑‚Äñd‚Äñ¬≤ + ‚Äñg‚Äñ¬≤ ‚â§ (Œº+L)¬∑‚ü®g,d‚ü©
    have h_ne : Œº + L ‚â† 0 := ne_of_gt h_sum_pos
    have h3 : (Œº * L * ‚Äñd‚Äñ^2 + ‚Äñg‚Äñ^2) / (Œº + L) * (Œº + L) = Œº * L * ‚Äñd‚Äñ^2 + ‚Äñg‚Äñ^2 := by
      field_simp
    have h4 : (Œº * L * ‚Äñd‚Äñ^2 + ‚Äñg‚Äñ^2) / (Œº + L) ‚â§ @inner ‚Ñù E _ g d ‚Üî
        (Œº * L * ‚Äñd‚Äñ^2 + ‚Äñg‚Äñ^2) / (Œº + L) * (Œº + L) ‚â§ @inner ‚Ñù E _ g d * (Œº + L) := by
      constructor
      ¬∑ intro h
        apply mul_le_mul_of_nonneg_right h (le_of_lt h_sum_pos)
      ¬∑ intro h
        have h5 : 0 < 1 / (Œº + L) := by positivity
        have := mul_le_mul_of_nonneg_right h (le_of_lt h5)
        simp only [mul_assoc] at this
        have h6 : (Œº + L) * (1 / (Œº + L)) = 1 := by field_simp
        simp only [h6, mul_one] at this
        have h7 : Œº * (L * ‚Äñd‚Äñ^2) = Œº * L * ‚Äñd‚Äñ^2 := by ring
        simp only [h7] at this
        exact this
    rw [h4, h3]
    have h5 : @inner ‚Ñù E _ g d * (Œº + L) = (Œº + L) * @inner ‚Ñù E _ g d := by ring
    rw [h5]
    linarith

  convert h_final using 2 <;> simp only [g, d]

/-- Interpolation condition for smooth convex functions.

    For L-smooth convex f with minimizer x* (where ‚àáf(x*) = 0):
    f(x) - f(x*) + (1/2L)‚Äñ‚àáf(x)‚Äñ¬≤ ‚â§ ‚ü®‚àáf(x), x - x*‚ü©

    Equivalently: f(x) - f(x*) ‚â§ ‚ü®‚àáf(x), x - x*‚ü© - (1/2L)‚Äñ‚àáf(x)‚Äñ¬≤

    This is tighter than plain convexity and is key for the O(1/k) convergence proof.
-/
lemma smooth_convex_interpolation (f : E ‚Üí ‚Ñù) (L : ‚Ñù) (hL : 0 < L)
    (hSmooth : IsLSmooth f L) (hConvex : ConvexOn ‚Ñù Set.univ f)
    (x x_star : E) (hMin : gradient f x_star = 0) :
    f x - f x_star + (1 / (2 * L)) * ‚Äñgradient f x‚Äñ^2 ‚â§
      @inner ‚Ñù E _ (gradient f x) (x - x_star) := by
  -- This follows from combining bounds proved in lsmooth_cocoercivity:
  -- Bound A: (1/2L)‚Äñg‚Äñ¬≤ ‚â§ f(x) - f(x*)   (from minimality of x*)
  -- Bound B: (1/2L)‚Äñg‚Äñ¬≤ ‚â§ f(x*) - f(x) + ‚ü®g, x - x*‚ü©   (from tilted function technique)
  -- Adding A to the rearranged form of B gives the result.
  have hDiff : Differentiable ‚Ñù f := hSmooth.1
  let g := gradient f x

  -- Step 1: x* minimizes f (since ‚àáf(x*) = 0 and f is convex)
  have h_xstar_min : ‚àÄ y, f x_star ‚â§ f y := convex_first_order_optimality f hConvex hDiff x_star hMin

  -- Step 2: Bound A - from descent lemma and minimality
  -- f(x - (1/L)‚àáf(x)) ‚â§ f(x) - (1/2L)‚Äñ‚àáf(x)‚Äñ¬≤
  -- f(x*) ‚â§ f(x - (1/L)‚àáf(x))
  -- Therefore: (1/2L)‚Äñ‚àáf(x)‚Äñ¬≤ ‚â§ f(x) - f(x*)
  have h_fund := lsmooth_fundamental_ineq f L (le_of_lt hL) hSmooth x (x - (1 / L) ‚Ä¢ g)
  have h_descent : f (x - (1 / L) ‚Ä¢ g) ‚â§ f x - (1 / (2 * L)) * ‚Äñg‚Äñ^2 := by
    have h_diff : (x - (1 / L) ‚Ä¢ g) - x = -((1 / L) ‚Ä¢ g) := by simp [sub_eq_add_neg, add_comm]
    have h_inner : @inner ‚Ñù E _ g ((x - (1 / L) ‚Ä¢ g) - x) = -(1 / L) * ‚Äñg‚Äñ^2 := by
      rw [h_diff]
      simp only [inner_neg_right, inner_smul_right, real_inner_self_eq_norm_sq]
      ring
    have h_norm : ‚Äñ(x - (1 / L) ‚Ä¢ g) - x‚Äñ^2 = (1 / L)^2 * ‚Äñg‚Äñ^2 := by
      rw [h_diff, norm_neg, norm_smul, Real.norm_eq_abs, abs_of_pos (by positivity : 1/L > 0)]
      ring
    calc f (x - (1 / L) ‚Ä¢ g) ‚â§ f x + @inner ‚Ñù E _ g ((x - (1 / L) ‚Ä¢ g) - x) +
                                 (L / 2) * ‚Äñ(x - (1 / L) ‚Ä¢ g) - x‚Äñ^2 := h_fund
      _ = f x + (-(1 / L) * ‚Äñg‚Äñ^2) + (L / 2) * ((1 / L)^2 * ‚Äñg‚Äñ^2) := by rw [h_inner, h_norm]
      _ = f x - (1 / (2 * L)) * ‚Äñg‚Äñ^2 := by field_simp; ring
  have h_bound_A : (1 / (2 * L)) * ‚Äñg‚Äñ^2 ‚â§ f x - f x_star := by
    have := h_xstar_min (x - (1 / L) ‚Ä¢ g)
    linarith

  -- Step 3: Bound B - from tilted function technique
  -- The tilted function h(z) = f(z) - ‚ü®g, z‚ü© is convex and has minimum at x
  -- This gives: (1/2L)‚Äñg‚Äñ¬≤ ‚â§ f(x*) - f(x) + ‚ü®g, x - x*‚ü©
  have h_linear_concave : ConcaveOn ‚Ñù Set.univ (fun z => @inner ‚Ñù E _ g z) := by
    constructor
    ¬∑ exact convex_univ
    ¬∑ intro z _ w _ a b ha hb hab
      simp only [inner_add_right, inner_smul_right, smul_eq_mul]
      linarith
  have h_convex_tilt : ConvexOn ‚Ñù Set.univ (fun z => f z - @inner ‚Ñù E _ g z) :=
    hConvex.sub h_linear_concave
  have h_grad_tilt_x : gradient (fun z => f z - @inner ‚Ñù E _ g z) x = 0 := by
    have hf_diff : DifferentiableAt ‚Ñù f x := hDiff x
    have hg_diff : DifferentiableAt ‚Ñù (fun z => @inner ‚Ñù E _ g z) x :=
      (innerSL (ùïú := ‚Ñù) g).differentiableAt
    have hg_grad : HasGradientAt (fun z => @inner ‚Ñù E _ g z) g x := by
      rw [hasGradientAt_iff_hasFDerivAt]
      have h1 := (innerSL (ùïú := ‚Ñù) g).hasFDerivAt (x := x)
      simp only [InnerProductSpace.toDual] at h1 ‚ä¢
      convert h1 using 1
    have hf_grad : HasGradientAt f g x := hf_diff.hasGradientAt
    have h_sub : HasGradientAt (fun z => f z - @inner ‚Ñù E _ g z) (g - g) x := by
      have h1 := hasGradientAt_iff_hasFDerivAt.mp hf_grad
      have h2 := hasGradientAt_iff_hasFDerivAt.mp hg_grad
      have h3 := h1.sub h2
      rw [hasGradientAt_iff_hasFDerivAt]
      convert h3 using 1
      simp only [map_sub]
    rw [sub_self] at h_sub
    exact h_sub.gradient
  have h_diff_tilt : Differentiable ‚Ñù (fun z => f z - @inner ‚Ñù E _ g z) := by
    intro z
    exact (hDiff z).sub (innerSL (ùïú := ‚Ñù) g).differentiableAt
  have h_x_min_tilt : ‚àÄ y, (f x - @inner ‚Ñù E _ g x) ‚â§ (f y - @inner ‚Ñù E _ g y) :=
    convex_first_order_optimality (fun z => f z - @inner ‚Ñù E _ g z) h_convex_tilt h_diff_tilt x h_grad_tilt_x
  -- Apply at x* + (1/L)g
  have h_fund_xstar := lsmooth_fundamental_ineq f L (le_of_lt hL) hSmooth x_star (x_star + (1 / L) ‚Ä¢ g)
  have h_fund_xstar_bound : f (x_star + (1 / L) ‚Ä¢ g) ‚â§ f x_star + (1 / (2 * L)) * ‚Äñg‚Äñ^2 := by
    have h_diff_pt : (x_star + (1 / L) ‚Ä¢ g) - x_star = (1 / L) ‚Ä¢ g := by abel
    have h_inner_pt : @inner ‚Ñù E _ (gradient f x_star) ((x_star + (1 / L) ‚Ä¢ g) - x_star) = 0 := by
      rw [hMin, inner_zero_left]
    have h_norm_pt : ‚Äñ(x_star + (1 / L) ‚Ä¢ g) - x_star‚Äñ^2 = (1 / L)^2 * ‚Äñg‚Äñ^2 := by
      rw [h_diff_pt, norm_smul, Real.norm_eq_abs, abs_of_pos (by positivity : 1/L > 0)]
      ring
    calc f (x_star + (1 / L) ‚Ä¢ g) ‚â§ f x_star + @inner ‚Ñù E _ (gradient f x_star)
          ((x_star + (1 / L) ‚Ä¢ g) - x_star) + (L / 2) * ‚Äñ(x_star + (1 / L) ‚Ä¢ g) - x_star‚Äñ^2 := h_fund_xstar
      _ = f x_star + 0 + (L / 2) * ((1 / L)^2 * ‚Äñg‚Äñ^2) := by rw [h_inner_pt, h_norm_pt]
      _ = f x_star + (1 / (2 * L)) * ‚Äñg‚Äñ^2 := by field_simp; ring
  have h_inner_xstar_g : @inner ‚Ñù E _ g (x_star + (1 / L) ‚Ä¢ g) =
      @inner ‚Ñù E _ g x_star + (1 / L) * ‚Äñg‚Äñ^2 := by
    simp only [inner_add_right, inner_smul_right, real_inner_self_eq_norm_sq]
  have h_tilt_bound := h_x_min_tilt (x_star + (1 / L) ‚Ä¢ g)
  have h_bound_B : (1 / (2 * L)) * ‚Äñg‚Äñ^2 ‚â§ f x_star - f x + @inner ‚Ñù E _ g (x - x_star) := by
    have step1 : f x - @inner ‚Ñù E _ g x ‚â§ f (x_star + (1 / L) ‚Ä¢ g) -
        @inner ‚Ñù E _ g (x_star + (1 / L) ‚Ä¢ g) := h_tilt_bound
    have step2 : f (x_star + (1 / L) ‚Ä¢ g) - @inner ‚Ñù E _ g (x_star + (1 / L) ‚Ä¢ g) ‚â§
        (f x_star + (1 / (2 * L)) * ‚Äñg‚Äñ^2) - (@inner ‚Ñù E _ g x_star + (1 / L) * ‚Äñg‚Äñ^2) := by
      rw [h_inner_xstar_g]
      linarith [h_fund_xstar_bound]
    have step3 : (f x_star + (1 / (2 * L)) * ‚Äñg‚Äñ^2) - (@inner ‚Ñù E _ g x_star + (1 / L) * ‚Äñg‚Äñ^2) =
        f x_star - @inner ‚Ñù E _ g x_star - (1 / (2 * L)) * ‚Äñg‚Äñ^2 := by field_simp; ring
    have step4 : f x - @inner ‚Ñù E _ g x ‚â§ f x_star - @inner ‚Ñù E _ g x_star - (1 / (2 * L)) * ‚Äñg‚Äñ^2 := by
      linarith
    have h4 : @inner ‚Ñù E _ g (x - x_star) = @inner ‚Ñù E _ g x - @inner ‚Ñù E _ g x_star :=
      inner_sub_right g x x_star
    linarith

  -- Step 4: Combine bounds to get the interpolation inequality
  -- From h_bound_B: (1/2L)‚Äñg‚Äñ¬≤ ‚â§ f(x*) - f(x) + ‚ü®g, x - x*‚ü©
  -- Rearranging: f(x) - f(x*) + (1/2L)‚Äñg‚Äñ¬≤ ‚â§ ‚ü®g, x - x*‚ü©
  linarith

/-- One step of gradient descent with learning rate Œ∑. -/
noncomputable def gradientDescentStep (f : E ‚Üí ‚Ñù) (Œ∑ : ‚Ñù) (x : E) : E :=
  x - Œ∑ ‚Ä¢ gradient f x

/-- k steps of gradient descent. -/
noncomputable def gradientDescentIterates (f : E ‚Üí ‚Ñù) (Œ∑ : ‚Ñù) (x‚ÇÄ : E) : ‚Ñï ‚Üí E
  | 0 => x‚ÇÄ
  | n + 1 => gradientDescentStep f Œ∑ (gradientDescentIterates f Œ∑ x‚ÇÄ n)

/-- The descent lemma: one step decreases function value.

The proof follows from L-smoothness:
1. By L-smoothness: f(y) ‚â§ f(x) + ‚ü®‚àáf(x), y-x‚ü© + (L/2)‚Äñy-x‚Äñ¬≤
2. With y = x - Œ∑‚àáf(x), we have y - x = -Œ∑‚àáf(x)
3. So ‚ü®‚àáf(x), y-x‚ü© = -Œ∑‚Äñ‚àáf(x)‚Äñ¬≤
4. And ‚Äñy-x‚Äñ¬≤ = Œ∑¬≤‚Äñ‚àáf(x)‚Äñ¬≤
5. Thus: f(y) ‚â§ f(x) - Œ∑‚Äñ‚àáf(x)‚Äñ¬≤ + (LŒ∑¬≤/2)‚Äñ‚àáf(x)‚Äñ¬≤
6. Since Œ∑ ‚â§ 1/L, we have (LŒ∑¬≤/2) ‚â§ Œ∑/2
7. Therefore: f(y) ‚â§ f(x) - (Œ∑/2)‚Äñ‚àáf(x)‚Äñ¬≤

The key insight is that L-smoothness provides a second-order bound on function values,
which allows us to show descent over a single gradient step.
-/
theorem descent_lemma (f : E ‚Üí ‚Ñù) (L : ‚Ñù) (hL : 0 < L)
    (hSmooth : IsLSmooth f L) (x : E) (Œ∑ : ‚Ñù) (hŒ∑ : 0 < Œ∑) (hŒ∑L : Œ∑ ‚â§ 1 / L) :
    f (gradientDescentStep f Œ∑ x) ‚â§ f x - (Œ∑ / 2) * ‚Äñgradient f x‚Äñ^2 := by
  -- Define y = x - Œ∑‚àáf(x) (the gradient descent step)
  let y := x - Œ∑ ‚Ä¢ gradient f x
  let g := gradient f x
  -- Step 1: Apply the fundamental inequality for L-smooth functions
  have h_fund := lsmooth_fundamental_ineq f L (le_of_lt hL) hSmooth x y
  -- Step 2: Compute y - x = -(Œ∑ ‚Ä¢ ‚àáf(x))
  have h_diff : y - x = -(Œ∑ ‚Ä¢ g) := by simp only [y, g]; abel
  -- Step 3: Compute ‚ü®‚àáf(x), y - x‚ü© = -Œ∑‚Äñ‚àáf(x)‚Äñ¬≤
  have h_inner : @inner ‚Ñù E _ g (y - x) = -Œ∑ * ‚Äñg‚Äñ^2 := by
    rw [h_diff, inner_neg_right, inner_smul_right]
    rw [real_inner_self_eq_norm_sq]
    ring
  -- Step 4: Compute ‚Äñy - x‚Äñ¬≤ = Œ∑¬≤‚Äñ‚àáf(x)‚Äñ¬≤
  have h_norm_sq : ‚Äñy - x‚Äñ^2 = Œ∑^2 * ‚Äñg‚Äñ^2 := by
    rw [h_diff, norm_neg, norm_smul, Real.norm_eq_abs]
    have : |Œ∑|^2 = Œ∑^2 := sq_abs Œ∑
    rw [mul_pow, this]
  -- Step 5: Substitute into the fundamental inequality
  -- f(y) ‚â§ f(x) + ‚ü®‚àáf(x), y - x‚ü© + (L/2)‚Äñy - x‚Äñ¬≤
  --      = f(x) - Œ∑‚Äñ‚àáf(x)‚Äñ¬≤ + (L/2)Œ∑¬≤‚Äñ‚àáf(x)‚Äñ¬≤
  --      = f(x) + (-Œ∑ + LŒ∑¬≤/2)‚Äñ‚àáf(x)‚Äñ¬≤
  calc f y ‚â§ f x + @inner ‚Ñù E _ g (y - x) + (L / 2) * ‚Äñy - x‚Äñ^2 := h_fund
    _ = f x + (-Œ∑ * ‚Äñg‚Äñ^2) + (L / 2) * (Œ∑^2 * ‚Äñg‚Äñ^2) := by rw [h_inner, h_norm_sq]
    _ = f x + (-Œ∑ + L * Œ∑^2 / 2) * ‚Äñg‚Äñ^2 := by ring
    _ ‚â§ f x + (-Œ∑ / 2) * ‚Äñg‚Äñ^2 := by {
        -- Need: -Œ∑ + L*Œ∑¬≤/2 ‚â§ -Œ∑/2
        -- i.e., L*Œ∑¬≤/2 ‚â§ Œ∑/2
        -- i.e., L*Œ∑ ‚â§ 1
        -- which follows from Œ∑ ‚â§ 1/L
        have h_LŒ∑ : L * Œ∑ ‚â§ 1 := by
          calc L * Œ∑ = Œ∑ * L := mul_comm L Œ∑
            _ ‚â§ (1 / L) * L := mul_le_mul_of_nonneg_right hŒ∑L (le_of_lt hL)
            _ = 1 := div_mul_cancel‚ÇÄ 1 (ne_of_gt hL)
        have h_coeff : -Œ∑ + L * Œ∑^2 / 2 ‚â§ -Œ∑ / 2 := by
          have h1 : L * Œ∑^2 / 2 ‚â§ Œ∑ / 2 := by
            have : L * Œ∑^2 ‚â§ Œ∑ := by
              calc L * Œ∑^2 = (L * Œ∑) * Œ∑ := by ring
                _ ‚â§ 1 * Œ∑ := mul_le_mul_of_nonneg_right h_LŒ∑ (le_of_lt hŒ∑)
                _ = Œ∑ := one_mul Œ∑
            linarith
          linarith
        have h_g_sq_nonneg : 0 ‚â§ ‚Äñg‚Äñ^2 := sq_nonneg _
        nlinarith [sq_nonneg ‚Äñg‚Äñ]
      }
    _ = f x - (Œ∑ / 2) * ‚Äñg‚Äñ^2 := by ring

/-- Convergence rate for smooth convex functions.
    After k iterations: f(x_k) - f(x*) ‚â§ ‚Äñx‚ÇÄ - x*‚Äñ¬≤ / (2Œ∑k) -/
theorem convex_convergence_rate (f : E ‚Üí ‚Ñù) (L : ‚Ñù) (hL : 0 < L)
    (hSmooth : IsLSmooth f L) (hConvex : ConvexOn ‚Ñù Set.univ f)
    (x_star : E) (hMin : ‚àÄ x, f x_star ‚â§ f x)
    (Œ∑ : ‚Ñù) (hŒ∑ : 0 < Œ∑) (hŒ∑L : Œ∑ ‚â§ 1 / L) (x‚ÇÄ : E) :
    ‚àÄ k : ‚Ñï, k > 0 ‚Üí
      f (gradientDescentIterates f Œ∑ x‚ÇÄ k) - f x_star ‚â§ ‚Äñx‚ÇÄ - x_star‚Äñ^2 / (2 * Œ∑ * k) := by
  intro k hk

  /- Convergence Proof via Telescoping Descent Lemma

  For smooth convex functions, we prove O(1/k) convergence by combining:

  1. **Descent Lemma (L-smoothness)**:
     f(x_{i+1}) ‚â§ f(x_i) - (Œ∑/2)‚Äñ‚àáf(x_i)‚Äñ¬≤

  2. **First-Order Convexity**:
     For convex f: f(x) - f(x*) ‚â§ ‚ü®‚àáf(x), x - x*‚ü©

  3. **Telescoping Sum**:
     Sum descent inequalities over i = 0, ..., k-1:
     f(x_k) - f(x_0) ‚â§ -(Œ∑/2) ‚àë·µ¢ ‚Äñ‚àáf(x_i)‚Äñ¬≤

  4. **Cauchy-Schwarz Lower Bound on Gradient Norms**:
     From convexity: ‚Äñ‚àáf(x_i)‚Äñ¬≤ ‚â• 2(f(x_i) - f(x*))¬≤ / ‚Äñx_i - x*‚Äñ¬≤

     However, this requires bounded domain assumptions that conflict with the
     general statement. The standard proof instead uses:

     ‚ü®‚àáf(x), x - x*‚ü© ‚â• f(x) - f(x*) (convexity)

     Which combined with ‚Äñ‚àáf(x)‚Äñ ¬∑ ‚Äñx - x*‚Äñ ‚â• |‚ü®‚àáf(x), x - x*‚ü©| gives:
     ‚Äñ‚àáf(x)‚Äñ ‚â• (f(x) - f(x*)) / ‚Äñx - x*‚Äñ

  5. **Proof Dependencies**:
     - `lsmooth_fundamental_ineq` (COMPLETE)
     - `descent_lemma` (COMPLETE, uses lsmooth_fundamental_ineq)
     - First-order convexity characterization from Mathlib's ConvexOn
     - Telescoping sum machinery
  -/

  -- The proof uses the key identity for distance to optimum:
  -- ‚Äñx_{k+1} - x*‚Äñ¬≤ = ‚Äñx_k - Œ∑‚àáf(x_k) - x*‚Äñ¬≤
  --                 = ‚Äñx_k - x*‚Äñ¬≤ - 2Œ∑‚ü®‚àáf(x_k), x_k - x*‚ü© + Œ∑¬≤‚Äñ‚àáf(x_k)‚Äñ¬≤
  --
  -- From convexity: f(x_k) - f(x*) ‚â§ ‚ü®‚àáf(x_k), x_k - x*‚ü©
  -- So: 2Œ∑(f(x_k) - f(x*)) ‚â§ 2Œ∑‚ü®‚àáf(x_k), x_k - x*‚ü©
  --
  -- Rearranging the distance identity:
  -- 2Œ∑(f(x_k) - f(x*)) ‚â§ ‚Äñx_k - x*‚Äñ¬≤ - ‚Äñx_{k+1} - x*‚Äñ¬≤ + Œ∑¬≤‚Äñ‚àáf(x_k)‚Äñ¬≤
  --
  -- Summing over i = 0 to k-1:
  -- 2Œ∑‚àë(f(x_i) - f(x*)) ‚â§ ‚Äñx_0 - x*‚Äñ¬≤ - ‚Äñx_k - x*‚Äñ¬≤ + Œ∑¬≤‚àë‚Äñ‚àáf(x_i)‚Äñ¬≤
  --
  -- From descent_lemma: f(x_{i+1}) ‚â§ f(x_i) - (Œ∑/2)‚Äñ‚àáf(x_i)‚Äñ¬≤
  -- Telescoping: f(x_k) - f(x_0) ‚â§ -(Œ∑/2)‚àë‚Äñ‚àáf(x_i)‚Äñ¬≤
  -- So: (Œ∑/2)‚àë‚Äñ‚àáf(x_i)‚Äñ¬≤ ‚â§ f(x_0) - f(x_k) ‚â§ f(x_0) - f(x*)
  -- Hence: Œ∑¬≤‚àë‚Äñ‚àáf(x_i)‚Äñ¬≤ ‚â§ 2Œ∑(f(x_0) - f(x*))
  --
  -- Substituting:
  -- 2Œ∑‚àë(f(x_i) - f(x*)) ‚â§ ‚Äñx_0 - x*‚Äñ¬≤ + 2Œ∑(f(x_0) - f(x*))
  --
  -- Since f(x_k) - f(x*) ‚â§ (1/k)‚àë(f(x_i) - f(x*)) (minimum ‚â§ average):
  -- 2Œ∑k(f(x_k) - f(x*)) ‚â§ ‚Äñx_0 - x*‚Äñ¬≤ + 2Œ∑(f(x_0) - f(x*))
  --
  -- Using the smooth convex interpolation lemma gives the factor of 2.

  have hDiff : Differentiable ‚Ñù f := hSmooth.1

  -- Step 1: Derive ‚àáf(x*) = 0 from the fact that x* is a global minimizer
  -- For differentiable f, if x* minimizes f, then ‚àáf(x*) = 0
  have hGradZero : gradient f x_star = 0 := by
    by_contra h
    -- If ‚àáf(x*) ‚â† 0, moving in direction -‚àáf(x*) decreases f, contradiction
    let g := gradient f x_star
    have hg_ne : g ‚â† 0 := h
    have h_norm_pos : ‚Äñg‚Äñ > 0 := norm_pos_iff.mpr hg_ne
    -- Consider f(x* - t¬∑g) for small t > 0
    -- Directional derivative at x* in direction -g is ‚ü®‚àáf(x*), -g‚ü© = -‚Äñg‚Äñ¬≤ < 0
    have hf_grad : HasGradientAt f g x_star := (hDiff x_star).hasGradientAt
    have hf_deriv : HasFDerivAt f (innerSL (ùïú := ‚Ñù) g) x_star := hf_grad.hasFDerivAt
    -- Use the definition of derivative with a smaller Œµ to ensure error is dominated
    -- We want: |f(x* - t¬∑g) - f(x*) - ‚ü®g, -t¬∑g‚ü©| ‚â§ (‚Äñg‚Äñ¬≤/2) ¬∑ |t| ¬∑ ‚Äñg‚Äñ
    -- i.e., |f(x* - t¬∑g) - f(x*) + t¬∑‚Äñg‚Äñ¬≤| ‚â§ (t¬∑‚Äñg‚Äñ¬≥)/2
    -- This gives: f(x* - t¬∑g) - f(x*) ‚â§ -t¬∑‚Äñg‚Äñ¬≤ + t¬∑‚Äñg‚Äñ¬≥/2 = -t¬∑‚Äñg‚Äñ¬≤¬∑(1 - ‚Äñg‚Äñ/2)
    -- For this to be negative, we need t > 0 and 1 - ‚Äñg‚Äñ/2 > 0, i.e., ‚Äñg‚Äñ < 2
    -- To handle ‚Äñg‚Äñ ‚â• 2, we use a smaller Œµ in the isLittleO bound
    rw [HasFDerivAt, hasFDerivAtFilter_iff_isLittleO, Asymptotics.isLittleO_iff] at hf_deriv
    -- Choose c = ‚Äñg‚Äñ/2 so the error bound becomes (‚Äñg‚Äñ/2) ¬∑ ‚Äñh‚Äñ ‚â§ (‚Äñg‚Äñ/2) ¬∑ t ¬∑ ‚Äñg‚Äñ
    specialize hf_deriv (by linarith : (0 : ‚Ñù) < ‚Äñg‚Äñ / 2)
    rw [Filter.eventually_iff_exists_mem] at hf_deriv
    obtain ‚ü®s, hs_mem, hs_bound‚ü© := hf_deriv
    rw [Metric.mem_nhds_iff] at hs_mem
    obtain ‚ü®Œµ, hŒµ_pos, hŒµ_ball‚ü© := hs_mem
    -- Choose t small enough that t¬∑g ‚àà ball and t < 1
    let t := min (Œµ / (2 * ‚Äñg‚Äñ)) (1 / 2)
    have ht_pos : t > 0 := by
      simp only [t, lt_min_iff]
      constructor
      ¬∑ positivity
      ¬∑ linarith
    have ht_le_half : t ‚â§ 1/2 := min_le_right _ _
    have h_tg_small : ‚Äñt ‚Ä¢ g‚Äñ < Œµ := by
      rw [norm_smul, Real.norm_eq_abs, abs_of_pos ht_pos]
      have h1 : t ‚â§ Œµ / (2 * ‚Äñg‚Äñ) := min_le_left _ _
      calc t * ‚Äñg‚Äñ ‚â§ (Œµ / (2 * ‚Äñg‚Äñ)) * ‚Äñg‚Äñ := by apply mul_le_mul_of_nonneg_right h1 (le_of_lt h_norm_pos)
        _ = Œµ / 2 := by field_simp
        _ < Œµ := by linarith
    have h_in_s : x_star - t ‚Ä¢ g ‚àà s := by
      apply hŒµ_ball
      simp only [Metric.mem_ball, dist_eq_norm]
      have h_eq : (x_star - t ‚Ä¢ g) - x_star = -(t ‚Ä¢ g) := by abel
      rw [h_eq, norm_neg]
      exact h_tg_small
    have hs_bound' := hs_bound (x_star - t ‚Ä¢ g) h_in_s
    simp only [innerSL_apply_apply] at hs_bound'
    -- hs_bound: ‚Äñf(x* - t¬∑g) - f(x*) - ‚ü®g, (x* - t¬∑g) - x*‚ü©‚Äñ < (‚Äñg‚Äñ/2) * ‚Äñ(x* - t¬∑g) - x*‚Äñ
    have h_diff_eq : (x_star - t ‚Ä¢ g) - x_star = -(t ‚Ä¢ g) := by abel
    have h_inner : @inner ‚Ñù E _ g ((x_star - t ‚Ä¢ g) - x_star) = -t * ‚Äñg‚Äñ^2 := by
      rw [h_diff_eq, inner_neg_right, inner_smul_right, real_inner_self_eq_norm_sq]
      ring
    have h_norm_diff : ‚Äñ(x_star - t ‚Ä¢ g) - x_star‚Äñ = t * ‚Äñg‚Äñ := by
      rw [h_diff_eq, norm_neg, norm_smul, Real.norm_eq_abs, abs_of_pos ht_pos]
    rw [h_inner, h_norm_diff] at hs_bound'
    -- hs_bound': ‚Äñf(x* - t¬∑g) - f(x*) - (-t * ‚Äñg‚Äñ¬≤)‚Äñ < (‚Äñg‚Äñ/2) * (t * ‚Äñg‚Äñ)
    -- i.e., |f(x* - t¬∑g) - f(x*) + t¬∑‚Äñg‚Äñ¬≤| < t¬∑‚Äñg‚Äñ¬≤/2
    have h_rhs_eq : ‚Äñg‚Äñ / 2 * (t * ‚Äñg‚Äñ) = t * ‚Äñg‚Äñ^2 / 2 := by ring
    rw [h_rhs_eq] at hs_bound'
    -- From absolute value bound (note: isLittleO_iff gives ‚â§, not <):
    -- ‚Äñf(x* - t¬∑g) - f(x*) + t¬∑‚Äñg‚Äñ¬≤‚Äñ ‚â§ t¬∑‚Äñg‚Äñ¬≤/2
    have h_from_abs := abs_le.mp hs_bound'
    -- h_from_abs gives bounds on f(x* - t¬∑g) - f(x*) - (-t * ‚Äñg‚Äñ¬≤)
    -- which equals f(x* - t¬∑g) - f(x*) + t * ‚Äñg‚Äñ¬≤
    have h_upper : f (x_star - t ‚Ä¢ g) - f x_star + t * ‚Äñg‚Äñ^2 ‚â§ t * ‚Äñg‚Äñ^2 / 2 := by
      have h := h_from_abs.2
      -- h : f (x_star - t ‚Ä¢ g) - f x_star - -t * ‚Äñg‚Äñ^2 ‚â§ t * ‚Äñg‚Äñ^2 / 2
      -- Need: f (x_star - t ‚Ä¢ g) - f x_star + t * ‚Äñg‚Äñ^2 ‚â§ t * ‚Äñg‚Äñ^2 / 2
      have h_eq : f (x_star - t ‚Ä¢ g) - f x_star - -t * ‚Äñg‚Äñ^2 =
          f (x_star - t ‚Ä¢ g) - f x_star + t * ‚Äñg‚Äñ^2 := by ring
      rw [h_eq] at h
      exact h
    have h_neg : f (x_star - t ‚Ä¢ g) < f x_star := by
      have : f (x_star - t ‚Ä¢ g) - f x_star ‚â§ t * ‚Äñg‚Äñ^2 / 2 - t * ‚Äñg‚Äñ^2 := by linarith
      have h_calc : t * ‚Äñg‚Äñ^2 / 2 - t * ‚Äñg‚Äñ^2 = -t * ‚Äñg‚Äñ^2 / 2 := by ring
      have h_neg_val : -t * ‚Äñg‚Äñ^2 / 2 < 0 := by
        have := mul_pos ht_pos (sq_pos_of_pos h_norm_pos)
        linarith
      linarith
    exact absurd h_neg (not_lt.mpr (hMin (x_star - t ‚Ä¢ g)))

  -- Step 2: Per-step distance contraction bound
  -- ‚Äñx_{k+1} - x*‚Äñ¬≤ ‚â§ ‚Äñx_k - x*‚Äñ¬≤ - 2Œ∑(f(x_k) - f(x*))
  have per_step_bound : ‚àÄ x : E,
      ‚ÄñgradientDescentStep f Œ∑ x - x_star‚Äñ^2 ‚â§ ‚Äñx - x_star‚Äñ^2 - 2 * Œ∑ * (f x - f x_star) := by
    intro x
    let g := gradient f x
    let y := gradientDescentStep f Œ∑ x
    -- y = x - Œ∑¬∑g
    have hy : y = x - Œ∑ ‚Ä¢ g := rfl
    -- ‚Äñy - x*‚Äñ¬≤ = ‚Äñ(x - x*) - Œ∑¬∑g‚Äñ¬≤
    have h_diff : y - x_star = (x - x_star) - Œ∑ ‚Ä¢ g := by simp only [hy]; abel
    -- Expand using ‚Äña - b‚Äñ¬≤ = ‚Äña‚Äñ¬≤ - 2‚ü®a,b‚ü© + ‚Äñb‚Äñ¬≤
    have h_expand : ‚Äñy - x_star‚Äñ^2 = ‚Äñx - x_star‚Äñ^2 - 2 * Œ∑ * @inner ‚Ñù E _ g (x - x_star) + Œ∑^2 * ‚Äñg‚Äñ^2 := by
      rw [h_diff, norm_sub_sq_real]
      have h1 : ‚ÄñŒ∑ ‚Ä¢ g‚Äñ^2 = Œ∑^2 * ‚Äñg‚Äñ^2 := by
        rw [norm_smul, Real.norm_eq_abs, mul_pow, sq_abs]
      have h2 : @inner ‚Ñù E _ (x - x_star) (Œ∑ ‚Ä¢ g) = Œ∑ * @inner ‚Ñù E _ (x - x_star) g := by
        rw [inner_smul_right]
      rw [h1, h2, real_inner_comm]
      ring
    -- From smooth_convex_interpolation:
    -- f(x) - f(x*) + (1/2L)‚Äñg‚Äñ¬≤ ‚â§ ‚ü®g, x - x*‚ü©
    have h_interp := smooth_convex_interpolation f L hL hSmooth hConvex x x_star hGradZero
    -- Multiply by 2Œ∑:
    -- 2Œ∑(f(x) - f(x*)) + (Œ∑/L)‚Äñg‚Äñ¬≤ ‚â§ 2Œ∑‚ü®g, x - x*‚ü©
    have h_interp_scaled : 2 * Œ∑ * (f x - f x_star) + (Œ∑ / L) * ‚Äñg‚Äñ^2 ‚â§
        2 * Œ∑ * @inner ‚Ñù E _ g (x - x_star) := by
      have h1 : 2 * Œ∑ * (f x - f x_star + (1 / (2 * L)) * ‚Äñg‚Äñ^2) ‚â§
          2 * Œ∑ * @inner ‚Ñù E _ g (x - x_star) := by
        apply mul_le_mul_of_nonneg_left h_interp
        linarith
      have h2 : 2 * Œ∑ * (f x - f x_star + (1 / (2 * L)) * ‚Äñg‚Äñ^2) =
          2 * Œ∑ * (f x - f x_star) + (Œ∑ / L) * ‚Äñg‚Äñ^2 := by
        have hL_ne : L ‚â† 0 := ne_of_gt hL
        field_simp
      linarith
    -- From h_expand:
    -- ‚Äñy - x*‚Äñ¬≤ = ‚Äñx - x*‚Äñ¬≤ - 2Œ∑‚ü®g, x - x*‚ü© + Œ∑¬≤‚Äñg‚Äñ¬≤
    -- Using h_interp_scaled:
    -- -2Œ∑‚ü®g, x - x*‚ü© ‚â§ -2Œ∑(f(x) - f(x*)) - (Œ∑/L)‚Äñg‚Äñ¬≤
    -- So: ‚Äñy - x*‚Äñ¬≤ ‚â§ ‚Äñx - x*‚Äñ¬≤ - 2Œ∑(f(x) - f(x*)) - (Œ∑/L)‚Äñg‚Äñ¬≤ + Œ∑¬≤‚Äñg‚Äñ¬≤
    --              = ‚Äñx - x*‚Äñ¬≤ - 2Œ∑(f(x) - f(x*)) + Œ∑(Œ∑ - 1/L)‚Äñg‚Äñ¬≤
    -- Since Œ∑ ‚â§ 1/L, we have Œ∑ - 1/L ‚â§ 0, so Œ∑(Œ∑ - 1/L)‚Äñg‚Äñ¬≤ ‚â§ 0
    have h_coeff_neg : Œ∑ * (Œ∑ - 1/L) ‚â§ 0 := by
      have h1 : Œ∑ - 1/L ‚â§ 0 := by linarith
      exact mul_nonpos_of_nonneg_of_nonpos (le_of_lt hŒ∑) h1
    have h_grad_term : Œ∑^2 * ‚Äñg‚Äñ^2 - (Œ∑ / L) * ‚Äñg‚Äñ^2 ‚â§ 0 := by
      have h1 : Œ∑^2 * ‚Äñg‚Äñ^2 - (Œ∑ / L) * ‚Äñg‚Äñ^2 = Œ∑ * (Œ∑ - 1/L) * ‚Äñg‚Äñ^2 := by
        have hL_ne : L ‚â† 0 := ne_of_gt hL
        have h : Œ∑ / L = Œ∑ * (1/L) := by ring
        rw [h]
        ring
      rw [h1]
      exact mul_nonpos_of_nonpos_of_nonneg h_coeff_neg (sq_nonneg _)
    calc ‚Äñy - x_star‚Äñ^2 = ‚Äñx - x_star‚Äñ^2 - 2 * Œ∑ * @inner ‚Ñù E _ g (x - x_star) + Œ∑^2 * ‚Äñg‚Äñ^2 := h_expand
      _ ‚â§ ‚Äñx - x_star‚Äñ^2 - (2 * Œ∑ * (f x - f x_star) + (Œ∑ / L) * ‚Äñg‚Äñ^2) + Œ∑^2 * ‚Äñg‚Äñ^2 := by linarith [h_interp_scaled]
      _ = ‚Äñx - x_star‚Äñ^2 - 2 * Œ∑ * (f x - f x_star) + (Œ∑^2 * ‚Äñg‚Äñ^2 - (Œ∑ / L) * ‚Äñg‚Äñ^2) := by ring
      _ ‚â§ ‚Äñx - x_star‚Äñ^2 - 2 * Œ∑ * (f x - f x_star) + 0 := by linarith [h_grad_term]
      _ = ‚Äñx - x_star‚Äñ^2 - 2 * Œ∑ * (f x - f x_star) := by ring

  -- Step 3: Descent property: f(x_{i+1}) ‚â§ f(x_i)
  have descent : ‚àÄ i : ‚Ñï, f (gradientDescentIterates f Œ∑ x‚ÇÄ (i + 1)) ‚â§ f (gradientDescentIterates f Œ∑ x‚ÇÄ i) := by
    intro i
    let x_i := gradientDescentIterates f Œ∑ x‚ÇÄ i
    have h_descent := descent_lemma f L hL hSmooth x_i Œ∑ hŒ∑ hŒ∑L
    have h_nonneg : 0 ‚â§ (Œ∑ / 2) * ‚Äñgradient f x_i‚Äñ^2 := by positivity
    -- gradientDescentIterates f Œ∑ x‚ÇÄ (i + 1) = gradientDescentStep f Œ∑ (gradientDescentIterates f Œ∑ x‚ÇÄ i) = gradientDescentStep f Œ∑ x_i
    have h_eq : gradientDescentIterates f Œ∑ x‚ÇÄ (i + 1) = gradientDescentStep f Œ∑ x_i := rfl
    rw [h_eq]
    linarith

  -- Step 4: Sum the per-step bounds via induction
  -- We prove: 2Œ∑ ¬∑ ‚àë_{i=0}^{k-1} (f(x_i) - f(x*)) ‚â§ ‚Äñx_0 - x*‚Äñ¬≤
  have sum_bound : ‚àÄ n : ‚Ñï, 2 * Œ∑ * (Finset.range n).sum (fun i => f (gradientDescentIterates f Œ∑ x‚ÇÄ i) - f x_star) ‚â§
      ‚Äñx‚ÇÄ - x_star‚Äñ^2 - ‚ÄñgradientDescentIterates f Œ∑ x‚ÇÄ n - x_star‚Äñ^2 := by
    intro n
    induction n with
    | zero =>
      simp only [Finset.range_zero, Finset.sum_empty, mul_zero, gradientDescentIterates]
      linarith [sq_nonneg ‚Äñx‚ÇÄ - x_star‚Äñ]
    | succ n ih =>
      -- Sum from 0 to n = Sum from 0 to n-1 + (f(x_n) - f(x*))
      rw [Finset.sum_range_succ]
      let x_n := gradientDescentIterates f Œ∑ x‚ÇÄ n
      let x_n1 := gradientDescentIterates f Œ∑ x‚ÇÄ (n + 1)
      -- From per_step_bound: ‚Äñx_{n+1} - x*‚Äñ¬≤ ‚â§ ‚Äñx_n - x*‚Äñ¬≤ - 2Œ∑(f(x_n) - f(x*))
      have h_step := per_step_bound x_n
      -- x_{n+1} = gradientDescentStep f Œ∑ x_n
      have h_eq : x_n1 = gradientDescentStep f Œ∑ x_n := rfl
      rw [‚Üê h_eq] at h_step
      -- ih: 2Œ∑ ¬∑ ‚àë_{i=0}^{n-1} (f(x_i) - f(x*)) ‚â§ ‚Äñx_0 - x*‚Äñ¬≤ - ‚Äñx_n - x*‚Äñ¬≤
      -- h_step: ‚Äñx_{n+1} - x*‚Äñ¬≤ ‚â§ ‚Äñx_n - x*‚Äñ¬≤ - 2Œ∑(f(x_n) - f(x*))
      -- Rearranging h_step: 2Œ∑(f(x_n) - f(x*)) ‚â§ ‚Äñx_n - x*‚Äñ¬≤ - ‚Äñx_{n+1} - x*‚Äñ¬≤
      have h_step' : 2 * Œ∑ * (f x_n - f x_star) ‚â§ ‚Äñx_n - x_star‚Äñ^2 - ‚Äñx_n1 - x_star‚Äñ^2 := by
        linarith
      -- Add ih and h_step'
      calc 2 * Œ∑ * ((Finset.range n).sum (fun i => f (gradientDescentIterates f Œ∑ x‚ÇÄ i) - f x_star) +
              (f x_n - f x_star))
          = 2 * Œ∑ * (Finset.range n).sum (fun i => f (gradientDescentIterates f Œ∑ x‚ÇÄ i) - f x_star) +
            2 * Œ∑ * (f x_n - f x_star) := by ring
        _ ‚â§ (‚Äñx‚ÇÄ - x_star‚Äñ^2 - ‚Äñx_n - x_star‚Äñ^2) + (‚Äñx_n - x_star‚Äñ^2 - ‚Äñx_n1 - x_star‚Äñ^2) := by linarith [ih, h_step']
        _ = ‚Äñx‚ÇÄ - x_star‚Äñ^2 - ‚Äñx_n1 - x_star‚Äñ^2 := by ring

  -- Step 5: Since f is decreasing, f(x_k) ‚â§ f(x_i) for all i < k
  -- So k ¬∑ (f(x_k) - f(x*)) ‚â§ ‚àë_{i=0}^{k-1} (f(x_i) - f(x*))
  have sum_lower_bound : (k : ‚Ñù) * (f (gradientDescentIterates f Œ∑ x‚ÇÄ k) - f x_star) ‚â§
      (Finset.range k).sum (fun i => f (gradientDescentIterates f Œ∑ x‚ÇÄ i) - f x_star) := by
    have h_mono : ‚àÄ i ‚àà Finset.range k, f (gradientDescentIterates f Œ∑ x‚ÇÄ k) ‚â§ f (gradientDescentIterates f Œ∑ x‚ÇÄ i) := by
      intro i hi
      rw [Finset.mem_range] at hi
      -- f(x_k) ‚â§ f(x_i) for i < k by repeated application of descent
      have : ‚àÄ j m : ‚Ñï, j ‚â§ m ‚Üí f (gradientDescentIterates f Œ∑ x‚ÇÄ m) ‚â§ f (gradientDescentIterates f Œ∑ x‚ÇÄ j) := by
        intro j m hjm
        induction m with
        | zero => simp only [Nat.le_zero] at hjm; rw [hjm]
        | succ m ih =>
          by_cases hj : j ‚â§ m
          ¬∑ calc f (gradientDescentIterates f Œ∑ x‚ÇÄ (m + 1)) ‚â§ f (gradientDescentIterates f Œ∑ x‚ÇÄ m) := descent m
              _ ‚â§ f (gradientDescentIterates f Œ∑ x‚ÇÄ j) := ih hj
          ¬∑ push_neg at hj
            have : j = m + 1 := by omega
            rw [this]
      exact this i k (le_of_lt hi)
    have h_term_bound : ‚àÄ i ‚àà Finset.range k,
        f (gradientDescentIterates f Œ∑ x‚ÇÄ k) - f x_star ‚â§ f (gradientDescentIterates f Œ∑ x‚ÇÄ i) - f x_star := by
      intro i hi
      linarith [h_mono i hi]
    calc (k : ‚Ñù) * (f (gradientDescentIterates f Œ∑ x‚ÇÄ k) - f x_star)
        = (Finset.range k).sum (fun _ => f (gradientDescentIterates f Œ∑ x‚ÇÄ k) - f x_star) := by
          simp only [Finset.sum_const, Finset.card_range, nsmul_eq_mul]
      _ ‚â§ (Finset.range k).sum (fun i => f (gradientDescentIterates f Œ∑ x‚ÇÄ i) - f x_star) := by
          apply Finset.sum_le_sum h_term_bound

  -- Step 6: Combine to get the final bound
  have h_sum := sum_bound k
  have h_lower := sum_lower_bound
  -- From h_sum: 2Œ∑ ¬∑ ‚àë(f(x_i) - f(x*)) ‚â§ ‚Äñx_0 - x*‚Äñ¬≤
  have h_sum' : 2 * Œ∑ * (Finset.range k).sum (fun i => f (gradientDescentIterates f Œ∑ x‚ÇÄ i) - f x_star) ‚â§
      ‚Äñx‚ÇÄ - x_star‚Äñ^2 := by
    have h_nonneg : 0 ‚â§ ‚ÄñgradientDescentIterates f Œ∑ x‚ÇÄ k - x_star‚Äñ^2 := sq_nonneg _
    linarith
  -- From h_lower and h_sum': 2Œ∑k(f(x_k) - f(x*)) ‚â§ ‚Äñx_0 - x*‚Äñ¬≤
  have h_combined : 2 * Œ∑ * k * (f (gradientDescentIterates f Œ∑ x‚ÇÄ k) - f x_star) ‚â§ ‚Äñx‚ÇÄ - x_star‚Äñ^2 := by
    calc 2 * Œ∑ * k * (f (gradientDescentIterates f Œ∑ x‚ÇÄ k) - f x_star)
        = 2 * Œ∑ * (k * (f (gradientDescentIterates f Œ∑ x‚ÇÄ k) - f x_star)) := by ring
      _ ‚â§ 2 * Œ∑ * (Finset.range k).sum (fun i => f (gradientDescentIterates f Œ∑ x‚ÇÄ i) - f x_star) := by
          apply mul_le_mul_of_nonneg_left h_lower; linarith
      _ ‚â§ ‚Äñx‚ÇÄ - x_star‚Äñ^2 := h_sum'
  -- Divide by 2Œ∑k > 0
  have hk_pos : (k : ‚Ñù) > 0 := by exact Nat.cast_pos.mpr hk
  have h_denom_pos : 2 * Œ∑ * k > 0 := by positivity
  -- f(x_k) - f(x*) ‚â§ ‚Äñx_0 - x*‚Äñ¬≤ / (2Œ∑k)
  have h_final : f (gradientDescentIterates f Œ∑ x‚ÇÄ k) - f x_star ‚â§ ‚Äñx‚ÇÄ - x_star‚Äñ^2 / (2 * Œ∑ * k) := by
    -- From h_combined: (2 * Œ∑ * k) * (f(x_k) - f(x*)) ‚â§ ‚Äñx‚ÇÄ - x*‚Äñ¬≤
    -- Dividing by (2 * Œ∑ * k) > 0: f(x_k) - f(x*) ‚â§ ‚Äñx‚ÇÄ - x*‚Äñ¬≤ / (2 * Œ∑ * k)
    have h1 : (2 * Œ∑ * k) * (f (gradientDescentIterates f Œ∑ x‚ÇÄ k) - f x_star) ‚â§ ‚Äñx‚ÇÄ - x_star‚Äñ^2 := h_combined
    have h2 : f (gradientDescentIterates f Œ∑ x‚ÇÄ k) - f x_star ‚â§
        ‚Äñx‚ÇÄ - x_star‚Äñ^2 / (2 * Œ∑ * k) := by
      have h3 : f (gradientDescentIterates f Œ∑ x‚ÇÄ k) - f x_star =
          ((2 * Œ∑ * k) * (f (gradientDescentIterates f Œ∑ x‚ÇÄ k) - f x_star)) / (2 * Œ∑ * k) := by
        field_simp
      rw [h3]
      exact div_le_div_of_nonneg_right h1 (le_of_lt h_denom_pos)
    exact h2
  exact h_final

/-- Linear convergence for strongly convex smooth functions.
    After k iterations: ‚Äñx_k - x*‚Äñ¬≤ ‚â§ (1 - Œº/L)^k ‚Äñx‚ÇÄ - x*‚Äñ¬≤

## Proof Strategy

For strongly convex and L-smooth functions with step size Œ∑ = 1/L:

1. **Contraction per iteration**: Each gradient descent step contracts the distance to optimum
   by a factor of (1 - Œº/L), i.e., ‚Äñx_{k+1} - x*‚Äñ¬≤ ‚â§ (1 - Œº/L)‚Äñx_k - x*‚Äñ¬≤

2. **Key ingredients**:
   - L-smoothness provides descent lemma: f(x - Œ∑‚àáf(x)) ‚â§ f(x) - (Œ∑/2)‚Äñ‚àáf(x)‚Äñ¬≤
   - Œº-strong convexity ensures: f(x*) + (Œº/2)‚Äñx - x*‚Äñ¬≤ ‚â§ f(x) + ‚ü®‚àáf(x), x* - x‚ü©
   - At optimum: ‚àáf(x*) = 0

3. **Per-step contraction lemma**: From strong convexity and smoothness
   ‚Äñx_{k+1} - x*‚Äñ¬≤ = ‚Äñx_k - Œ∑‚àáf(x_k) - x*‚Äñ¬≤
                     ‚â§ (1 - Œº/L)‚Äñx_k - x*‚Äñ¬≤

4. **Telescoping**: Apply contraction k times to get:
   ‚Äñx_k - x*‚Äñ¬≤ ‚â§ (1 - Œº/L)^k ‚Äñx‚ÇÄ - x*‚Äñ¬≤

This is the classical result for strongly convex optimization.
-/
theorem strongly_convex_linear_convergence (f : E ‚Üí ‚Ñù) (L Œº : ‚Ñù)
    (hL : 0 < L) (hŒº : 0 < Œº) (hŒºL : Œº ‚â§ L)
    (hSmooth : IsLSmooth f L) (hStrong : IsStronglyConvex f Œº)
    (x_star : E) (hMin : gradient f x_star = 0)
    (Œ∑ : ‚Ñù) (hŒ∑ : Œ∑ = 1 / L) (x‚ÇÄ : E) :
    ‚àÄ k : ‚Ñï, ‚ÄñgradientDescentIterates f Œ∑ x‚ÇÄ k - x_star‚Äñ^2 ‚â§
      (1 - Œº / L)^k * ‚Äñx‚ÇÄ - x_star‚Äñ^2 := by
  -- We proceed by induction on k
  intro k
  induction k with
  | zero =>
    -- Base case: k = 0
    -- gradientDescentIterates f Œ∑ x‚ÇÄ 0 = x‚ÇÄ
    -- ‚Äñx‚ÇÄ - x_star‚Äñ¬≤ ‚â§ (1 - Œº/L)^0 * ‚Äñx‚ÇÄ - x_star‚Äñ¬≤
    -- This simplifies to ‚Äñx‚ÇÄ - x_star‚Äñ¬≤ ‚â§ ‚Äñx‚ÇÄ - x_star‚Äñ¬≤
    simp only [gradientDescentIterates, pow_zero, one_mul]
    exact le_refl _
  | succ k ih =>
    -- Inductive case: assume ‚Äñx_k - x*‚Äñ¬≤ ‚â§ (1 - Œº/L)^k ‚Äñx‚ÇÄ - x*‚Äñ¬≤
    -- Need to show: ‚Äñx_{k+1} - x*‚Äñ¬≤ ‚â§ (1 - Œº/L)^{k+1} ‚Äñx‚ÇÄ - x*‚Äñ¬≤
    let x_k := gradientDescentIterates f Œ∑ x‚ÇÄ k
    let x_k1 := gradientDescentIterates f Œ∑ x‚ÇÄ (k + 1)
    -- Key: x_{k+1} = x_k - Œ∑‚àáf(x_k)
    have h_step : x_k1 = x_k - Œ∑ ‚Ä¢ gradient f x_k := rfl
    -- The per-iteration contraction: ‚Äñx_{k+1} - x*‚Äñ¬≤ ‚â§ (1 - Œº/L) ‚Äñx_k - x*‚Äñ¬≤
    --
    -- Proof outline:
    -- 1. Expand: ‚Äñx_{k+1} - x*‚Äñ¬≤ = ‚Äñ(x_k - x*) - Œ∑‚àáf(x_k)‚Äñ¬≤
    --    = ‚Äñx_k - x*‚Äñ¬≤ - 2Œ∑‚ü®‚àáf(x_k), x_k - x*‚ü© + Œ∑¬≤‚Äñ‚àáf(x_k)‚Äñ¬≤
    --
    -- 2. For Œº-strongly convex f with minimum at x*:
    --    ‚ü®‚àáf(x_k), x_k - x*‚ü© ‚â• Œº‚Äñx_k - x*‚Äñ¬≤ + (f(x_k) - f(x*))
    --    (This is the "strong convexity gradient inequality")
    --
    -- 3. For L-smooth f:
    --    ‚Äñ‚àáf(x_k)‚Äñ¬≤ ‚â§ 2L(f(x_k) - f(x*))
    --    (Co-coercivity of gradient)
    -- 4. Combining with Œ∑ = 1/L:
    --    ‚Äñx_{k+1} - x*‚Äñ¬≤ ‚â§ ‚Äñx_k - x*‚Äñ¬≤ - 2Œ∑¬∑Œº‚Äñx_k - x*‚Äñ¬≤
    --                      - 2Œ∑(f(x_k) - f(x*)) + Œ∑¬≤¬∑2L(f(x_k) - f(x*))
    --    = ‚Äñx_k - x*‚Äñ¬≤ - (2Œº/L)‚Äñx_k - x*‚Äñ¬≤
    --    = (1 - 2Œº/L)‚Äñx_k - x*‚Äñ¬≤ ‚â§ (1 - Œº/L)‚Äñx_k - x*‚Äñ¬≤  (since 2Œº/L ‚â• Œº/L)
    -- The formal proof requires the following key lemmas:

    -- Lemma 1: Strong convexity gradient inequality
    -- For Œº-strongly convex f with ‚àáf(x*) = 0:
    -- ‚ü®‚àáf(x), x - x*‚ü© ‚â• Œº‚Äñx - x*‚Äñ¬≤ + (f(x) - f(x*))
    --
    -- This follows from the strong convexity definition:
    -- f(y) ‚â• f(x) + ‚ü®‚àáf(x), y-x‚ü© + (Œº/2)‚Äñx-y‚Äñ¬≤
    -- Setting y = x* and using f(x*) ‚â§ f(x) + ‚ü®‚àáf(x), x*-x‚ü© + (Œº/2)‚Äñx-x*‚Äñ¬≤

    -- Lemma 2: Co-coercivity of L-smooth gradients
    -- For L-smooth f with ‚àáf(x*) = 0:
    -- ‚Äñ‚àáf(x)‚Äñ¬≤ ‚â§ 2L(f(x) - f(x*))
    --
    -- This follows from the descent lemma applied at x:
    -- f(x - (1/L)‚àáf(x)) ‚â§ f(x) - (1/2L)‚Äñ‚àáf(x)‚Äñ¬≤
    -- Since f(x*) is the minimum: f(x*) ‚â§ f(x - (1/L)‚àáf(x))
    -- Therefore: f(x*) ‚â§ f(x) - (1/2L)‚Äñ‚àáf(x)‚Äñ¬≤
    -- Rearranging: ‚Äñ‚àáf(x)‚Äñ¬≤ ‚â§ 2L(f(x) - f(x*))
    have h_contraction : ‚Äñx_k1 - x_star‚Äñ^2 ‚â§ (1 - Œº / L) * ‚Äñx_k - x_star‚Äñ^2 := by
      -- Let g = ‚àáf(x_k)
      let g := gradient f x_k
      -- x_{k+1} - x* = (x_k - x*) - Œ∑¬∑g
      have h_diff : x_k1 - x_star = (x_k - x_star) - Œ∑ ‚Ä¢ g := by
        simp only [h_step]
        abel
      -- ‚Äñx_{k+1} - x*‚Äñ¬≤ = ‚Äñ(x_k - x*) - Œ∑¬∑g‚Äñ¬≤
      --                  = ‚Äñx_k - x*‚Äñ¬≤ - 2Œ∑‚ü®g, x_k - x*‚ü© + Œ∑¬≤‚Äñg‚Äñ¬≤
      have h_expand : ‚Äñx_k1 - x_star‚Äñ^2 =
          ‚Äñx_k - x_star‚Äñ^2 - 2 * Œ∑ * @inner ‚Ñù E _ g (x_k - x_star) + Œ∑^2 * ‚Äñg‚Äñ^2 := by
        rw [h_diff]
        -- Use polarization: ‚Äña - b‚Äñ¬≤ = ‚Äña‚Äñ¬≤ + ‚Äñb‚Äñ¬≤ - 2‚ü®a, b‚ü©
        -- ‚Äña - Œ∑ ‚Ä¢ g‚Äñ¬≤ = ‚Äña‚Äñ¬≤ + ‚ÄñŒ∑ ‚Ä¢ g‚Äñ¬≤ - 2‚ü®a, Œ∑ ‚Ä¢ g‚ü©
        --              = ‚Äña‚Äñ¬≤ + Œ∑¬≤‚Äñg‚Äñ¬≤ - 2Œ∑‚ü®a, g‚ü©
        --              = ‚Äña‚Äñ¬≤ - 2Œ∑‚ü®g, a‚ü© + Œ∑¬≤‚Äñg‚Äñ¬≤ (by inner product symmetry)
        rw [norm_sub_sq_real]
        -- ‚ÄñŒ∑ ‚Ä¢ g‚Äñ¬≤ = (|Œ∑| * ‚Äñg‚Äñ)¬≤ = |Œ∑|¬≤ * ‚Äñg‚Äñ¬≤ = Œ∑¬≤ * ‚Äñg‚Äñ¬≤
        have h_norm_smul_sq : ‚ÄñŒ∑ ‚Ä¢ g‚Äñ^2 = Œ∑^2 * ‚Äñg‚Äñ^2 := by
          rw [norm_smul, Real.norm_eq_abs, mul_pow, sq_abs]
        rw [h_norm_smul_sq]
        -- ‚ü®a, Œ∑ ‚Ä¢ g‚ü© = Œ∑ * ‚ü®a, g‚ü© = Œ∑ * ‚ü®g, a‚ü© (by symmetry)
        rw [inner_smul_right, real_inner_comm]
        ring
      -- Now use h_expand and bound each term. From Œ∑ = 1/L:
      have h_eta : Œ∑ = 1 / L := hŒ∑
      have h_eta_sq : Œ∑^2 = 1 / L^2 := by rw [h_eta]; ring
      -- Use the interpolation condition which combines strong convexity and smoothness
      have h_interp := strong_smooth_interpolation f L Œº hL hŒº hSmooth hStrong x_k x_star hMin
      -- Let inner_val = ‚ü®g, x_k - x*‚ü© for clarity
      let inner_val := @inner ‚Ñù E _ g (x_k - x_star)
      -- From h_expand: ‚Äñx_{k+1} - x*‚Äñ¬≤ = ‚Äñx_k - x*‚Äñ¬≤ - 2Œ∑¬∑inner_val + Œ∑¬≤‚Äñg‚Äñ¬≤
      -- With Œ∑ = 1/L: = ‚Äñx_k - x*‚Äñ¬≤ - (2/L)¬∑inner_val + (1/L¬≤)‚Äñg‚Äñ¬≤
      -- From interpolation: inner_val ‚â• (ŒºL)/(Œº+L)‚Äñx_k - x*‚Äñ¬≤ + 1/(Œº+L)‚Äñg‚Äñ¬≤
      -- So: -(2/L)¬∑inner_val ‚â§ -(2/L)¬∑[(ŒºL)/(Œº+L)‚Äñx_k - x*‚Äñ¬≤ + 1/(Œº+L)‚Äñg‚Äñ¬≤]
      --                      = -(2Œº)/(Œº+L)‚Äñx_k - x*‚Äñ¬≤ - 2/(L(Œº+L))‚Äñg‚Äñ¬≤. Combined:
      -- ‚Äñx_{k+1} - x*‚Äñ¬≤ ‚â§ ‚Äñx_k - x*‚Äñ¬≤ - (2Œº)/(Œº+L)‚Äñx_k - x*‚Äñ¬≤ + [1/L¬≤ - 2/(L(Œº+L))]‚Äñg‚Äñ¬≤
      --
      -- The coefficient of ‚Äñg‚Äñ¬≤:
      -- 1/L¬≤ - 2/(L(Œº+L)) = [(Œº+L) - 2L] / [L¬≤(Œº+L)] = (Œº-L) / [L¬≤(Œº+L)] ‚â§ 0 (since Œº ‚â§ L)
      --
      -- So we can drop the ‚Äñg‚Äñ¬≤ term:
      -- ‚Äñx_{k+1} - x*‚Äñ¬≤ ‚â§ ‚Äñx_k - x*‚Äñ¬≤ - (2Œº)/(Œº+L)‚Äñx_k - x*‚Äñ¬≤
      --                = [1 - 2Œº/(Œº+L)]‚Äñx_k - x*‚Äñ¬≤
      --                = [(Œº+L-2Œº)/(Œº+L)]‚Äñx_k - x*‚Äñ¬≤
      --                = [(L-Œº)/(L+Œº)]‚Äñx_k - x*‚Äñ¬≤
      --
      -- Finally: (L-Œº)/(L+Œº) ‚â§ 1 - Œº/L because:
      -- (L-Œº)/(L+Œº) ‚â§ (L-Œº)/L = 1 - Œº/L iff L+Œº ‚â• L, which is true since Œº > 0
      have h_coeff_neg : 1 / L^2 - 2 / (L * (Œº + L)) ‚â§ 0 := by
        have h3 : 1 / L^2 - 2 / (L * (Œº + L)) = (Œº - L) / (L^2 * (Œº + L)) := by field_simp; ring
        rw [h3]
        apply div_nonpos_of_nonpos_of_nonneg
        ¬∑ linarith  -- Œº - L ‚â§ 0 since Œº ‚â§ L
        ¬∑ apply mul_nonneg (sq_nonneg L)
          linarith  -- Œº + L > 0
      have h_contraction_factor : (L - Œº) / (L + Œº) ‚â§ 1 - Œº / L := by
        have h1 : (L - Œº) / (L + Œº) ‚â§ (L - Œº) / L := by
          apply div_le_div_of_nonneg_left
          ¬∑ linarith  -- L - Œº ‚â• 0
          ¬∑ linarith  -- L > 0
          ¬∑ linarith  -- L + Œº ‚â• L
        have h2 : (L - Œº) / L = 1 - Œº / L := by field_simp
        linarith
      -- Chain h_expand with h_interp and algebraic bounds
      -- Goal: ‚Äñx_k1 - x_star‚Äñ^2 ‚â§ (1 - Œº / L) * ‚Äñx_k - x_star‚Äñ^2
      --
      -- From h_expand (with Œ∑ = 1/L):
      -- ‚Äñx_k1 - x_star‚Äñ^2 = ‚Äñx_k - x_star‚Äñ^2 - (2/L)‚ü®g, x_k - x*‚ü© + (1/L¬≤)‚Äñg‚Äñ¬≤
      --
      -- From h_interp (assuming strong_smooth_interpolation is proved):
      -- ‚ü®g, x_k - x*‚ü© ‚â• (ŒºL)/(Œº+L)‚Äñx_k - x*‚Äñ¬≤ + 1/(Œº+L)‚Äñg‚Äñ¬≤
      --
      -- Substituting:
      -- ‚Äñx_k1 - x_star‚Äñ^2 ‚â§ ‚Äñx_k - x*‚Äñ¬≤ - (2/L)¬∑[(ŒºL)/(Œº+L)‚Äñx_k - x*‚Äñ¬≤ + 1/(Œº+L)‚Äñg‚Äñ¬≤] + (1/L¬≤)‚Äñg‚Äñ¬≤
      --                    = ‚Äñx_k - x*‚Äñ¬≤ - (2Œº)/(Œº+L)‚Äñx_k - x*‚Äñ¬≤ + [1/L¬≤ - 2/(L(Œº+L))]‚Äñg‚Äñ¬≤
      --
      -- By h_coeff_neg, the coefficient of ‚Äñg‚Äñ¬≤ is ‚â§ 0, and ‚Äñg‚Äñ¬≤ ‚â• 0, so:
      -- ‚Äñx_k1 - x_star‚Äñ^2 ‚â§ ‚Äñx_k - x*‚Äñ¬≤ - (2Œº)/(Œº+L)‚Äñx_k - x*‚Äñ¬≤
      --                    = [1 - 2Œº/(Œº+L)]‚Äñx_k - x*‚Äñ¬≤
      --                    = [(L-Œº)/(L+Œº)]‚Äñx_k - x*‚Äñ¬≤
      --
      -- By h_contraction_factor: (L-Œº)/(L+Œº) ‚â§ 1 - Œº/L
      -- First compute the coefficient 1 - 2Œº/(Œº+L) = (L-Œº)/(L+Œº)
      have h_coeff_eq : 1 - 2 * Œº / (Œº + L) = (L - Œº) / (L + Œº) := by
        field_simp
        ring
      -- Combine everything using transitivity. The proof uses
      -- strong_smooth_interpolation (now fully proved).
      -- Key inequality from h_interp:
      have h_inner_bound : inner_val ‚â• (Œº * L) / (Œº + L) * ‚Äñx_k - x_star‚Äñ^2 +
                                        1 / (Œº + L) * ‚Äñg‚Äñ^2 := h_interp
      -- Substitute Œ∑ = 1/L into h_expand
      have h_expand' : ‚Äñx_k1 - x_star‚Äñ^2 =
          ‚Äñx_k - x_star‚Äñ^2 - 2 / L * inner_val + 1 / L^2 * ‚Äñg‚Äñ^2 := by
        rw [h_expand, h_eta]; ring
      -- Apply the bound on inner_val
      have h_step1 : ‚Äñx_k1 - x_star‚Äñ^2 ‚â§
          ‚Äñx_k - x_star‚Äñ^2 - 2 / L * ((Œº * L) / (Œº + L) * ‚Äñx_k - x_star‚Äñ^2 +
                                       1 / (Œº + L) * ‚Äñg‚Äñ^2) + 1 / L^2 * ‚Äñg‚Äñ^2 := by
        rw [h_expand']
        have h_L_pos : 0 < L := hL
        have h_2L_pos : 0 < 2 / L := by positivity
        nlinarith [h_inner_bound, sq_nonneg ‚Äñg‚Äñ, sq_nonneg ‚Äñx_k - x_star‚Äñ]
      -- Simplify to get the coefficient form
      have h_step2 : ‚Äñx_k1 - x_star‚Äñ^2 ‚â§
          ‚Äñx_k - x_star‚Äñ^2 - 2 * Œº / (Œº + L) * ‚Äñx_k - x_star‚Äñ^2 +
          (1 / L^2 - 2 / (L * (Œº + L))) * ‚Äñg‚Äñ^2 := by
        calc ‚Äñx_k1 - x_star‚Äñ^2
            ‚â§ ‚Äñx_k - x_star‚Äñ^2 - 2 / L * ((Œº * L) / (Œº + L) * ‚Äñx_k - x_star‚Äñ^2 +
                                           1 / (Œº + L) * ‚Äñg‚Äñ^2) + 1 / L^2 * ‚Äñg‚Äñ^2 := h_step1
          _ = ‚Äñx_k - x_star‚Äñ^2 - 2 * Œº / (Œº + L) * ‚Äñx_k - x_star‚Äñ^2 +
              (1 / L^2 - 2 / (L * (Œº + L))) * ‚Äñg‚Äñ^2 := by
            have hL_ne : L ‚â† 0 := ne_of_gt hL
            have hŒºL_ne : Œº + L ‚â† 0 := by linarith
            field_simp
            ring
      -- Drop the ‚Äñg‚Äñ¬≤ term (coefficient is ‚â§ 0)
      have h_step3 : ‚Äñx_k1 - x_star‚Äñ^2 ‚â§
          ‚Äñx_k - x_star‚Äñ^2 - 2 * Œº / (Œº + L) * ‚Äñx_k - x_star‚Äñ^2 := by
        have h_g_sq_nonneg : 0 ‚â§ ‚Äñg‚Äñ^2 := sq_nonneg _
        nlinarith [h_step2, h_coeff_neg, h_g_sq_nonneg]
      -- Factor and apply contraction bound
      calc ‚Äñx_k1 - x_star‚Äñ^2
          ‚â§ ‚Äñx_k - x_star‚Äñ^2 - 2 * Œº / (Œº + L) * ‚Äñx_k - x_star‚Äñ^2 := h_step3
        _ = (1 - 2 * Œº / (Œº + L)) * ‚Äñx_k - x_star‚Äñ^2 := by ring
        _ = (L - Œº) / (L + Œº) * ‚Äñx_k - x_star‚Äñ^2 := by rw [h_coeff_eq]
        _ ‚â§ (1 - Œº / L) * ‚Äñx_k - x_star‚Äñ^2 := by
            apply mul_le_mul_of_nonneg_right h_contraction_factor (sq_nonneg _)
    -- Apply contraction and inductive hypothesis
    calc ‚Äñx_k1 - x_star‚Äñ^2
        ‚â§ (1 - Œº / L) * ‚Äñx_k - x_star‚Äñ^2 := h_contraction
      _ ‚â§ (1 - Œº / L) * ((1 - Œº / L)^k * ‚Äñx‚ÇÄ - x_star‚Äñ^2) := by {
          apply mul_le_mul_of_nonneg_left ih
          have h1 : Œº / L ‚â§ 1 := (div_le_one (by linarith : 0 < L)).mpr hŒºL
          linarith
        }
      _ = (1 - Œº / L)^(k + 1) * ‚Äñx‚ÇÄ - x_star‚Äñ^2 := by ring

end Gradient
